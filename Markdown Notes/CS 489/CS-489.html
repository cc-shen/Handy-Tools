<!DOCTYPE html>
  <html>
    <head>
      <title>CS-489</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      
        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({"extensions":["tex2jax.js"],"jax":["input/TeX","output/HTML-CSS"],"messageStyle":"none","tex2jax":{"processEnvironments":false,"processEscapes":true,"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"TeX":{"extensions":["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]},"HTML-CSS":{"availableFonts":["TeX"]}});
        </script>
        <script type="text/javascript" async src="file:////Users/Charles/.vscode/extensions/shd101wyy.markdown-preview-enhanced-0.2.9/node_modules/@shd101wyy/mume/dependencies/mathjax/MathJax.js"></script>
        
      
      
      
      
      
      

      <style> 
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{padding:0 1.6em;margin-top:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc li{margin-bottom:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{list-style-type:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  150px);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none} 
       
      </style>
    </head>
    <body for="html-export">
      <div class="mume markdown-preview   ">
      <h1 id="cs-489698-introduction-to-machine-learning">CS 489/698 - Introduction to Machine Learning</h1>
<ul>
<li>Instructor: YaoLiang Yu (<a href="yaoliang.yu@uwaterloo.ca" class="uri">yaoliang.yu@uwaterloo.ca</a>)
<ul>
<li>Office hours: DC 3617: TTh 2:40 - 3:40</li>
</ul></li>
<li>TAs: Amir, Jingjing, Nicole, Nimesh, Shrinu</li>
<li>Website: <a href="https://cs.uwaterloo.ca/~y328yu/mycourses/489" class="uri">https://cs.uwaterloo.ca/~y328yu/mycourses/489</a></li>
<li>Piazza: announcements, questions, discussions, etc.</li>
<li>Learn: assignments, solutions, grades, etc.</li>
</ul>
<h2 id="lecture-00-introduction-_september-07-2017_">Lecture 00: Introduction, <em>September 07, 2017</em></h2>
<h3 id="course-overview">Course Overview</h3>
<h4 id="what-is-machine-learning">What is Machine Learning</h4>
<p>“…field of study that gives computers the ability to learn without being explicitly programmed.” – Arthur Samuel (1959)</p>
<h4 id="learning-categories">Learning Categories</h4>
<h5 id="supervised">Supervised</h5>
<ul>
<li>Classification</li>
<li>Regression</li>
<li>Ranking</li>
</ul>
<p>Teacher provides answer</p>
<h5 id="reinforcement">Reinforcement</h5>
<ul>
<li>Control</li>
<li>Pricing</li>
<li>Gaming</li>
</ul>
<p>Teacher provides motivation</p>
<h5 id="unsupervised">Unsupervised</h5>
<ul>
<li>Clustered</li>
<li>Visualization</li>
<li>Representation</li>
</ul>
<p>Surprise, surprise</p>
<h4 id="supervised-formally">Supervised, formally</h4>
<ul>
<li>Given a <em>training set</em> of <em>pairs</em> of examples <span class="math inline">\((x_{i}, y_{i})\)</span></li>
<li>Return a <em>function</em> <span class="math inline">\(f: X \rightarrow Y\)</span></li>
<li>On an <em>unseen test</em> example <span class="math inline">\(x\)</span>, output <span class="math inline">\(f(x)\)</span></li>
<li>The goal is to do well on unseen test data
<ul>
<li>Usually do not care about performance on training data</li>
</ul></li>
</ul>
<h3 id="focus-of-ml-research">Focus of ML Research</h3>
<ul>
<li>Representation and Interpretation</li>
<li>Generalization</li>
<li>Complexity</li>
<li>Efficiency</li>
<li>Applications</li>
</ul>
<h3 id="collaboration-with-focal">Collaboration with focal</h3>
<p><a href="agastya@focal.systems" class="uri">agastya@focal.systems</a></p>
<h2 id="lecture-01-perceptron-_september-12-2017_">Lecture 01: Perceptron, <em>September 12, 2017</em></h2>
<h3 id="spam-filtering-example">Spam Filtering Example</h3>
<table>
<thead>
<tr class="header">
<th></th>
<th>and</th>
<th>viagra</th>
<th>the</th>
<th>of</th>
<th>nigeria</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\({\bf x}^{1}\)</span></td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>+1</td>
</tr>
<tr class="even">
<td><span class="math inline">\({\bf x}^{2}\)</span></td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>-1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\({\bf x}^{3}\)</span></td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>+1</td>
</tr>
<tr class="even">
<td><span class="math inline">\({\bf x}^{4}\)</span></td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>-1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\({\bf x}^{5}\)</span></td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>+1</td>
</tr>
<tr class="even">
<td><span class="math inline">\({\bf x}^{6}\)</span></td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>-1</td>
</tr>
</tbody>
</table>
<ul>
<li><em>Training set</em> <span class="math inline">\((X = [x_{1}, x_{2}, ..., x_{n}], y = {y_{1}, y_{2}, ..., y_{n}})\)</span>
<ul>
<li><span class="math inline">\(x_{i}\)</span> in <span class="math inline">\(X = \mathbb{R}^{d}\)</span>: instance <span class="math inline">\(i\)</span> with <span class="math inline">\(d\)</span> dimensional features</li>
<li><span class="math inline">\(y_{i}\)</span> in <span class="math inline">\(Y = \{-1, 1\}\)</span>: instance <span class="math inline">\(i\)</span> is spam or ham?</li>
</ul></li>
<li>Good <em>feature representation</em> is of uttermost importance</li>
</ul>
<h3 id="batch-vs-online">Batch vs. Online</h3>
<ul>
<li><strong>Batch Learning</strong>
<ul>
<li>Interested in performance <em>test</em> set <span class="math inline">\(X&#39;\)</span></li>
<li>Training set <span class="math inline">\((X, y)\)</span> is just a means</li>
<li>Statistical assumption on <span class="math inline">\(X\)</span> and <span class="math inline">\(X&#39;\)</span></li>
</ul></li>
<li><strong>Online learning</strong>
<ul>
<li>Data <em>comes one by one</em> (streaming)</li>
<li>Need to predict <span class="math inline">\(y\)</span> before knowing its true value</li>
<li>Interested in making as few mistakes as possible</li>
<li>“Friendliness” of the sequence <span class="math inline">\((x_{1}, y_{1}), (x_{2}, y_{2}),...\)</span></li>
</ul></li>
<li>Online to Batch conversion</li>
</ul>
<h3 id="linear-threshold-function">Linear Threshold Function</h3>
<p>Find <span class="math inline">\(({\bf w}, b)\)</span> such that for all <span class="math inline">\(i\)</span>: <span class="math display">\[y_{i} = \text{sign}(\langle {\bf w}, {\bf x}^{i}\rangle + b)\]</span></p>
<ul>
<li><span class="math inline">\({\bf w}\)</span> in <span class="math inline">\(R^{d}\)</span>: weight vector for the <em>separating hyperplane</em></li>
<li><span class="math inline">\(b\)</span> in <span class="math inline">\(R\)</span>: offset (threshold, bias) of the separating hyperplane</li>
<li>sign: threshold function</li>
</ul>
<p><span class="math display">\[
sign(t) =
\begin{cases}
    1, &amp; t &gt; 0 \\
    -1, &amp; t \leq 0
\end{cases}
\]</span></p>
<p>For <span class="math inline">\(t = 0\)</span>, it does not matter what <span class="math inline">\(sign(t)\)</span> returns as long as its consistent.</p>
<h3 id="simplification">Simplification</h3>
<p><span class="math display">\[\langle{\bf w}, {\bf x}\rangle + b = \bigg\langle \underbrace{\begin{pmatrix} {\bf w} \\ b \end{pmatrix}}_{\rlap{\text{also denoted as }w}\phantom{eeeeeeee}}, \begin{pmatrix} {\bf x} \\ 1 \end{pmatrix} \bigg\rangle\]</span></p>
<p>Padding constant 1 to the end of each <span class="math inline">\(\bf {x}\)</span></p>
<p><span class="math display">\[y \cdot \text{sign}(\langle {\bf w}, {\bf x}\rangle) = \text{sign}(\langle {\bf w}, y{\bf x})\]</span></p>
<p>Pre-multiply each <span class="math inline">\({\bf x}\)</span> with its label <span class="math inline">\(y\)</span></p>
<p>Find <span class="math inline">\({\bf w}\)</span> such that <span class="math inline">\(A^{\intercal}{\bf w} &gt; 0\)</span></p>
<ul>
<li><span class="math inline">\(A = [{\bf a}^{1}, ..., {\bf a}^{n}]\)</span></li>
<li><span class="math inline">\({\bf a}^{i} = \begin{pmatrix} y_{i}{\bf x}^{i} \\ y_{i} \end{pmatrix}\)</span></li>
</ul>
<h3 id="perceptron-rosenblatt58">Perceptron [Rosenblatt’58]</h3>
<p>Nonlinear activation function</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\phantom{eee}~x_{1} \xrightarrow{w_{1}} \\
&amp;\phantom{eee}~x_{2} \xrightarrow{w_{2}} \\
&amp;\phantom{eee}~x_{3} \xrightarrow{w_{3}} \Sigma \longrightarrow \geq 0? \xrightarrow{\text{Nonlinear activation function}} \\
&amp;\underbrace{x_{4} \xrightarrow{w_{4}}}_{\text{Linear superposition}}
\end{aligned}
\]</span></p>
<h3 id="the-perceptron-algorithm">The Perceptron Algorithm</h3>
<p>Input: <span class="math inline">\(A\in \mathbb{R}^{(d+1)\times n}\)</span>, threshold <span class="math inline">\(\delta \geq 0\)</span>, initialize <span class="math inline">\({\bf w}_{0} \in \mathbb{R}^{d+1}\)</span> arbitrarily</p>
<ol style="list-style-type: decimal">
<li><strong>repeat</strong></li>
<li>    select some column <span class="math inline">\({\bf a}\)</span> of <span class="math inline">\(A\)</span></li>
<li>    <strong>if</strong> <span class="math inline">\(\langle {\bf a}, {\bf w_{t}} \rangle \leq \delta\)</span> <strong>then</strong></li>
<li>        <span class="math inline">\({\bf w}_{t+1} = {\bf w}_{t} + {\bf a}\)</span> <code>// update only when making a mistake</code></li>
<li>        <span class="math inline">\(t \leftarrow t + 1\)</span></li>
<li><strong>until</strong> <span class="math inline">\(convergence\)</span></li>
</ol>
<p>Typically <span class="math inline">\(\delta = 0\)</span>, <span class="math inline">\({\bf w}_{0} = {\bf 0}\)</span></p>
<p><span class="math display">\[\langle {\bf a}, {\bf w} \rangle = y(\langle x, {\bf w} \rangle + b) &lt; 0 \text{ iff. } \underbrace{y}_{\text{truth}} \neq \underbrace{\text{sign}(\langle x, {\bf w} \rangle + b)}_{\text{prediction}}\]</span></p>
<p><strong>Lazy</strong> update: if it ain’t broken, don’t fix it.</p>
<p><span class="math display">\[\langle {\bf a}, {\bf w}_{t + 1} \rangle = \langle {\bf a}, {\bf w}_{t} + {\bf a} \rangle = \langle {\bf a}, {\bf w}_{t} \rangle + ||a||^{2}_{2} &gt; \langle {\bf a}, {\bf w}_{t} \rangle\]</span></p>
<h4 id="does-it-work">Does It Work</h4>
<table>
<thead>
<tr class="header">
<th></th>
<th>and</th>
<th>viagra</th>
<th>the</th>
<th>of</th>
<th>nigeria</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\({\bf x}^{1}\)</span></td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>+1</td>
</tr>
<tr class="even">
<td><span class="math inline">\({\bf x}^{2}\)</span></td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>-1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\({\bf x}^{3}\)</span></td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>+1</td>
</tr>
<tr class="even">
<td><span class="math inline">\({\bf x}^{4}\)</span></td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>-1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\({\bf x}^{5}\)</span></td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>+1</td>
</tr>
<tr class="even">
<td><span class="math inline">\({\bf x}^{6}\)</span></td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>-1</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
{\bf w} \leftarrow {\bf w} + y{\bf x}
\]</span></p>
<p><span class="math display">\[
b \leftarrow b + y
\]</span></p>
<ul>
<li><span class="math inline">\(w_{0} = \begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}\)</span>, <span class="math inline">\(b_{0} = 0\)</span>, prediction on <span class="math inline">\({\bf x}^{1}\)</span> is <em>undefined</em></li>
<li><span class="math inline">\(w_{1} = \begin{pmatrix} 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \end{pmatrix}\)</span>, <span class="math inline">\(b_{1} = 1\)</span>, predict 1 on <span class="math inline">\({\bf x}^{2}\)</span>, <em>wrong</em></li>
<li><span class="math inline">\(w_{2} = \begin{pmatrix} 1 &amp; 1 &amp; -1 &amp; 0 &amp; 1 \end{pmatrix}\)</span>, <span class="math inline">\(b_{2} = 0\)</span>, prediction on <span class="math inline">\({\bf x}^{3}\)</span> i <em>undefined</em></li>
<li><span class="math inline">\(w_{3} = \begin{pmatrix} 1 &amp; 2 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix}\)</span>, <span class="math inline">\(b_{3} = 1\)</span>, predict 1 on <span class="math inline">\({\bf x}^{4}\)</span>, <em>wrong</em></li>
<li><span class="math inline">\(w_{4} = \begin{pmatrix} 0 &amp; 2 &amp; 0 &amp; -1 &amp; 1 \end{pmatrix}\)</span>, <span class="math inline">\(b_{4} = 0\)</span>, predict 1 on <span class="math inline">\({\bf x}^{5}\)</span>, <em>correct</em></li>
<li><span class="math inline">\(w_{4} = \begin{pmatrix} 0 &amp; 2 &amp; 0 &amp; -1 &amp; 1 \end{pmatrix}\)</span>, <span class="math inline">\(b_{4} = 0\)</span>, predict -1 on <span class="math inline">\({\bf x}^{6}\)</span>, <em>correct</em></li>
</ul>
<h3 id="perception-convergence-theorem">Perception Convergence Theorem</h3>
<hr>
<p><strong>Thm.</strong> (Block’62; Novikoff’62)<br>
<em>Assume there exists some <span class="math inline">\({\bf w}\)</span> such that <span class="math inline">\(A^{\intercal}{\bf w} &gt; 0\)</span></em>, then the perception algorithm converges to some <span class="math inline">\(w^{*}\)</span>. If each column of <span class="math inline">\(A\)</span> is selected indefinitely often, then <span class="math inline">\(A^{\intercal}{\bf w}^{*} &gt; \delta\)</span>.</p>
<p><strong>Proof</strong><br>
Note that <span class="math inline">\(||w||_{p} = \left( \sum_{i} |w_{i}|^{p} \right)^{1/p}\)</span></p>
<p><span class="math inline">\(\forall \gamma &gt; 0\)</span>, <span class="math inline">\(\exists~w^{*}\)</span> such that <span class="math inline">\(A^{\intercal}w^{*} \geq \gamma &gt; 0\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
\langle w_{t + 1}, w^{*} \rangle
&amp;= \langle w_{t} + a, w^{*} \rangle
= \langle w_{t}, w^{*} \rangle + \langle a, w^{*} \rangle \\
&amp;\geq \langle w_{t}, w^{*} \rangle + \gamma \\
&amp;\geq \cdots \geq \underbrace{\langle w_{0}, w^{*} \rangle}_{0} + (t)\gamma
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
|w_{t+1}||_{2}^{2}
&amp;= || w_{t} + a ||^{2}_{2}
= \langle w_{t} + a, w_{t} + a \rangle
= ||w_{t}||^{2}_{2} + 2\underbrace{\langle w_{t}, a \rangle}_{\leq~\delta~=~0} + ||a||^{2}_{2} \\
&amp;\leq ||w_{t}||^{2}_{2} + ||a||^{2}_{2} \\
&amp;\leq ||w_{t}||^{2}_{2} + R^{2} \leq ||w_{t}||^{2}_{2} + (t+1)R^{2}
\end{aligned}
\]</span></p>
<hr>
<p><strong>Cor.</strong><br>
Let <span class="math inline">\(\delta = 0\)</span> and <span class="math inline">\(w_{0} = 0\)</span>. Then percetpion converges after at most <span class="math inline">\((\frac{R}{\gamma})^{2}\)</span> steps, where <span class="math display">\[R = \underset{i}{\max} ||A_{:i}||_{2},~~~\gamma = \underset{w:||w||_{2} \leq 1}{\max} \underset{i}{\min} \langle w, A_{:i} \rangle\]</span></p>
<hr>
<h4 id="the-margin">The Margin</h4>
<p><span class="math display">\[
\begin{aligned}
\min_{{\bf w}^{*}:~A^{\intercal}{\bf w}^{*}~\geq~\gamma{\bf 1}} \frac{||{\bf w}^{*}||^{2}_{2}}{\gamma^{2}} &amp;= \min_{{\bf w}:~A^{\intercal}{\bf w}~\geq~{\bf 1}} ||{\bf w}||^{2}_{2} \\
&amp;= \min_{({\bf w}, t):~||{\bf w}||_{2}\leq t,~A^{\intercal}{\bf w} \geq {\bf 1}} t^{2} \\
&amp;= \min_{({\bf w}, t):~||{\bf w}||_{2}\leq 1,~A^{\intercal}{\bf w} \geq \frac{1}{t}{\bf 1}} t^{2} \\
&amp;= \min_{({\bf w}, t):~||{\bf w}||_{2}\leq 1,~A^{\intercal}{\bf w} &gt; 0} \frac{1}{\displaystyle \min_{{\bf a} \in A} \langle {\bf a}, {\bf w} \rangle} \\
&amp;= \left(\frac{1}{\displaystyle \max_{{\bf w}:~||{\bf w}||_{2} \leq 1} \min_{{\bf a} \in A} \langle {\bf a}, {\bf w} \rangle}\right)^{2}
\end{aligned}
\]</span></p>
<p>Where the margin <span class="math inline">\(\gamma\)</span> is <span class="math inline">\(\displaystyle \max_{{\bf w}:~||{\bf w}||_{2} \leq 1} \min_{{\bf a} \in A} \langle {\bf a}, {\bf w} \rangle\)</span></p>
<h3 id="what-does-the-bound-mean">What Does the Bound Mean</h3>
<hr>
<p><strong>Cor.</strong><br>
Let <span class="math inline">\(\delta = 0\)</span> and <span class="math inline">\({\bf w}_{0} = {\bf 0}\)</span>. Then perceptron converges after at most <span class="math inline">\((R/\gamma)^{2}\)</span> steps, where <span class="math display">\[
R = \max_{i} ||A_{:i}||_{2},~\gamma = \max_{{\bf w}:~||{\bf w}||_{2} \leq 1} \min_{i} \langle {\bf w}, A_{:i} \rangle
\]</span></p>
<hr>
<p>Treating <span class="math inline">\(R\)</span> and <span class="math inline">\(\gamma\)</span> as constants, then the number of mistakes are independent of <span class="math inline">\(n\)</span> and <span class="math inline">\(d\)</span>!.<br>
Otherwise may need exponential time… Can we <em>improve</em> the bound?</p>
<h4 id="but">But</h4>
<p>Pro: the larger the margin, the faster perceptron converges.</p>
<p>Con: But perceptron stops at an arbitrary linear separator…</p>
<p>Which one do you prefer? <span class="math display">\[
\min_{A^{\intercal}{\bf w} \geq 1} \frac{1}{2}||{\bf w}||_{2}^{2}
\approx
\min_{y_{i}(\langle {\bf w}, {\bf x}^{i} \rangle + b) \geq 1} \frac{1}{2}||{\bf w}||_{2}^{2}
\]</span></p>
<h3 id="what-if-non-separable">What If Non-Separable</h3>
<p>Find a better feature representation.</p>
<p>Use a deeper model.</p>
<p>Soft margin <span class="math display">\[
\forall {\bf w}^{*}, \forall\gamma &gt; 0,~\text{and}~\forall{\bf a} \in {\bf A}: \langle {\bf a}, {\bf w}^{*} \rangle \geq \gamma - (\gamma - \langle {\bf a}, {\bf w}^{*} \rangle) +
\]</span></p>
<h3 id="perceptron-boundedness-theorem">Perceptron Boundedness Theorem</h3>
<p>Perceptron convergence requires the <em>existence</em> of a separating hyperplane.<br>
    How to check this assumption in practice?<br>
    What if it fails? (It will)</p>
<hr>
<p><strong>Thm</strong> (Minsky and Papert’67; Block and Levin’70)<br>
The iterates of the perceptron algorithm are always bounded. In particular, if there is no separating hyperplane, then perceptron cycles.</p>
<hr>
<h3 id="when-to-stop-perceptron">When to Stop Perceptron</h3>
<p>Online learning: never.</p>
<p>Batch Learning</p>
<ul>
<li>Maximum number of iteration reached or run out of time</li>
<li>Training error stops changing</li>
<li>Validation error stops decreasing</li>
<li>Weights stopped changing much, if using a diminishing step size <span class="math inline">\(\eta_{t}\)</span>, i.e., <span class="math inline">\({\bf w}_{t+1} \leftarrow {\bf w}_{t} + \eta_{t}y_{i}{\bf x}^{i}\)</span></li>
</ul>
<h3 id="multi-class-perceptron">Multi-class Perceptron</h3>
<p>One vs. all</p>
<ul>
<li>Class c as positive</li>
<li>All other classes as negative</li>
<li>Highest activation wins: <span class="math inline">\(\text{pred} = \text{argmax}_{c}{\bf w}_{c}^{\intercal}{\bf x}\)</span></li>
</ul>
<p>One vs. one</p>
<ul>
<li>Class c as positive</li>
<li>Class c’ as negative</li>
<li>Voting</li>
</ul>
<h3 id="the-winnow-algorithm-littlestone98">The Winnow Algorithm (Littlestone’98)</h3>
<p>Input: <span class="math inline">\(A\in \mathbb{R}^{(d&#39;)\times n}\)</span>, threshold <span class="math inline">\(\delta \geq 0\)</span>, step size <span class="math inline">\(\eta &gt; 0\)</span>, initialize <span class="math inline">\({\bf w}_{0} \in \color{red}{int}\Delta_{d&#39;-1}\)</span></p>
<ol style="list-style-type: decimal">
<li><strong>repeat</strong></li>
<li>    select some column <span class="math inline">\({\bf a}\)</span> of <span class="math inline">\(A\)</span></li>
<li>    <strong>if</strong> <span class="math inline">\(\langle {\bf a}, {\bf w_{t}} \rangle \leq \delta\)</span> <strong>then</strong></li>
<li>        <span class="math inline">\({\bf w}_{t+1} = {\bf w}_{t} \odot \exp(\eta {\bf a})\)</span> <code>// update only when making a mistake</code></li>
<li>        <span class="math inline">\({\bf w}_{t+1} \leftarrow \frac{{\bf w}_{t+1}}{||{\bf w}_{t+1}||_{1}}\)</span> <code>// normalize</code></li>
<li>        <span class="math inline">\(t \leftarrow t + 1\)</span></li>
<li><strong>until</strong> <span class="math inline">\(convergence\)</span></li>
</ol>
<p>Multiplicative vs. additive</p>
<hr>
<p><strong>Thm.</strong> (Littlestone’88)<br>
Assume there exists some <em>non-negative</em> <span class="math inline">\({\bf w}\)</span> such that <span class="math inline">\(A^{\intercal}{\bf w} &gt; 0\)</span>, then winnow converges to some <span class="math inline">\({\bf w}^{*}\)</span>. If each column of <span class="math inline">\(A\)</span> is selected indefinitely often, then <span class="math inline">\(A^{\intercal}{\bf w}^{*} &gt; \delta\)</span>.</p>
<hr>
<h2 id="lecture-02-linear-regression-_september-14-2017_">Lecture 02: Linear Regression, <em>September 14, 2017</em></h2>
<h3 id="how-much-should-i-bid-for">How Much Should I Bid For</h3>
<ul>
<li>Interpolation vs. Extrapolation</li>
<li>Linear vs. Non-linear</li>
</ul>
<h4 id="laffer-curve">Laffer Curve</h4>
<p>A curve that represents the relationship between tax revenue and tax rate.<br>
A downward parabola curve, with an revenue maximizing tax rate <span class="math inline">\(t^{*}\)</span>.</p>
<h3 id="regression">Regression</h3>
<p>Given a pair <span class="math inline">\((X, Y)\)</span>, find function <span class="math inline">\(f\)</span> such that <span class="math display">\[f(X) \approx Y\]</span></p>
<ul>
<li><span class="math inline">\(X\)</span>: feature vector, <span class="math inline">\(d\)</span>-dimensional real vector</li>
<li><span class="math inline">\(Y\)</span>: response, <span class="math inline">\(m\)</span>-dimensional real vector</li>
</ul>
<p>Two problems:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\((X, Y)\)</span> is uncertain: samples from an <em>unknown distribution</em></li>
<li>How to measure the error: need a <em>loss</em> function</li>
</ol>
<h3 id="risk-minimization">Risk Minimization</h3>
<p>Minimize the expected loss, aka <em>risk</em> <span class="math display">\[
\begin{aligned}
\min_{f: X \rightarrow Y} \underbrace{{\bf E}[L(f(X), Y)]}_{} &amp; &amp; L: y \times y \rightarrow {\bf R}_{+} \\
&amp; &amp; L(y, y) \equiv 0
\end{aligned}
\]</span></p>
<p>Which loss to use?<br>
Not always clear, convenience dominates for now</p>
<p>Least squares: <span class="math inline">\(\displaystyle \min_{f} {\bf E}||f(X) - Y||_{2}^{2}\)</span></p>
<h3 id="the-regression-function">The Regression Function</h3>
<p><span class="math display">\[{\bf E}||f(X) - Y||_{2}^{2} = {\bf E}||f(X) - {\bf E}(Y|X)||_{2}^{2} + \underbrace{{\bf E}||{\bf E}(Y|X) - Y||_{2}^{2}}_{\text{Inherent noise variance}}\]</span></p>
<p>Regression function: <span class="math display">\[f^{*}(X) = m(X) = {\bf E}(Y|X)\]</span> where <span class="math inline">\(m\)</span> is the mean function.</p>
<p>Many ways to estimate <span class="math inline">\(m(X)\)</span></p>
<p>Simplest: Let’s assume it is linear (affine)!</p>
<h3 id="linear-regression">Linear Regression</h3>
<p><em>Assumption</em>: <span class="math inline">\(m(X) = {\bf E}(Y|X) = XA + {\bf b}\)</span></p>
<p>Goal: <span class="math inline">\(\displaystyle \min_{A, {\bf b}} {\bf E}||XA + {\bf b} - Y||_{2}^{2}\)</span><br>
Where <span class="math inline">\({\bf E}\)</span> is an unknown distribution.</p>
<p>Law of Large Numbers: <span class="math inline">\(\displaystyle \frac{1}{n}\sum_{i = 1}^{n}Z_{i} \rightarrow {\bf E}(Z)\)</span></p>
<p>Reality:<br>
    <span class="math inline">\(\displaystyle \min_{A, {\bf b}}\underbrace{\frac{1}{n}\sum_{i=1}^{n}||X_{i}A + {\bf b} - Y_{i}||_{2}^{2}}_{\text{empirical risk}}\)</span></p>
<h4 id="simplification-again">Simplification, again</h4>
<p><span class="math display">\[
\begin{aligned}
&amp; \min_{A, {\bf b}} \frac{1}{n}\sum_{i=1}^{n}||X_{i}A + {\bf b} - Y_{i}||_{2}^{2} \\
\Rightarrow W \leftarrow \begin{pmatrix} A \\ {\bf b} \end{pmatrix} \\
X_{i} \leftarrow (X_{i}, 1) \\
&amp; \min_{W} \frac{1}{n}\sum_{i=1}^{n}||X_{i}W - Y_{i}||_{2}^{2} \\
\Rightarrow X = \begin{bmatrix} X_{1} \\ X_{2} \\ \vdots \\ X_{n} \end{bmatrix}
Y = \begin{bmatrix} Y_{1} \\ Y_{2} \\ \vdots \\ Y_{n} \end{bmatrix} \\
&amp; \min_{W \in {\bf R}^{(d+1)\times m}}||XW - Y||_{F}^{2}
\end{aligned}
\]</span></p>
<h4 id="finally">Finally</h4>
<p><span class="math display">\[
\min_{W \in {\bf R}^{(d+1)\times m}}\overbrace{||XW - Y||_{F}^{2}}^{\text{sum of square residuals}}
\]</span></p>
<p><span class="math inline">\(XW \rightarrow\)</span> hyperplane (again!), parameterized by <span class="math inline">\(W\)</span>.<br>
<span class="math inline">\(Y \rightarrow\)</span> true responses.</p>
<p>In transpose form: <span class="math inline">\(WX^{\intercal} - Y^{\intercal}\)</span></p>
<h4 id="why-least-squares">Why Least Squares</h4>
<hr>
<p><strong>Thm.</strong> (Sondermann’86; Friedland and Torokhti’07; Yu and Schuurmans’11)<br>
Among all minimizers of <span class="math inline">\(\displaystyle \min_{W}||AWB - C||_{F}\)</span>, <span class="math inline">\(W=A^{+}CB^{+}\)</span> is the one that has minimal F-norm.</p>
<hr>
<p><strong>Pseudo-inverse</strong> <span class="math inline">\(A^{+}\)</span> is the <em>unqiue</em> matrix <span class="math inline">\(G\)</span> such that <span class="math display">\[AGA = A,~GAG = G,~(AG)^{\intercal} = AG,~(GA)^{\intercal} = GA\]</span></p>
<p><strong>Singular Value Decomposition</strong> <span class="math inline">\(A = USV^{\intercal}\)</span>, <span class="math inline">\(A^{+} = VS^{-1}U^{\intercal}\)</span></p>
<h5 id="optimizatino-detour">Optimizatino Detour</h5>
<p><span class="math display">\[\min_{x} f(x)\]</span></p>
<hr>
<p><strong>Format’s Theorem</strong><br>
Necessarily <span class="math inline">\(Df(x) = 0\)</span></p>
<hr>
<p>(Préchet) <strong>Derivative at <span class="math inline">\(x\)</span></strong>. <span class="math display">\[\lim_{0\neq\delta\rightarrow 0}\frac{|f(x + \delta) - f(x) - Df(x)\delta|}{|\delta|} = 0\]</span></p>
<p>Example.<br>
<span class="math inline">\(f({\bf x}) = {\bf x}^{\intercal}A{\bf x} + {\bf x}^{\intercal}{\bf b} + c\)</span><br>
<span class="math inline">\(Df({\bf x}) = (A + A^{\intercal}){\bf x} + {\bf b}\)</span></p>
<h4 id="solving-least-squares">Solving Least Squares</h4>
<p>Note, <span class="math inline">\(m = 1\)</span> <span class="math inline">\(X \in \mathbb{R}^{n \times (d+1)}\)</span><br>
<span class="math inline">\(W \in \mathbb{R}^{(d+1)\times m}\)</span><br>
<span class="math inline">\(Y \in \mathbb{R}^{r \times m}\)</span><br>
<span class="math inline">\(X^{\intercal}XW \in \mathbb{R}^{(d+1)\times m}\)</span><br>
<span class="math inline">\(A^{\intercal}Y \in \mathbb{R}^{(d+1)\times m}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\min_{W \in {\bf R}^{(d+1)\times m}}||XW - Y||_{F}^{2} &amp;= W^{\intercal}(X^{\intercal}X)W - 2W^{\intercal}X^{\intercal}Y + Y^{\intercal}{Y} \\
&amp;\Downarrow \\
X^{\intercal}XW &amp;= X^{\intercal}Y ~~~ \text{Normal Equation}
\end{aligned}
\]</span></p>
<p><span class="math inline">\(X^{\intercal}X\)</span> may not be invertible, but there is always a solution. Even invertible, <em>never ever compute</em> <span class="math inline">\(W = (X^{\intercal}X)^{-1}X^{\intercal}Y\)</span> !<br>
Instead, solve the linear system.</p>
<h4 id="prediction">Prediction</h4>
<p>Once we have <span class="math inline">\(W\)</span>, we can predict <span class="math display">\[\hat{Y} = X_{\text{test}}W\]</span></p>
<p>How to evaluate? <span class="math display">\[(Y_{\text{test}} - \hat{Y})^{2}\]</span></p>
<p>Sometimes we evaluate using a different</p>
<h3 id="robustness">Robustness</h3>
<p><strong>Huber’s Loss</strong> <span class="math display">\[
H(\hat{y}, y) =
\begin{cases}
\frac{1}{2}(\hat{y} - y^{2}), &amp; |\hat{y} - y| \leq \delta \\
\delta(|\hat{y} - y| - \frac{1}{2}\delta), &amp; |\hat{y} - y| \geq \delta
\end{cases}
\]</span></p>
<h3 id="multi-task-learning">Multi-task Learning</h3>
<p><span class="math display">\[X^{\intercal}XW = X^{\intercal}Y\]</span></p>
<p>Everything we’ve shown still holds if <span class="math inline">\(Y\)</span> is <span class="math inline">\(m\)</span>-dimensional.<br>
But, can solve each column of <span class="math inline">\(Y\)</span> <em>independently</em> <span class="math display">\[X^{\intercal}XW_{;j} = X^{\intercal}Y_{:j}\]</span> Things are more interesting if we had <em>regularization</em>.</p>
<h3 id="regularization">Regularization</h3>
<h4 id="iii-posedness">III-Posedness</h4>
<p>Let <span class="math inline">\(x_{1} = 0\)</span>, <span class="math inline">\(x_{2} = \epsilon\)</span>, <span class="math inline">\(y_{1} = 1\)</span>, <span class="math inline">\(y_{2} = -1\)</span></p>
<p><span class="math display">\[
X =
\begin{bmatrix}
0 &amp; 1 \\
\epsilon &amp; 1
\end{bmatrix}
~~ Y =
\begin{bmatrix}
1 \\
-1
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
w = X^{-1}y =
\begin{bmatrix}
\frac{2}{\epsilon} \\
1
\end{bmatrix}
\]</span></p>
<p>Slight disturbance leads to chaotic behaviour.</p>
<h4 id="tiknohov-regularization-hoerl-and-kennard70">Tiknohov Regularization (Hoerl and Kennard’70)</h4>
<p>Ridge regression: <span class="math display">\[
\min_{W \in {\bf R}^{(d+1)\times m}} ||XW - Y||_{F}^{2} + \lambda||W||_{F}^{2}
\]</span> <span class="math display">\[\Longrightarrow (X^{\intercal}X + \lambda I)W = X^{\intercal}Y\]</span> where <span class="math inline">\(\lambda\)</span> is the reg. constant (hyperparameter).</p>
<p>With positive lambda, slight perturbation in input leads to <em>proportional</em> (with regards to <span class="math inline">\(\frac{1}{\lambda}\)</span>) perturbation in output.</p>
<h4 id="data-augmentation">Data Augmentation</h4>
<p><span class="math display">\[
\begin{aligned}
\min_{W \in {\bf R}^{(d+1)\times m}} &amp;||XW - Y||_{F}^{2} + \lambda||W||_{F}^{2} \\
&amp;\Updownarrow \\
\min_{W \in {\bf R}^{(d+1)\times m}} &amp;||\widetilde{X}W - \widetilde{Y}||_{F}^{2}
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\widetilde{X} =
\begin{bmatrix}
X \\
\sqrt{\lambda}I
\end{bmatrix}
~~ \widetilde{Y} =
\begin{bmatrix}
Y \\
0
\end{bmatrix}
\]</span></p>
<h4 id="sparsity">Sparsity</h4>
<p>Ridge regression weight is always dense,</p>
<ul>
<li>Computationally heavy</li>
<li>Interpretationally cumbersome</li>
</ul>
<p>Lasso (Tibshirani’96) <span class="math display">\[
\min_{||W||_{1} \leq C}||XW - Y||_{F}^{2}
\]</span></p>
<h4 id="regularization-vs-constraint">Regularization vs. Constraint</h4>
<p><span class="math display">\[
\begin{aligned}
\min_{W}\ell(W) &amp;+ \lambda \cdot r(W) &amp;&amp; \text{computationally appealing}\\
\text{always true } \Downarrow ~&amp;~ \Uparrow \text{mild conditions} \\
\min_{W:r(W) \leq C} &amp;\ell(W) &amp;&amp; \text{theoretically appealing}
\end{aligned}
\]</span></p>
<h3 id="cross-validation">Cross-Validation</h3>
<p>See written notes.</p>
<h2 id="lecture-03-logistic-regression-_september-19-2017_">Lecture 03: Logistic Regression, <em>September 19, 2017</em></h2>
<p>Often times in Machine Learning, it’s either <em>Regression</em> or <em>Classification</em>.</p>
<h3 id="classification">Classification</h3>
<p>Higher “complexity” datasets need higher “capacity” models.<br>
Formally, higher <em>VC-Dimensionality</em>.</p>
<h3 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h3>
<p><span class="math display">\[w := w - \eta\bigtriangledown Q_{i}(w)\]</span></p>
<h3 id="perceptron-review">Perceptron Review</h3>
<p>Limited in capacity, cannot separate if data is mixed.<br>
It also finds the first line that separates (classifies) the data regardless of how it touches the support vectors.</p>
<h3 id="bernoulli-model">Bernoulli Model</h3>
<p>Let <span class="math inline">\(P(Y=1|X={\bf x}) = p({\bf x}; {\bf w})\)</span>, parameterized by <span class="math inline">\(\bf w\)</span>.</p>
<p>Conditional likelihood on <span class="math inline">\(\{({\bf x_{1}}, y_{1}), \ldots, ({\bf x_{n}}, y_{n})\}\)</span>: <span class="math display">\[P(Y_{1} = y_{1}, \ldots, Y_{n} = y_{n}|X_{1} = {\bf x_{1}}, \ldots, X_{n} = {\bf x_{n}})\]</span> Simplifies if independence holds, (assuming <span class="math inline">\(y_{1}\)</span> is <span class="math inline">\(\{0, 1\}\)</span>-valued) <span class="math display">\[\prod_{i=1}^{n}P(Y_{i}=y_{i}|X_{i} = {\bf x_{i}}) = \prod_{i=1}^{n}p({\bf x_{i}}; {\bf w})^{y_{i}}(1 - p({\bf x_{i}}; {\bf w}))^{1 - y_{i}}\]</span></p>
<h4 id="naive-solution">Naive Solution</h4>
<h2 id="lecture-04-multi-layer-perceptron_september-21-2017_">Lecture 04: Multi-Layer Perceptron,<em>September 21, 2017</em></h2>
<p>Perceptron: finds a dividing hyperplane to classify a data set into two.</p>
<p>Logistic Regression: provides a soft confidence metrics to classify data; not hard dividing hyperplane.<br>
Still a linear transformation.</p>
<h3 id="failure-of-perceptron">Failure of Perceptron</h3>
<h4 id="the-xor-problem">The XOR Problem</h4>
<p>$$X={(0,0), (0,1), (1,0), (1,1)},~y={}</p>
<h5 id="fixing-the-problem">Fixing the Problem</h5>
<p>The perceptron model (hyperplanes) underfits the data (<code>xor</code> function).</p>
<p>Fix representation, richer model.<br>
Fix model, richer representation.</p>
<p>Neural Network: still use hyperplane, but <em>learn</em> representation</p>
<h3 id="neural-network">Neural Network</h3>
<h4 id="two-layer-perceptron">Two-Layer Perceptron</h4>
<p>Nonlinearity: non-linear function.</p>
<p><span class="math display">\[{\bf z} = U{\bf x} + {\bf c} ~~\text{1\textsuperscript{st}}\]</span> <span class="math display">\[{\bf h} = f({\bf z})\]</span> <span class="math display">\[\hat{y} = \langle {\bf h}, {\bf w} \rangle + b\]</span></p>
<h4 id="weights-training">Weights Training</h4>
<p><span class="math display">\[\hat{y} = q(x; \theta)\]</span></p>
<p><span class="math inline">\(\hat{y}\)</span> is the prediction.<br>
<span class="math inline">\(q\)</span> is the neural network.<br>
<span class="math inline">\(x; \theta\)</span> is <span class="math inline">\(x\)</span> parameterized by <span class="math inline">\(\theta\)</span>.<br>
<span class="math inline">\(\theta\)</span> is all of the weights.</p>
<h4 id="gradient-descent">Gradient Descent</h4>
<p><span class="math display">\[\min_{\Theta}L(\Theta) := \frac{1}{n}\sum_{i=1}^{n}\ell\left[{\bf q}({\bf x}_{i}; \Theta), {\bf y}_{i}\right]\]</span></p>
<p><span class="math inline">\(\displaystyle \Theta_{t+1} \leftarrow \Theta_{t} - \eta_{t}\bigtriangledown L(\Theta_{t})\)</span></p>
<p>Need to subtract the gradient because we are minimizing <span class="math inline">\(L(\Theta)\)</span>.</p>
<h4 id="activation-function">Activation Function</h4>
<p>Sigmoid <span class="math display">\[f(t)=\sigma(t) = \frac{1}{1+e^{-t}} = \frac{e^{t}}{1+e^{t}}\]</span> Great if we want a number between 0 and 1; often used to classification.<br>
Saturate curve.</p>
<p>Tanh <span class="math display">\[f(t) = \tanh(t) = \frac{e^{t} - e^{-t}}{e^{t} + e^{-t}}\]</span> Great if we we want a number between -1 and 1.<br>
Smooth curve.</p>
<p>Rectified Linear <span class="math display">\[f(t) = t_{+} := \max(t, 0)\]</span> Non-smooth curve.</p>
<h3 id="backpropagation">Backpropagation</h3>
<p><em>Chain rule</em> for derivatives: <span class="math display">\[f(x) = g[h(x)] \rightarrow f&#39;(x) = g&#39;[h(x)]\times h&#39;(x)\]</span></p>
<p>Efficiently computes the derivative in Neural Network.</p>
<p>Two passes; complexity</p>
<h4 id="backprop-through-a-computation-graph">Backprop Through a Computation Graph</h4>
<h3 id="code-basics">Code Basics</h3>
<ul>
<li>Shape <span class="math inline">\(\rightarrow\)</span> shape of the matrix
<ul>
<li><code>a = np.array([[1,2],[3,4],[5,7])</code></li>
<li><code>np.shape(a)</code> <span class="math inline">\(\Rightarrow\)</span> <code>(3, 2)</code></li>
</ul></li>
</ul>
<h4 id="types-of-nodes-in-computation-graph">Types of Nodes in Computation Graph</h4>
<ul>
<li>Constants</li>
<li>Variables</li>
<li>Placeholders</li>
<li>Unary Operations</li>
<li>Binary Operations</li>
</ul>
<h2 id="lecture-05-k-nearest-neighbours-_september-26-28-2017_">Lecture 05: K-nearest Neighbours, <em>September 26 &amp; 28, 2017</em></h2>
<h3 id="classification-revisited">Classification Revisited</h3>
<p><span class="math display">\[\hat{y} = \text{sign}({\bf x}^{\intercal}{\bf w} + b)\]</span></p>
<p>Decision boundary: <span class="math inline">\({\bf x}^{\intercal}{\bf w} + b = 0\)</span></p>
<p>Parametric: finite-dim <span class="math inline">\(\bf w\)</span>.</p>
<p>Non-parametric: non specific form (or inf-dim <span class="math inline">\(\bf w\)</span>).</p>
<h3 id="nearest-neighbour">1-Nearest Neighbour</h3>
<p>Store training set <span class="math inline">\((X, {\bf y})\)</span></p>
<p>For query (test point) <span class="math inline">\({\bf x}&#39;\)</span></p>
<ul>
<li>Find nearest point <span class="math inline">\(\bf x\)</span> in <span class="math inline">\(X\)</span></li>
<li>Predict <span class="math inline">\(y&#39; = y({\bf x})\)</span></li>
</ul>
<p>Compute the distance and find the nearest neightbour and predict based on the nearest neighbour.</p>
<h4 id="defining-nearest">Defining “Nearest”</h4>
<p>Need to measure distance or similarity.</p>
<p><span class="math inline">\(d: X \times X \rightarrow R_{+}\)</span> such that</p>
<ul>
<li><em>symmetry</em>: <span class="math inline">\(d(x, x&#39;) = d(x&#39;, x)\)</span></li>
<li><em>definite</em>: <span class="math inline">\(d(x, x&#39;) = 0\)</span> iff. <span class="math inline">\(x = x&#39;\)</span></li>
<li><em>triangle inequality</em>: <span class="math inline">\(d(a, b) \leq d(a, c) + d(c, b)\)</span></li>
</ul>
<p>L<sub>p</sub> distance: <span class="math inline">\(d_{p}(x, x&#39;) = ||x - x&#39;||_{p}\)</span></p>
<ul>
<li><span class="math inline">\(p = 2\)</span>: Euclidean distsance</li>
<li><span class="math inline">\(p = 1\)</span>: Manhattan distance</li>
<li><span class="math inline">\(p = inf\)</span>: Chebyshev distance
<ul>
<li><span class="math inline">\(||\overrightarrow{z}||_{\inf}\)</span></li>
</ul></li>
</ul>
<h4 id="complexity-of-1-nn">Complexity of 1-NN</h4>
<p>Training: 0… but <span class="math inline">\(O(nd)\)</span> space</p>
<p>Testing: <span class="math inline">\(O(nd)\)</span> for each query point</p>
<ul>
<li><span class="math inline">\(n\)</span>: number of training samples</li>
<li><span class="math inline">\(d\)</span>: number of features</li>
</ul>
<h3 id="normalization">Normalization</h3>
<p>Usually, for each feature, subtract the mean and divide by standard deviation.<br>
Or, equivalently, use a different distance metric.</p>
<h3 id="learning-the-metric">Learning the Metric</h3>
<p>Manhalanobias Distance, <span class="math display">\[d_{M}({\bf x}, {\bf x&#39;}) = \sqrt{({\bf x} - {\bf x&#39;}^{\intercal}M({\bf x} - {\bf x&#39;})},~~~ M \in \mathbb{S}^{d}_{+}\]</span></p>
<p>Or equivalently let <span class="math inline">\(M = LL^{\intercal}\)</span>, <span class="math inline">\(L \in \mathbb{R}^{d\times h}\)</span>.<br>
First, perform linear transformation <span class="math inline">\({\bf x} \mapsto L^{\intercal}{\bf x}\)</span>.<br>
Then use the usual Euclidean distsance, <span class="math display">\[\min_{M \in \mathbb{S}^{d}_{+}} f(M) \text{ such that } d_{M}({\bf x}, {\bf x&#39;}) \text{ is small iff. } y = y&#39;\]</span></p>
<h3 id="k-nn">K-NN</h3>
<p>Store training set <span class="math inline">\((X, {\bf y})\)</span>.</p>
<p>For query (test point) <span class="math inline">\({\bf x&#39;}\)</span></p>
<ul>
<li>Find <span class="math inline">\(k\)</span> nearest points <span class="math inline">\({\bf x}_{1}, {\bf x}_{2}, \ldots, {\bf x}_{k}\)</span> in <span class="math inline">\(X\)</span></li>
<li>Predict <span class="math inline">\(y&#39; = y({\bf x}_{1}, {\bf x}_{2}, \ldots, {\bf x}_{k})\)</span>
<ul>
<li>Usually a majority vote among the labels for <span class="math inline">\({\bf x}_{1}, {\bf x}_{2}, \ldots, {\bf x}_{k}\)</span></li>
<li>Say <span class="math inline">\(y_{1} = 1, y_{2} = 1, y_{3} = -1, y_{4}, y_{5} = -1 \rightarrow y&#39; = 1\)</span></li>
</ul></li>
<li>Test complexity: <span class="math inline">\(O(nd)\)</span></li>
</ul>
<h3 id="bayes-rule-under-iid-assumption">Bayes Rule Under iid Assumption</h3>
<p>iid: independent and indentical distribution.</p>
<p>Bayes error, <span class="math display">\[P^{*} = \min_{f:X\rightarrow\{\pm 1\}}{\bf P}(f(X) \neq Y)\]</span></p>
<p>Indicator function of a subset A of a set X is a function <span class="math display">\[\displaystyle {\bf 1}_{A} = X \rightarrow \{0, 1\}\]</span> defined as <span class="math display">\[{\bf 1}_{A}(x) := \begin{cases}1 &amp;\text{if }x \in A \\0 &amp;\text{if }x \not\in A\end{cases}\]</span></p>
<p>Bayes rule derivation, <span class="math display">\[\begin{aligned}Pr(f(X) \neq Y) &amp;= 1 - Pr(f(X) = Y) \\ &amp;= 1 - Pr(f(X) = Y, Y = 1) - Pr(f(X) = Y, Y = 1) \\ &amp;= 1 - Pr(f(X) = -1) - Pr(f(X) = 1) \\ &amp;=1 - E[Pr(f(X) = 1, Y = 1 | X)] - E[Pr(f(X) = -1, Y = -1 | X)] \\ &amp;= 1 - E[{\bf 1}_{f(X) = 1}\cdot \underbrace{Pr(Y=1|X)]}_{\eta(x)} - E[{\bf 1}_{f(X)=-1}\cdot \underbrace{Pr(Y=-1|X)}_{1 - \eta(x)}] \\ &amp;= E[\eta(X) + {\bf 1}_{f(X)=1)(1 - 2\eta(X))}]\end{aligned}\]</span></p>
<p><span class="math inline">\(f^{*}(X) = 1 \text{ iff. } \eta(X) \geq \frac{1}{2}\)</span></p>
<h3 id="multi-class">Multi-class</h3>
<p><span class="math display">\[f^{*}(X) = \underset{m=1,\ldots,c}{\operatorname{argmax}}~{\bf P}(Y = m|X)\]</span> <span class="math display">\[P^{*} = {\bf E}[1 - \max_{m = 1,\ldots, c}~{\bf P}(Y = m | X)]\]</span> This is the <em>best</em> we can do <em>even when we know the distribution of</em> <span class="math inline">\((X, Y)\)</span>.</p>
<p>How big can <span class="math inline">\(P^{*}\)</span> be?</p>
<h4 id="at-most-twice-worse-cover-hart-67">At Most Twice Worse (Cover &amp; Hart’ 67)</h4>
<p>See written notes.</p>
<h3 id="nn-vs-k-nn">1-NN vs. k-NN</h3>
<p>error(1NN) = <span class="math inline">\(\frac{1}{2^{n}}\)</span></p>
<p>error(kNN) for <span class="math inline">\(k = 2t +1\)</span>: <span class="math inline">\(\displaystyle \frac{1}{2^{n}}\sum_{i=0}^{t}\begin{pmatrix}n \\ i\end{pmatrix}\)</span></p>
<h3 id="curse-of-dimensionality">Curse of Dimensionality</h3>
<hr>
<p><strong>Thm.</strong><br>
For any <span class="math inline">\(c &gt; 1\)</span> and <em>any</em> learning algorithm <span class="math inline">\(L\)</span>, there exists a distribution over <span class="math inline">\([0, 1]^{d} \times \{0, 1\}\)</span> such that the Bayes error is <span class="math inline">\(0\)</span> but for sample size <span class="math inline">\(n \leq \frac{(c+1)^{d}}{2}\)</span>, the error of the rule <span class="math inline">\(L\)</span> is greater than <span class="math inline">\(\frac{1}{4}\)</span>.</p>
<hr>
<p>k-NN is effective when we have <em>many</em> training samples.</p>
<p>Dimensionality reduction may be helpful.</p>
<h3 id="locally-linear-embedding-saul-roweis00">Locally Linear Embedding (Saul &amp; Roweis’00)</h3>
<p><span class="math display">\[min_{W1=1}||{\bf x}_{i} - \sum_{j}W_{ij}{\bf x}_{j}||^{2}_{2}\]</span> <span class="math display">\[\min_{Z} \sum_{i}||{\bf z}_{i} - \sum_{j}W_{ij}{\bf z}_{j}||^{2}_{2}\]</span></p>
<h3 id="k-nn-for-regression">k-NN for Regression</h3>
<p>Training: store training set <span class="math inline">\((X, y)\)</span>.<br>
Query <span class="math inline">\(x&#39;\)</span></p>
<ul>
<li>Find k-NNs <span class="math inline">\({\bf x}_{1}, {\bf x}_{2}, \ldots, {\bf x}_{k}\)</span> in <span class="math inline">\(X\)</span></li>
<li>Output <span class="math inline">\(y&#39; = y({\bf x}_{1}, {\bf x}_{2}, \ldots, {\bf x}_{k}) = \frac{y_{1} + y_{2} + \cdots + y_{k}}{k}\)</span></li>
</ul>
<h3 id="why-1-nn-works">Why 1-NN Works</h3>
<p>Let <span class="math inline">\(X_{1}\)</span> be the NN for query <span class="math inline">\(X\)</span>.<br>
Assuming <span class="math inline">\(P(Y_{1}=j|X_{1}) - P(Y=j|X) \to 0\)</span> as <span class="math inline">\(n \to\)</span> inf.<br>
Thus, the error of 1-NN, <span class="math display">\[\begin{aligned}P(Y_{1}\neq Y | X) &amp;= \sum_{i\neq j}P(Y=i|X)E[P(Y_{1}=j|X_{1})|X] \\
&amp;\rightarrow \sum_{i\neq j}P(Y=i|X)P(Y=j|X) \\
&amp;= 1 - \sum_{i}P^{2}(Y=i|X)\end{aligned}\]</span></p>
<p>Two conditions: <span class="math inline">\(\sum_{i}P(Y=i|X) = 1\)</span> and <span class="math inline">\(\max_{i}P(Y=i|X) = 1 - P^{*}\)</span></p>
<h3 id="are-all-neighbours-equal">Are All Neighbours Equal</h3>
<p><span class="math display">\[y&#39; = \frac{y_{1} + y_{2} + \cdots + y_{k}}{k} \iff y&#39; = \underset{y}{\operatorname{argmin}}\sum_{j=1}^{n}1_{j\in kNN(x&#39;)}(y - y_{j})^{2}\]</span> Where <span class="math inline">\(j \in kNN(x&#39;)\)</span> is the Parzen Window.</p>
<p>More generally, can weight the neighbours <span class="math display">\[y&#39; = \frac{\sum_{i=1}w_{i}y_{i}}{\sum_{i=1}^{n}w_{i}} \iff y&#39; = \underset{y}{\operatorname{argmin}}\sum_{j=1}^{n}w_{i}(y - y_{j})^{2}\]</span></p>
<p>For instance, <span class="math inline">\(w_{i} = \exp(-d(x&#39;, x_{i})/\sigma)\)</span></p>
<h3 id="density-estimation">Density Estimation</h3>
<p>Given iid samples <span class="math inline">\(X_{1}, \ldots, X_{n}\)</span>, estimate density function <span class="math inline">\(X \sim p(x)\)</span>.<br>
Kernel: function <span class="math inline">\(K: R \to R\)</span> with integral 1.<br>
Kernel density estimation <span class="math display">\[\hat{p}(x) = \frac{1}{nh}\sum_{i=1}^{n}K\left(\frac{X_{i}-x}{h}\right)\]</span> where <span class="math inline">\(h\)</span> is the bandwidth.</p>
<h4 id="nonparametric-regression">Nonparametric Regression</h4>
<p>Recall the regression function, <span class="math display">\[m(x) = {\bf E}(Y|X) = \int f\frac{p(x,y)}{p(x)}\text{d}y\]</span></p>
<p>Plugin estimator <span class="math display">\[\hat{p}(x) = \frac{1}{nh}\sum_{i=1}^{n}K\left(\frac{X_{i}-x}{h}\right)\]</span> <span class="math display">\[\hat{p}(x, y) = \frac{1}{nh^{2}}\sum_{i=1}^{n}K\left(\frac{X_{i}-x}{h}\right)K\left(\frac{Y_{i}-y}{h}\right)\]</span></p>
<h2 id="lecture-06-hard-margin-svm-_october-3rd-2017_">Lecture 06: Hard-Margin SVM, <em>October 3rd, 2017</em></h2>
<h3 id="perceptron-revisited">Perceptron Revisited</h3>
<p>Two classes: <span class="math inline">\(y \in \{0, 1\}\)</span><br>
<em>Assuming</em> linear separable, exist <span class="math inline">\({\bf w}\)</span> and <span class="math inline">\(b\)</span> such that for all <span class="math inline">\(i\)</span>, <span class="math inline">\(y_{i}({\bf w}^{\intercal}{\bf x}_{i} + b) &gt; 0\)</span>.<br>
Find <em>any</em> such <span class="math inline">\({\bf w}\)</span> and <span class="math inline">\(b\)</span> <span class="math display">\[\begin{aligned}
&amp; \min_{{\bf w}, b} &amp;&amp; 0 \rightarrow\text{feasibility problem}\\
&amp; s.t. &amp;&amp; \forall i, y_{i}({\bf w}^{\intercal}{\bf x}_{i} + b) &gt; 0
\end{aligned}\]</span></p>
<h3 id="margin">Margin</h3>
<p>Take <em>any</em> linear separating hyperplane <span class="math inline">\(H\)</span> for all <span class="math inline">\(i, y_{i}({\bf w}^{\intercal}{\bf x}_{i}+b) &gt; 0\)</span>.<br>
Move <span class="math inline">\(H\)</span> until it touches some positve point, <span class="math inline">\(H_{1}\)</span>.<br>
Move <span class="math inline">\(H\)</span> until it touches some negative point, <span class="math inline">\(H_{-1}\)</span>.</p>
<p>We get <span class="math inline">\(\text{margin} = \text{dist}(H_{1}, H) \wedge \text{dist}(H_{-1}, H)\)</span></p>

      </div>
      <div class="md-sidebar-toc"><ul>
<li><a href="#cs-489698-introduction-to-machine-learning">CS 489/698 - Introduction to Machine Learning</a>
<ul>
<li><a href="#lecture-00-introduction-_september-07-2017_">Lecture 00: Introduction, <em>September 07, 2017</em></a>
<ul>
<li><a href="#course-overview">Course Overview</a>
<ul>
<li><a href="#what-is-machine-learning">What is Machine Learning</a></li>
<li><a href="#learning-categories">Learning Categories</a>
<ul>
<li><a href="#supervised">Supervised</a></li>
<li><a href="#reinforcement">Reinforcement</a></li>
<li><a href="#unsupervised">Unsupervised</a></li>
</ul>
</li>
<li><a href="#supervised-formally">Supervised, formally</a></li>
</ul>
</li>
<li><a href="#focus-of-ml-research">Focus of ML Research</a></li>
<li><a href="#collaboration-with-focal">Collaboration with focal</a></li>
</ul>
</li>
<li><a href="#lecture-01-perceptron-_september-12-2017_">Lecture 01: Perceptron, <em>September 12, 2017</em></a>
<ul>
<li><a href="#spam-filtering-example">Spam Filtering Example</a></li>
<li><a href="#batch-vs-online">Batch vs. Online</a></li>
<li><a href="#linear-threshold-function">Linear Threshold Function</a></li>
<li><a href="#simplification">Simplification</a></li>
<li><a href="#perceptron-rosenblatt58">Perceptron [Rosenblatt'58]</a></li>
<li><a href="#the-perceptron-algorithm">The Perceptron Algorithm</a>
<ul>
<li><a href="#does-it-work">Does It Work</a></li>
</ul>
</li>
<li><a href="#perception-convergence-theorem">Perception Convergence Theorem</a>
<ul>
<li><a href="#the-margin">The Margin</a></li>
</ul>
</li>
<li><a href="#what-does-the-bound-mean">What Does the Bound Mean</a>
<ul>
<li><a href="#but">But</a></li>
</ul>
</li>
<li><a href="#what-if-non-separable">What If Non-Separable</a></li>
<li><a href="#perceptron-boundedness-theorem">Perceptron Boundedness Theorem</a></li>
<li><a href="#when-to-stop-perceptron">When to Stop Perceptron</a></li>
<li><a href="#multi-class-perceptron">Multi-class Perceptron</a></li>
<li><a href="#the-winnow-algorithm-littlestone98">The Winnow Algorithm (Littlestone'98)</a></li>
</ul>
</li>
<li><a href="#lecture-02-linear-regression-_september-14-2017_">Lecture 02: Linear Regression, <em>September 14, 2017</em></a>
<ul>
<li><a href="#how-much-should-i-bid-for">How Much Should I Bid For</a>
<ul>
<li><a href="#laffer-curve">Laffer Curve</a></li>
</ul>
</li>
<li><a href="#regression">Regression</a></li>
<li><a href="#risk-minimization">Risk Minimization</a></li>
<li><a href="#the-regression-function">The Regression Function</a></li>
<li><a href="#linear-regression">Linear Regression</a>
<ul>
<li><a href="#simplification-again">Simplification, again</a></li>
<li><a href="#finally">Finally</a></li>
<li><a href="#why-least-squares">Why Least Squares</a>
<ul>
<li><a href="#optimizatino-detour">Optimizatino Detour</a></li>
</ul>
</li>
<li><a href="#solving-least-squares">Solving Least Squares</a></li>
<li><a href="#prediction">Prediction</a></li>
</ul>
</li>
<li><a href="#robustness">Robustness</a></li>
<li><a href="#multi-task-learning">Multi-task Learning</a></li>
<li><a href="#regularization">Regularization</a>
<ul>
<li><a href="#iii-posedness">III-Posedness</a></li>
<li><a href="#tiknohov-regularization-hoerl-and-kennard70">Tiknohov Regularization (Hoerl and Kennard'70)</a></li>
<li><a href="#data-augmentation">Data Augmentation</a></li>
<li><a href="#sparsity">Sparsity</a></li>
<li><a href="#regularization-vs-constraint">Regularization vs. Constraint</a></li>
</ul>
</li>
<li><a href="#cross-validation">Cross-Validation</a></li>
</ul>
</li>
<li><a href="#lecture-03-logistic-regression-_september-19-2017_">Lecture 03: Logistic Regression, <em>September 19, 2017</em></a>
<ul>
<li><a href="#classification">Classification</a></li>
<li><a href="#stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</a></li>
<li><a href="#perceptron-review">Perceptron Review</a></li>
<li><a href="#bernoulli-model">Bernoulli Model</a>
<ul>
<li><a href="#naive-solution">Naive Solution</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#lecture-04-multi-layer-perceptron_september-21-2017_">Lecture 04: Multi-Layer Perceptron,<em>September 21, 2017</em></a>
<ul>
<li><a href="#failure-of-perceptron">Failure of Perceptron</a>
<ul>
<li><a href="#the-xor-problem">The XOR Problem</a>
<ul>
<li><a href="#fixing-the-problem">Fixing the Problem</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#neural-network">Neural Network</a>
<ul>
<li><a href="#two-layer-perceptron">Two-Layer Perceptron</a></li>
<li><a href="#weights-training">Weights Training</a></li>
<li><a href="#gradient-descent">Gradient Descent</a></li>
<li><a href="#activation-function">Activation Function</a></li>
</ul>
</li>
<li><a href="#backpropagation">Backpropagation</a>
<ul>
<li><a href="#backprop-through-a-computation-graph">Backprop Through a Computation Graph</a></li>
</ul>
</li>
<li><a href="#code-basics">Code Basics</a>
<ul>
<li><a href="#types-of-nodes-in-computation-graph">Types of Nodes in Computation Graph</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#lecture-05-k-nearest-neighbours-_september-26-28-2017_">Lecture 05: K-nearest Neighbours, <em>September 26 &amp; 28, 2017</em></a>
<ul>
<li><a href="#classification-revisited">Classification Revisited</a></li>
<li><a href="#nearest-neighbour">1-Nearest Neighbour</a>
<ul>
<li><a href="#defining-nearest">Defining &quot;Nearest&quot;</a></li>
<li><a href="#complexity-of-1-nn">Complexity of 1-NN</a></li>
</ul>
</li>
<li><a href="#normalization">Normalization</a></li>
<li><a href="#learning-the-metric">Learning the Metric</a></li>
<li><a href="#k-nn">K-NN</a></li>
<li><a href="#bayes-rule-under-iid-assumption">Bayes Rule Under iid Assumption</a></li>
<li><a href="#multi-class">Multi-class</a>
<ul>
<li><a href="#at-most-twice-worse-cover-hart-67">At Most Twice Worse (Cover &amp; Hart' 67)</a></li>
</ul>
</li>
<li><a href="#nn-vs-k-nn">1-NN vs. k-NN</a></li>
<li><a href="#curse-of-dimensionality">Curse of Dimensionality</a></li>
<li><a href="#locally-linear-embedding-saul-roweis00">Locally Linear Embedding (Saul &amp; Roweis'00)</a></li>
<li><a href="#k-nn-for-regression">k-NN for Regression</a></li>
<li><a href="#why-1-nn-works">Why 1-NN Works</a></li>
<li><a href="#are-all-neighbours-equal">Are All Neighbours Equal</a></li>
<li><a href="#density-estimation">Density Estimation</a>
<ul>
<li><a href="#nonparametric-regression">Nonparametric Regression</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#lecture-06-hard-margin-svm-_october-3rd-2017_">Lecture 06: Hard-Margin SVM, <em>October 3rd, 2017</em></a>
<ul>
<li><a href="#perceptron-revisited">Perceptron Revisited</a></li>
<li><a href="#margin">Margin</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
      <a id="sidebar-toc-btn">≡</a>
    </body>
    
    
    
    <script>
(function bindTaskListEvent() {
  var taskListItemCheckboxes = document.body.getElementsByClassName('task-list-item-checkbox')
  for (var i = 0; i < taskListItemCheckboxes.length; i++) {
    var checkbox = taskListItemCheckboxes[i]
    var li = checkbox.parentElement
    if (li.tagName !== 'LI') li = li.parentElement
    if (li.tagName === 'LI') {
      li.classList.add('task-list-item')
    }
  }
}())    
</script>
    
<script>

var sidebarTOCBtn = document.getElementById('sidebar-toc-btn')
sidebarTOCBtn.addEventListener('click', function(event) {
  event.stopPropagation()
  if (document.body.hasAttribute('html-show-sidebar-toc')) {
    document.body.removeAttribute('html-show-sidebar-toc')
  } else {
    document.body.setAttribute('html-show-sidebar-toc', true)
  }
})
</script>
      
  </html>