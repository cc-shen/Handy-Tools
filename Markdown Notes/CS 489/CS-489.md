# CS 489/698 - Introduction to Machine Learning

- Instructor: YaoLiang Yu ([yaoliang.yu@uwaterloo.ca](yaoliang.yu@uwaterloo.ca))
	- Office hours: DC 3617: TTh 2:40 - 3:40
- TAs: Amir, Jingjing, Nicole, Nimesh, Shrinu
- Website: [https://cs.uwaterloo.ca/~y328yu/mycourses/489](https://cs.uwaterloo.ca/~y328yu/mycourses/489)
- Piazza: announcements, questions, discussions, etc.
- Learn: assignments, solutions, grades, etc.

## Lecture 00: Introduction, _September 07, 2017_

### Course Logistics

- Prerequisite
	- Programming
		- MATLAB
		- Python
		- [Julia](https://julialang.org/)

- Optional Textbooks
	- The Elements of Statistical Learning
	- Machine Learning, A Probabilistic Perspective
	- Deep Learning
	- Understanding Machine Learning

- Assignments
	- 5 assignments, 2 weeks long each, 10% each
	- Submit on LEARN, typeset using LaTeX

- Exam
	- No midterm
	- Final Exam, 50%
		- Open book
		- No electronics

- Optional Project
	- Related to ML
	- Allow to learn something new and hopefully significant
	- Be _interesting_ and _nontrivial_
	- 1 page proposal and <= 8 pages report

### Course Overview

#### What is Machine Learning

"...field of study that gives computers the ability to learn without being explicitly programmed." -- Arthur Samuel (1959)

#### Learning Categories

##### Supervised

- Classification
- Regression
- Ranking

Teacher provides answer

##### Reinforcement

- Control
- Pricing
- Gaming

Teacher provides motivation

##### Unsupervised

- Clustered
- Visualization
- Representation

Surprise, surprise

#### Supervised, formally

- Given a _training set_ of _pairs_ of examples $(x_{i}, y_{i})$
- Return a _function_ $f: X \rightarrow Y$
- On an _unseen test_ example $x$, output $f(x)$
- The goal is to do well on unseen test data
	- Usually do not care about performance on training data

### Focus of ML Research

- Representation and Interpretation
- Generalization
- Complexity
- Efficiency
- Applications

### Collaboration with focal

[agastya@focal.systems](agastya@focal.systems)

## Lecture 01: Perception, _September 02, 2017_

### Spam Filtering Example

|   |   |
|---|---|
|   |   |

- Training set $(X = [x_{1}, x_{2}, ..., x_{n}], y = {y_{1}, y_{2}, ..., y_{n}})$
	- $x_{i}$ in $X = \mathbb{R}^{d}$: instance $i$ with $d$ dimenstional features
	- $y_{i}$ in $Y = \{-1, 1\}$: instance

### Batch vs. Online

- **Batch Learning**
	- Interested in performance _test_ set $X'$
	- Training set $(X, y)$ is just a means
	- Statistical assumption on $X$ and $X'$
- **Online learning**
	- Data _comes one by one_ (streaming)
	- Need to predict $y$ before knowing its true value
	- Interested in making as few mistakes as possible
	- "Friendliness" of the sequence $(x_{1}, y_{1}), (x_{2}, y_{2}),...$
- Online to Batch conversion

### Linear Threshold Function

Find $({\bf w}, b)$ such that
$$y_{i} = \text{sign}(\langle {\bf w}, {\bf x_{i}}\rangle + b)$$

### Simplification

$$\langle{\bf w}, {\bf x}\rangle + b = \bigg\langle \begin{pmatrix} w \\ b \end{pmatrix}, \begin{pmatrix} x \\ 1 \end{pmatrix} \bigg\rangle$$

### Perception Convergence Theorem

---

**Thm.** _Assume there exists some ${\bf w}$ such that $A^{\intercal}{\bf w} > 0$_, then the perception algorithm converges to some $w^{*}$. If each column of $A$ is selected indefinitely often, then $A^{\intercal}{\bf w}^{*} > \delta$.

**Proof**  
Note that
$||w||_{p} = \left( \sum_{i} |w_{i}|^{p} \right)^{1/p}$

$\forall \gamma > 0$, $\exists~w^{*}$ such that $A^{\intercal}w^{*} \geq \gamma > 0$.  


```math
\begin{aligned}
\langle w_{t + 1}, w^{*} \rangle
&= \langle w_{t} + a, w^{*} \rangle
= \langle w_{t}, w^{*} \rangle + \langle a, w^{*} \rangle \\
&\geq \langle w_{t}, w^{*} \rangle + \gamma \\
&\geq \cdots \geq \underbrace{\langle w_{0}, w^{*} \rangle}_{0} + (t)\gamma
\end{aligned}
```

```math
\begin{aligned}
|w_{t+1}||_{2}^{2}
&= || w_{t} + a ||^{2}_{2}
= \langle w_{t} + a, w_{t} + a \rangle
= ||w_{t}||^{2}_{2} + 2\underbrace{\langle w_{t}, a \rangle}_{\leq \delta = 0} + ||a||^{2}_{2} \\
&\leq ||w_{t}||^{2}_{2} + ||a||^{2}_{2} \\
&\leq ||w_{t}||^{2}_{2} + R^{2} \leq ||w_{t}||^{2}_{2} + (t+1)R^{2}
\end{aligned}
```

---

**Cor.** Let $\delta = 0$ and $w_{0} = 0$. Then percetpion converges after at most $(\frac{R}{\gamma})^{2}$ steps, where
$$R = \underset{i}{\max} ||A_{:i}||_{2},~~~\gamma = \underset{w:||w||_{2} \leq 1}{\max} \underset{i}{\min} \langle w, A_{:i} \rangle$$

---

**Cor.**

---

### What If Non-Separable

Find a better feature representation.

Use a deeper model.

Soft margin
$$\forall {\bf w}^{*}, \forall\gamma > 0,~\text{and}~\forall{\bf a} \in {\bf A}: \langle {\bf a}, {\bf w}^{*} \rangle$$

### Perceptron Boundedness Theorem

Perceptron convergence requires the _existence_ of a separating hyperplane.  
&nbsp; How to check this assumption in practice?  
&nbsp; What if it fails? (It will)

---

**Thm** The iterates of the perceptron algorithm are always bounded. In particular, if there is ......

---

### When to Stop Perceptron

Online learning: never.

Batch Learning

- Maximum number of iteration reached or run out of time
- Training error stops changing
- Validation error stops decreasing
- Weights stopped changing much, if using a diminishing step size $\eta_{t}$, i.e., ${\bf w}_{t+1} \leftarrow {\bf w}_{t} + \eta_{t}y_{i}{\bf x}_{i}$
