# CS 489/698 - Introduction to Machine Learning

- Instructor: YaoLiang Yu ([yaoliang.yu@uwaterloo.ca](yaoliang.yu@uwaterloo.ca))
	- Office hours: DC 3617: TTh 2:40 - 3:40
- TAs: Amir, Jingjing, Nicole, Nimesh, Shrinu
- Website: [https://cs.uwaterloo.ca/~y328yu/mycourses/489](https://cs.uwaterloo.ca/~y328yu/mycourses/489)
- Piazza: announcements, questions, discussions, etc.
- Learn: assignments, solutions, grades, etc.

## Lecture 00: Introduction, _September 07, 2017_

### Course Overview

#### What is Machine Learning

"...field of study that gives computers the ability to learn without being explicitly programmed." -- Arthur Samuel (1959)

#### Learning Categories

##### Supervised

- Classification
- Regression
- Ranking

Teacher provides answer

##### Reinforcement

- Control
- Pricing
- Gaming

Teacher provides motivation

##### Unsupervised

- Clustered
- Visualization
- Representation

Surprise, surprise

#### Supervised, formally

- Given a _training set_ of _pairs_ of examples $(x_{i}, y_{i})$
- Return a _function_ $f: X \rightarrow Y$
- On an _unseen test_ example $x$, output $f(x)$
- The goal is to do well on unseen test data
	- Usually do not care about performance on training data

### Focus of ML Research

- Representation and Interpretation
- Generalization
- Complexity
- Efficiency
- Applications

### Collaboration with focal

[agastya@focal.systems](agastya@focal.systems)

## Lecture 01: Perceptron, _September 12, 2017_

### Spam Filtering Example

| | and | viagra | the | of | nigeria | y |
|---|---|---|---|---|---|---|
| ${\bf x}^{1}$ | 1 | 1 | 0 | 1 | 1 | +1 |
| ${\bf x}^{2}$ | 0 | 0 | 1 | 1 | 0 | -1 |
| ${\bf x}^{3}$ | 0 | 1 | 1 | 0 | 0 | +1 |
| ${\bf x}^{4}$ | 1 | 0 | 0 | 1 | 0 | -1 |
| ${\bf x}^{5}$ | 1 | 0 | 1 | 0 | 1 | +1 |
| ${\bf x}^{6}$ | 1 | 0 | 1 | 1 | 0 | -1 |

- _Training set_ $(X = [x_{1}, x_{2}, ..., x_{n}], y = {y_{1}, y_{2}, ..., y_{n}})$
	- $x_{i}$ in $X = \mathbb{R}^{d}$: instance $i$ with $d$ dimensional features
	- $y_{i}$ in $Y = \{-1, 1\}$: instance $i$ is spam or ham?
- Good _feature representation_ is of uttermost importance

### Batch vs. Online

- **Batch Learning**
	- Interested in performance _test_ set $X'$
	- Training set $(X, y)$ is just a means
	- Statistical assumption on $X$ and $X'$
- **Online learning**
	- Data _comes one by one_ (streaming)
	- Need to predict $y$ before knowing its true value
	- Interested in making as few mistakes as possible
	- "Friendliness" of the sequence $(x_{1}, y_{1}), (x_{2}, y_{2}),...$
- Online to Batch conversion

### Linear Threshold Function

Find $({\bf w}, b)$ such that for all $i$:
$$y_{i} = \text{sign}(\langle {\bf w}, {\bf x}^{i}\rangle + b)$$

- ${\bf w}$ in $R^{d}$: weight vector for the _separating hyperplane_
- $b$ in $R$: offset (threshold, bias) of the separating hyperplane
- sign: threshold function

$$
sign(t) =
\begin{cases}
	1, & t > 0 \\
	-1, & t \leq 0
\end{cases}
$$

For $t = 0$, it does not matter what $sign(t)$ returns as long as its consistent.

### Simplification

$$
\langle{\bf w}, {\bf x}\rangle + b
=
\bigg\langle
\underbrace{\begin{pmatrix} {\bf w} \\ b \end{pmatrix}}_{\rlap{\text{also denoted as }w}\phantom{eeeeeeee}},
\begin{pmatrix} {\bf x} \\ 1 \end{pmatrix}
\bigg\rangle
$$

Padding constant 1 to the end of each $\bf {x}$

$$y \cdot \text{sign}(\langle {\bf w}, {\bf x}\rangle) = \text{sign}(\langle {\bf w}, y{\bf x})$$

Pre-multiply each ${\bf x}$ with its label $y$

Find ${\bf w}$ such that $A^{\intercal}{\bf w} > 0$

- $A = [{\bf a}^{1}, ..., {\bf a}^{n}]$
- ${\bf a}^{i} = \begin{pmatrix} y_{i}{\bf x}^{i} \\ y_{i} \end{pmatrix}$

### Perceptron [Rosenblatt'58]

Nonlinear activation function

$$
\begin{aligned}
&\phantom{eee}~x_{1} \xrightarrow{w_{1}} \\
&\phantom{eee}~x_{2} \xrightarrow{w_{2}} \\
&\phantom{eee}~x_{3} \xrightarrow{w_{3}} \Sigma \longrightarrow \geq 0? \xrightarrow{\text{Nonlinear activation function}} \\
&\underbrace{x_{4} \xrightarrow{w_{4}}}_{\text{Linear superposition}}
\end{aligned}
$$

### The Perceptron Algorithm

Input: $A\in \mathbb{R}^{(d+1)\times n}$,  threshold $\delta \geq 0$, initialize ${\bf w}_{0} \in \mathbb{R}^{d+1}$ arbitrarily

1. **repeat**
2. &nbsp; &nbsp; select some column ${\bf a}$ of $A$
3. &nbsp; &nbsp; **if** $\langle {\bf a}, {\bf w_{t}} \rangle \leq \delta$ **then**
4. &nbsp; &nbsp; &nbsp; &nbsp; ${\bf w}_{t+1} = {\bf w}_{t} + {\bf a}$ `// update only when making a mistake`
5. &nbsp; &nbsp; &nbsp; &nbsp; $t \leftarrow t + 1$
6. **until** $convergence$

Typically $\delta = 0$, ${\bf w}_{0} = {\bf 0}$

$$
\langle {\bf a}, {\bf w} \rangle
=
y(\langle x, {\bf w} \rangle + b) < 0
\text{ iff. }
\underbrace{y}_{\text{truth}}
\neq
\underbrace{\text{sign}(\langle x, {\bf w} \rangle + b)}_{\text{prediction}}
$$

**Lazy** update: if it ain't broken, don't fix it.

$$
\langle {\bf a}, {\bf w}_{t + 1} \rangle
=
\langle {\bf a}, {\bf w}_{t} + {\bf a} \rangle
=
\langle {\bf a}, {\bf w}_{t} \rangle + ||a||^{2}_{2}
>
\langle {\bf a}, {\bf w}_{t} \rangle
$$

#### Does It Work

| | and | viagra | the | of | nigeria | y |
|---|---|---|---|---|---|---|
| ${\bf x}^{1}$ | 1 | 1 | 0 | 1 | 1 | +1 |
| ${\bf x}^{2}$ | 0 | 0 | 1 | 1 | 0 | -1 |
| ${\bf x}^{3}$ | 0 | 1 | 1 | 0 | 0 | +1 |
| ${\bf x}^{4}$ | 1 | 0 | 0 | 1 | 0 | -1 |
| ${\bf x}^{5}$ | 1 | 0 | 1 | 0 | 1 | +1 |
| ${\bf x}^{6}$ | 1 | 0 | 1 | 1 | 0 | -1 |

$$
{\bf w} \leftarrow {\bf w} + y{\bf x}
$$

$$
b \leftarrow b + y
$$

- $w_{0} = \begin{pmatrix} 0 & 0 & 0 & 0 & 0 \end{pmatrix}$, $b_{0} = 0$, prediction on ${\bf x}^{1}$ is _undefined_
- $w_{1} = \begin{pmatrix} 1 & 1 & 0 & 1 & 1 \end{pmatrix}$, $b_{1} = 1$, predict 1 on ${\bf x}^{2}$, _wrong_
- $w_{2} = \begin{pmatrix} 1 & 1 & -1 & 0 & 1 \end{pmatrix}$, $b_{2} = 0$, prediction on ${\bf x}^{3}$ i _undefined_
- $w_{3} = \begin{pmatrix} 1 & 2 & 0 & 0 & 1 \end{pmatrix}$, $b_{3} = 1$, predict 1 on ${\bf x}^{4}$, _wrong_
- $w_{4} = \begin{pmatrix} 0 & 2 & 0 & -1 & 1 \end{pmatrix}$, $b_{4} = 0$, predict 1 on ${\bf x}^{5}$, _correct_
- $w_{4} = \begin{pmatrix} 0 & 2 & 0 & -1 & 1 \end{pmatrix}$, $b_{4} = 0$, predict -1 on ${\bf x}^{6}$, _correct_

### Perception Convergence Theorem

---

**Thm.** (Block'62; Novikoff'62)  
_Assume there exists some ${\bf w}$ such that $A^{\intercal}{\bf w} > 0$_, then the perception algorithm converges to some $w^{*}$.
If each column of $A$ is selected indefinitely often, then $A^{\intercal}{\bf w}^{*} > \delta$.

**Proof**  
Note that
$||w||_{p} = \left( \sum_{i} |w_{i}|^{p} \right)^{1/p}$

$\forall \gamma > 0$, $\exists~w^{*}$ such that $A^{\intercal}w^{*} \geq \gamma > 0$.  

$$
\begin{aligned}
\langle w_{t + 1}, w^{*} \rangle
&= \langle w_{t} + a, w^{*} \rangle
= \langle w_{t}, w^{*} \rangle + \langle a, w^{*} \rangle \\
&\geq \langle w_{t}, w^{*} \rangle + \gamma \\
&\geq \cdots \geq \underbrace{\langle w_{0}, w^{*} \rangle}_{0} + (t)\gamma
\end{aligned}
$$

$$
\begin{aligned}
|w_{t+1}||_{2}^{2}
&= || w_{t} + a ||^{2}_{2}
= \langle w_{t} + a, w_{t} + a \rangle
= ||w_{t}||^{2}_{2} + 2\underbrace{\langle w_{t}, a \rangle}_{\leq~\delta~=~0} + ||a||^{2}_{2} \\
&\leq ||w_{t}||^{2}_{2} + ||a||^{2}_{2} \\
&\leq ||w_{t}||^{2}_{2} + R^{2} \leq ||w_{t}||^{2}_{2} + (t+1)R^{2}
\end{aligned}
$$

---

**Cor.**  
Let $\delta = 0$ and $w_{0} = 0$. Then percetpion converges after at most $(\frac{R}{\gamma})^{2}$ steps, where
$$R = \underset{i}{\max} ||A_{:i}||_{2},~~~\gamma = \underset{w:||w||_{2} \leq 1}{\max} \underset{i}{\min} \langle w, A_{:i} \rangle$$

---

#### The Margin

$$
\begin{aligned}
\min_{{\bf w}^{*}:~A^{\intercal}{\bf w}^{*}~\geq~\gamma{\bf 1}} \frac{||{\bf w}^{*}||^{2}_{2}}{\gamma^{2}} &= \min_{{\bf w}:~A^{\intercal}{\bf w}~\geq~{\bf 1}} ||{\bf w}||^{2}_{2} \\
&= \min_{({\bf w}, t):~||{\bf w}||_{2}\leq t,~A^{\intercal}{\bf w} \geq {\bf 1}} t^{2} \\
&= \min_{({\bf w}, t):~||{\bf w}||_{2}\leq 1,~A^{\intercal}{\bf w} \geq \frac{1}{t}{\bf 1}} t^{2} \\
&= \min_{({\bf w}, t):~||{\bf w}||_{2}\leq 1,~A^{\intercal}{\bf w} > 0} \frac{1}{\displaystyle \min_{{\bf a} \in A} \langle {\bf a}, {\bf w} \rangle} \\
&= \left(\frac{1}{\displaystyle \max_{{\bf w}:~||{\bf w}||_{2} \leq 1} \min_{{\bf a} \in A} \langle {\bf a}, {\bf w} \rangle}\right)^{2}
\end{aligned}
$$

Where the margin $\gamma$ is $\displaystyle \max_{{\bf w}:~||{\bf w}||_{2} \leq 1} \min_{{\bf a} \in A} \langle {\bf a}, {\bf w} \rangle$

### What Does the Bound Mean

---

**Cor.**  
Let $\delta = 0$ and ${\bf w}_{0} = {\bf 0}$. Then perceptron converges after at most $(R/\gamma)^{2}$ steps, where
$$
R = \max_{i} ||A_{:i}||_{2},~\gamma = \max_{{\bf w}:~||{\bf w}||_{2} \leq 1} \min_{i} \langle {\bf w}, A_{:i} \rangle
$$

---

Treating $R$ and $\gamma$ as constants, then the number of mistakes are independent of $n$ and $d$!.  
Otherwise may need exponential time... Can we _improve_ the bound?

#### But

Pro: the larger the margin, the faster perceptron converges.

Con: But perceptron stops at an arbitrary linear separator...

Which one do you prefer?
$$
\min_{A^{\intercal}{\bf w} \geq 1} \frac{1}{2}||{\bf w}||_{2}^{2}
\approx
\min_{y_{i}(\langle {\bf w}, {\bf x}^{i} \rangle + b) \geq 1} \frac{1}{2}||{\bf w}||_{2}^{2}
$$

### What If Non-Separable

Find a better feature representation.

Use a deeper model.

Soft margin
$$
\forall {\bf w}^{*}, \forall\gamma > 0,~\text{and}~\forall{\bf a} \in {\bf A}: \langle {\bf a}, {\bf w}^{*} \rangle \geq \gamma - (\gamma - \langle {\bf a}, {\bf w}^{*} \rangle) +
$$

### Perceptron Boundedness Theorem

Perceptron convergence requires the _existence_ of a separating hyperplane.  
&nbsp; &nbsp; How to check this assumption in practice?  
&nbsp; &nbsp; What if it fails? (It will)

---

**Thm** (Minsky and Papert'67; Block and Levin'70)  
The iterates of the perceptron algorithm are always bounded. In particular, if there is no separating hyperplane, then perceptron cycles.

---

### When to Stop Perceptron

Online learning: never.

Batch Learning

- Maximum number of iteration reached or run out of time
- Training error stops changing
- Validation error stops decreasing
- Weights stopped changing much, if using a diminishing step size $\eta_{t}$, i.e., ${\bf w}_{t+1} \leftarrow {\bf w}_{t} + \eta_{t}y_{i}{\bf x}^{i}$

### Multiclass Percetron

One vs. all

- Class c as positive
- All other classes as negative
- Highest activation wins: $\text{pred} = \text{argmax}_{c}{\bf w}_{c}^{\intercal}{\bf x}$

One vs. one

- Class c as positive
- Class c' as negative
- Voting

### The Winnow Algorithm (Littlestone'98)

Input: $A\in \mathbb{R}^{(d')\times n}$,  threshold $\delta \geq 0$, step size $\eta > 0$, initialize ${\bf w}_{0} \in \color{red}{int}\Delta_{d'-1}$

1. **repeat**
2. &nbsp; &nbsp; select some column ${\bf a}$ of $A$
3. &nbsp; &nbsp; **if** $\langle {\bf a}, {\bf w_{t}} \rangle \leq \delta$ **then**
4. &nbsp; &nbsp; &nbsp; &nbsp; ${\bf w}_{t+1} = {\bf w}_{t} \odot \exp(\eta {\bf a})$ `// update only when making a mistake`
5. &nbsp; &nbsp; &nbsp; &nbsp; ${\bf w}_{t+1} \leftarrow \frac{{\bf w}_{t+1}}{||{\bf w}_{t+1}||_{1}}$ `// normalize`
6. &nbsp; &nbsp; &nbsp; &nbsp; $t \leftarrow t + 1$
7. **until** $convergence$

Multiplicative vs. additive

---

**Thm.** (Littlestone'88)  
Assume there exists some _non-negative_ ${\bf w}$ such that $A^{\intercal}{\bf w} > 0$, then winnow converges to some ${\bf w}^{*}$.
If each column of $A$ is selected indefinitely often, then $A^{\intercal}{\bf w}^{*} > \delta$.

---

## Lecture 02: Linear Regression, _September 14, 2017_

### How Much Should I Bid For

- Interpolation vs. Extrapolation
- Linear vs. Non-linear

#### Laffer Curve

A curve that represents the relationship between tax revenue and tax rate.  
A downward parabola curve, with an revenue maximizing tax rate $t^{*}$.

### Regression

Given a pair $(X, Y)$, find function $f$ such that
$$f(X) \approx Y$$

- $X$: feature vector, $d$-dimensional real vector
- $Y$: response, $m$-dimensional real vector

Two problems:

1. $(X, Y)$ is uncertain: samples from an _unknown distribution_
2. How to measure the error: need a _loss_ function

### Risk Minimization

Minimize the expected loss, aka _risk_
$$
\begin{aligned}
\min_{f: X \rightarrow Y} \underbrace{{\bf E}[L(f(X), Y)]}_{} & & L: y \times y \rightarrow {\bf R}_{+} \\
& & L(y, y) \equiv 0
\end{aligned}
$$

Which loss to use?  
Not always clearl conenience dominates for now

Least squares: $\displaystyle \min_{f} {\bf E}||f(X) - Y||_{2}^{2}$

### The Regression Function

$$
{\bf E}||f(X) - Y||_{2}^{2}
=
{\bf E}||f(X) - {\bf E}(Y|X)||_{2}^{2} + \underbrace{{\bf E}||{\bf E}(Y|X) - Y||_{2}^{2}}_{\text{Inherent noise variance}}
$$

Regression function:
$$f^{*}(X) = m(X) = {\bf E}(Y|X)$$
where $m$ is the mean function.

Many ways to estimate $m(X)$

Simplest: Let's assume it is linear (affine)!

### Linear Regression

_Assumption_: $m(X) = {\bf E}(Y|X) = XA + {\bf b}$

Goal: $\displaystyle \min_{A, {\bf b}} {\bf E}||XA + {\bf b} - Y||_{2}^{2}$  
Where ${\bf E}$ is an unknown distribution.

Law of Large Numbers: $\displaystyle \frac{1}{n}\sum_{i = 1}^{n}Z_{i} \rightarrow {\bf E}(Z)$

Reality:  
&nbsp; &nbsp; $\displaystyle \min_{A, {\bf b}}\underbrace{\frac{1}{n}\sum_{i=1}^{n}||X_{i}A + {\bf b} - Y_{i}||_{2}^{2}}_{\text{empirical risk}}$

#### Simplification, again

$$
\begin{aligned}
& \min_{A, {\bf b}} \frac{1}{n}\sum_{i=1}^{n}||X_{i}A + {\bf b} - Y_{i}||_{2}^{2} \\
\Rightarrow W \leftarrow \begin{pmatrix} A \\ {\bf b} \end{pmatrix} \\
X_{i} \leftarrow (X_{i}, 1) \\
& \min_{W} \frac{1}{n}\sum_{i=1}^{n}||X_{i}W - Y_{i}||_{2}^{2} \\
\Rightarrow X = \begin{bmatrix} X_{1} \\ X_{2} \\ \vdots \\ X_{n} \end{bmatrix}
Y = \begin{bmatrix} Y_{1} \\ Y_{2} \\ \vdots \\ Y_{n} \end{bmatrix} \\
& \min_{W \in {\bf R}^{(d+1)\times m}}||XW - Y||_{F}^{2}
\end{aligned}
$$

#### Finally

$$
\min_{W \in {\bf R}^{(d+1)\times m}}\overbrace{||XW - Y||_{F}^{2}}^{\text{sum of square residuals}}
$$

$XW \rightarrow$ hyperplane (again!), parameterized by $W$.  
$Y \rightarrow$ true responses.

In transpose form: $WX^{\intercal} - Y^{\intercal}$

#### Why Least Squares

---

**Thm.** (Sondermann'86; Friedland and Torokhti'07; Yu and Schuurmans'11)  
Among all minimizers of $\displaystyle \min_{W}||AWB - C||_{F}$, $W=A^{+}CB^{+}$ is the one that has minimal F-norm.

---

**Pseudo-inverse** $A^{+}$ is the _unqiue_ matrix $G$ such that
$$AGA = A,~GAG = G,~(AG)^{\intercal} = AG,~(GA)^{\intercal} = GA$$

**Singular Value Decomposition** $A = USV^{\intercal}$, $A^{+} = VS^{-1}U^{\intercal}$

##### Optimizatino Detour

$$\min_{x} f(x)$$

---

**Format's Theorem**  
Necessarily $Df(x) = 0$

---

(Préchet) **Derivative at $x$**.
$$\lim_{0\neq\delta\rightarrow 0}\frac{|f(x + \delta) - f(x) - Df(x)\delta|}{|\delta|} = 0$$

Example.  
$f({\bf x}) = {\bf x}^{\intercal}A{\bf x} + {\bf x}^{\intercal}{\bf b} + c$  
$Df({\bf x}) = (A + A^{\intercal}){\bf x} + {\bf b}$

#### Solving Least Squares

Note, $m = 1$
$X \in \mathbb{R}^{n \times (d+1)}$  
$W \in \mathbb{R}^{(d+1)\times m}$  
$Y \in \mathbb{R}^{r \times m}$  
$X^{\intercal}XW \in \mathbb{R}^{(d+1)\times m}$  
$A^{\intercal}Y \in \mathbb{R}^{(d+1)\times m}$

$$
\begin{aligned}
\min_{W \in {\bf R}^{(d+1)\times m}}||XW - Y||_{F}^{2} &= W^{\intercal}(X^{\intercal}X)W - 2W^{\intercal}X^{\intercal}Y + Y^{\intercal}{Y} \\
&\Downarrow \\
X^{\intercal}XW &= X^{\intercal}Y ~~~ \text{Normal Equation}
\end{aligned}
$$

$X^{\intercal}X# may not be invertible, but there is always a solution. 
Even invertible, _never ever compute_ $W = (X^{\intercal}X)^{-1}X^{\intercal}Y$ !  
Instead, solve the linear system.

#### Prediction

Once we have $W$, we can predict
$$\hat{Y} = X_{\text{test}}W$$

How to evaluate?
$$(Y_{\text{test}} - \hat{Y})^{2}$$

Sometimes we evaluate using a different 

### Robustness

**Huber's Loss**
$$
H(\hat{y}, y) =
\begin{cases}
\frac{1}{2}(\hat{y} - y^{2}), & |\hat{y} - y| \leq \delta \\
\delta(|\hat{y} - y| - \frac{1}{2}\delta), & |\hat{y} - y| \geq \delta
\end{cases}
$$

### Multi-task Learning

$$X^{\intercal}XW = X^{\intercal}Y$$

Everything we've shown still holds if $Y$ is $m$-dimensional.  
But, can solve each column of $Y$ _independently_
$$X^{\intercal}XW_{;j} = X^{\intercal}Y_{:j}$$
Things are more interesting if we had _regularization_.

### Regularization

#### III-Posedness

Let $x_{1} = 0$, $x_{2} = \epsilon$, $y_{1} = 1$, $y_{2} = -1$

$$
X =
\begin{bmatrix}
0 & 1 \\
\epsilon & 1
\end{bmatrix}
~~ Y =
\begin{bmatrix}
1 \\
-1
\end{bmatrix}
$$

$$
w = X^{-1}y =
\begin{bmatrix}
\frac{2}{\epsilon} \\
1
\end{bmatrix}
$$

Slight disturbance leads to chaotic behaviour.

#### Tiknohov Regularization (Hoerl and Kennard'70)

Ridge regression:
$$
\min_{W \in {\bf R}^{(d+1)\times m}} ||XW - Y||_{F}^{2} + \lambda||W||_{F}^{2}
$$
$$\Longrightarrow (X^{\intercal}X + \lambda I)W = X^{\intercal}Y$$
where $\lambda$ is the reg. constant (hyperparameter).

With positive lambda, slight perturbation in input leads to _proportional_ (with regards to $\frac{1}{\lambda}$) perturbation in output.

#### Data Augmentation

$$
\begin{aligned}
\min_{W \in {\bf R}^{(d+1)\times m}} &||XW - Y||_{F}^{2} + \lambda||W||_{F}^{2} \\
&\Updownarrow \\
\min_{W \in {\bf R}^{(d+1)\times m}} &||\widetilde{X}W - \widetilde{Y}||_{F}^{2}
\end{aligned}
$$

$$
\widetilde{X} =
\begin{bmatrix}
X \\
\sqrt{\lambda}I
\end{bmatrix}
~~ \widetilde{Y} =
\begin{bmatrix}
Y \\
0
\end{bmatrix}
$$

#### Sparsity

Ridge regression weight is always dense,

- Computationally heavy
- Interpretationally cumbersome

Lasso (Tibshirani'96)

$$
\min_{||W||_{1} \leq C}||XW - Y||_{F}^{2}
$$

#### Regularization vs. Constraint

$$
\begin{aligned}
\min_{W}\ell(W) &+ \lambda \cdot r(W) && \text{computationally appealing}\\
\text{always true } \Downarrow ~&~ \Uparrow \text{mild conditions} \\
\min_{W:r(W) \leq C} &\ell(W) && \text{theoretically appealing}
\end{aligned}
$$

### Cross-Validation

See written notes.
