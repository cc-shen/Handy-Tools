\documentclass[12pt, leqno]{article}
\usepackage[margin = 1.5in]{geometry}
\setlength{\parindent}{0in}
\usepackage{amsfonts, amssymb, amsthm, mathtools, tikz, qtree, float}
\usepackage[lined]{algorithm2e}
\usepackage[T1]{fontenc}
\usepackage{ae, aecompl, color}
\usepackage[pdftex, pdfauthor={Charles Shen}, pdftitle={STAT 231: Statistics}, pdfsubject={Notes from STAT 231: at the University of Waterloo}, pdfkeywords={course notes, notes, Waterloo, University of Waterloo}, pdfproducer={LaTeX}, pdfcreator={pdflatex}]{hyperref}
\usepackage{cleveref}
\usepackage{wrapfig}
\allowdisplaybreaks
\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace}

\definecolor{darkish-blue}{RGB}{25,103,185}

\hypersetup{
    colorlinks,
    citecolor=darkish-blue,
    filecolor=darkish-blue,
    linkcolor=darkish-blue,
    urlcolor=darkish-blue
}

\theoremstyle{definition}
\newtheorem*{defn}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{corollary}{Corollary}
\newtheorem{ex}{Example}[section]

\crefname{ex}{Example}{Example}

\setlength{\marginparwidth}{1.5in}
\newcommand{\lecture}[1]{\marginpar{{\footnotesize $\leftarrow$ \underline{#1}}}}

\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

\begin{document}
  \let\ref\Cref

  \title{\bf{STAT 231: Statistics}}
  \date{Spring 2016, University of Waterloo \\ \center Formulas and Notes.}
  \author{Charles Shen}

  \blfootnote{Feel free to email feedback to me at \href{mailto:ccshen902@gmail.com}{ccshen902@gmail.com}.}

  \maketitle
  \begin{center}
    \textbf{Assume all $\log$ are in base $e$ unless specified.} \\
    I've tried to use $\ln$ for consistency, \\
    but there may be a few inconsistency.
  \end{center}
  \newpage
  \tableofcontents
  \newpage

  \section{Numerical Summaries}
  \subsection{Measure of Location}
  \subsubsection{Mean}
  The \emph{sample mean}

  \section{Distribution Theory}
  If $Y_{1}, Y_{2}, \dots, Y_{n} ~ N(\mu, \sigma^{2})$ and they're independent, then
  $$\bar{Y} \sim N(\mu, \frac{\sigma^{2}}{n})$$
  and
  $$\frac{\bar{Y} - \mu}{\sigma/\sqrt{n}} = Z \sim N(0, 1) $$
  For large $n$, then
  $$\frac{\bar{Y} - \mu}{s/\sqrt{n}} \approx Z \sim N(0, 1) $$

  \section{Statistical Models and Maximum Likelihood Estimation}
  \begin{defn}
    \emph{The \textbf{relative likelihood function} is defined as}
    $$R(\theta) = \frac{L(\theta)}{L(\hat{\theta})} ~~~for~\theta \in \Omega$$
    $Note~that~0 \leq R(\theta) \leq 1~for~all~\theta \in \Omega$.
  \end{defn}
  \begin{defn}
    \emph{The \textbf{log likelihood function} is defined as}
    $$l(\theta) = \ln L(\theta) ~~~for~\theta \in \Omega$$
  \end{defn}
  \subsection{Likelihood Function for Binomial Distribution}
  The maximum likelihood estimate of $\theta$ is $\bar{\theta} = y / n$.

  \subsection{Likelihood Function for Poisson Distribution}
  The value $\theta = \bar{y}$ maximizes $l(\theta)$ and so $\hat{\theta} = \bar{y}$ is the maximum likelihood estimate of $\theta$.

  \subsection{Likelihood Function for Exponential Distribution}
  The value $\theta = \bar{y}$ maximizes $l(\theta)$ and so $\hat{\theta} = \bar{y}$ is the maximum likelihood estimate of $\theta$ for an Exponential Distribution $\sim Exp(\theta)$.

  \subsection{Likelihood Function for Gaussian Distribution}
  The maximum likelihood estimate of $\theta$ is $\hat{\theta} = (\hat{\mu}, \hat{\sigma})$, where
  $$
  \hat{\mu} = \frac{1}{n}\sum_{i=1}^{n}y_{i} = \bar{y}
  ~~\text{and}~~
  \hat{\sigma} = \left[\frac{1}{n}\sum_{i=1}^{n}(y_{i} - \bar{y})^{2}\right]^{1/2}
  $$
  Note that $\hat{\sigma} \not = \sigma$ (sample variance).

  \subsection{Invariance Property of Maximum Likelihood Estimates}
  \begin{theorem}
    \emph{If $\hat{\theta}$ is the maximum likelihood estimate of $\theta$, then $g(\hat{\theta})$ is the maximum likelihood estimate of $g(\theta)$.}
  \end{theorem}

  \section{Estimation}
  \subsection{Confidence Intervals and Pivotal Quantities}
  In general, construct a pivot using the estimator, use that to construct coverage interval, estimate it and find the confidence interval.

  \begin{defn}
    A $100p\%$, where $0 \leq p \leq 1$, confidence interval tells $100p\%$ of the intervals constructed from samples will contain the true unknown value of $\mu$ (or $\sigma$).
  \end{defn}

  To determine the correct value to look for in the distribution tables, calculate $(1+p)/2$ where $100p\%$ is the level of confidence. \\
  For example, the $95\%$ confidence interval needs to look at $\frac{1+0.95}{2} = 0.975$.

  \begin{theorem}
  \textbf{Central Limit Theorem} \\
    If $n$ is large, and if $Y_{1}, \dots, Y_{n}$ are drawn from a distribution with mean $\mu$ and variance $\sigma^{2}$, then $\bar{Y} \sim N\left(\mu, \frac{\sigma^{2}}{n}\right)$.
  \end{theorem}

  For a Binomial Distribution, the confidence interval is
  $$\left[~\hat{\pi} \pm z^{*}\sqrt{\frac{\hat{\pi}(1 - \hat{\pi})}{n}}~\right]$$
  where $\hat{\pi} = \frac{y}{n}$, $y$ is the observed data. \\
  To determine the sample size
  $$n \geq \left(\frac{z^{*}}{MoE}\right)^{2}\hat{\pi}(1 - \hat{\pi})$$
  where $MoE$ is the margin of error. \\
  To be conservative, we usually pick $\hat{\pi} = 0.5$ as it maximizes $\hat{\pi}(1 - \hat{\pi})$. \\

  For a Poisson Distribution, the pivotal quantity is
  $$\frac{\bar{Y} - \mu}{\sqrt{\frac{\bar{Y}}{n}}} = Z \sim N(0,1)$$
  and the confidence interval is
  $$\left[~\bar{y} \pm z^{*}\sqrt{\frac{\bar{y}}{n}}~\right]$$

  \subsection{Chi-Squared Distribution $\sim X_{k}^{2}$}
  The Gamma function is
  $$\Gamma(\alpha) = \int_{0}^{\infty}y^{\alpha -1}e^{-y}dy ~~\text{for}~\alpha > 0$$
  Properties of the Gamma Function:
  \begin{itemize}
    \item $\Gamma(\alpha) = (\alpha - 1)\Gamma(\alpha - 1)$
    \item $\Gamma(\alpha) = (\alpha - 1)!$
    \item $\Gamma(1/2) = \sqrt{\pi}$
  \end{itemize}
  The $X_{k}^{2}$ distribution is a continuous family of distributions on $(0, \infty)$ with probability density function
  $$f(x;k) = \frac{1}{2^{k/2}\Gamma(k/2)}x^{(k/2)-1}e^{-x/2}~~~\text{for}~x > 0$$
  where $k \in \{1, 2, \dots\}$ is a parameter of the distribution. \\
  $k$ is referred to as the ``degrees of freedom'' (d.f) parameter. \\

  For $X \sim X_{k}^{2}$
  \begin{itemize}
    \item $E(X) = k$ and $Var(X) = 2k$
    \item If $k = 1$, $X = Z^{2}$ and $Z \sim G(0,1)$
    \item If $k = 2$, $X \sim Exp(2)$ ($\theta = 2$)
    \item If $k$ is large, $X \stackrel{Appr.}{\sim} N(k, 2k)$
    \item Let $X_{k_{1}}, X_{k_{1}}$ be independent random variables with $X_{k_{i}} \sim X_{k_{i}}^{2}$. \\
    Then $X_{k_{1}} + X_{k_{2}} =  X_{k_{1} + k_{2}}^{2}$.
  \end{itemize}

  \begin{theorem}
    If $Y_{i} \sim Exp(\mu)$, then
    $$\frac{2Y_{i}}{\mu} \sim Exp(2) \rightarrow X_{2}^{2}$$
  \end{theorem}

  \subsection{Student's \emph{t} Distribution}
  Student's $t$ distribution has probability density function
  $$
  f(t;k) = c_{k}\left(1 + \frac{t^{2}}{k}\right)^{-(k+1)/2}
  ~~~\text{for}~t \in \Re~\text{and}~ k = 1, 2, \dots
  $$
  where the constant $c_{k}$ is given by
  $$
  c_{k} = \frac{\Gamma(\frac{k+1}{2})}{\sqrt{k\pi}\Gamma(\frac{k}{2})}
  ~~~k\text{ is the }degrees~of~freedom
  $$
  Properties of $T$:
  \begin{itemize}
    \item[i)] Range of $T$: $(-\infty, \infty)$
    \item[ii)] T is symmetric around 0
    \item[iii)] As $k \uparrow$, $T \rightarrow Z$
  \end{itemize}

  \begin{theorem}
    \emph{Suppose $Z \sim G(0,1)$ and $U \sim X_{k}^{2}$ independently. Let}
    $$T = \frac{Z}{\sqrt{U / k}}$$
    $$\rightarrow \frac{\bar{Y} - M}{s / \sqrt{n}} \sim t_{n-1}$$
    \emph{Then T has \textbf{Student's} t \textbf{distribution with} k \bf{degrees of freedom}}.
  \end{theorem}

  \subsection{Likelihood-Based Confidence Intervals}
  \begin{theorem}
    $A~100p\%~likelihood~interval~is~an~approximate~100q\%~where~q = 2P(Z \leq \sqrt{-2\ln p}) - 1~and~Z \sim N(0,1).$
  \end{theorem}

  \begin{ex}
    Show that a 1\% likelihood interval is an approximate 99.8\% confidence interval. \\
    Note that $p = 0.01$
    \begin{align*}
      q &= 2P(Z \leq \sqrt{-2\ln(0.01)}) - 1 \\
        &\approx 2P(Z \leq 3.03) - 1 \\
        &= 2(0.99878) - 1 \\
        &= 0.998 = 99.8\%
    \end{align*}
  \end{ex}

  \begin{theorem}
    \emph{If a is a value such that}
    $$P = 2P(Z \leq a) - 1 ~~ where ~ Z \sim N(0,1)$$
    \emph{then the likelihood interval }$\{\theta : R(\theta) \geq e^{-a^{2}/2}\}$ \emph{is an approximate $100p\%$ confidence interval.}
  \end{theorem}

  \begin{ex}
    Since
    $$0.95 = 2P(Z \leq 1.96) - 1 ~\text{where}~Z\sim N(0,1)$$
    and
    $$e^{-(1.96)^{2}/2} = e^{-1.9208} \approx 0.1465 \approx 0.15$$
    therefore a $15\%$ \emph{likelihood interval} for $\theta$ is also an approximate $95\%$ \emph{confidence interval} for $\theta$.`'
  \end{ex}

  \subsection{Confidence Intervals for Parameters in the $G(\mu, \sigma)$ Model}
  If $Y_{1}, \dots, Y_{n}$ are independent $N(\mu, \sigma^{2})$, then $\bar{Y} \sim N(\mu, \frac{\sigma^{2}}{n})$ and
  \begin{align}
     \frac{\bar{Y} - \mu}{s / \sqrt{n}}~ &\sim t_{n-1} \\
     \frac{(n-1)S^{2}}{\sigma^{2}} ~&\sim X_{n-1}^{2}
  \end{align}

  General Rule: \\
  The Confidence Interval for $\mu$ if $\sigma$ is unknown is
  $$\left[~\bar{y} \pm t^{*}\frac{s}{\sqrt{n}}~\right]$$
  When $\sigma$ is unknown, we replace $\sigma$ by its estimate $s$, and we use t-pivot. \\
  Confidence interval when $\sigma$ is known is
  $$\left[~\bar{y} \pm z^{*}\frac{\sigma}{\sqrt{n}}~\right]$$
  When $\sigma$ is known, we use z-pivot. \\

  If $n$ is really large, then the $t^{*}$ value converges to the corresponding $z^{*}$ value (by Central Limit Theorem). \\

  \textbf{Confidence Intervals for $\sigma^{2}$ and $\sigma$}
  \begin{theorem}
    \emph{Suppose} $Y_{1}, Y_{2}, \dots, Y_{n}$ \emph{is a random sample from the} $G(\mu, \sigma)$ \emph{distribution with sample variance} $S^{2}$.
    \emph{Then the random variable}
    $$\frac{(n-1)S^{2}}{\sigma^{2}} = \frac{1}{\sigma^{2}}\sum_{i=1}^{n}(Y_{i} - \bar{Y})^{2}$$
    \emph{has a Chi-squared distribution with} $n - 1$ \emph{degrees of freedom}.
  \end{theorem}
  Using the theorem, we can construct a $100p\%$ confidence interval for the parameter $\sigma^{2}$ or $\sigma$. \\
  Recall this is the same as the equation (2) in this sub-section. \\
  We can find constants $a$ and $b$ such that
  $$P(a \leq U \leq b) = p$$
  where $U \sim X_{n-1}^{2}$. \\
  So a $100p\%$ confidence interval for $\sigma^{2}$ is
  $$\left[~\frac{(n-1)s^{2}}{b},\frac{(n-1)s^{2}}{a}~\right]$$
  and a $100p\%$ confidence interval for $\sigma$ is
  $$\left[~\sqrt{\frac{(n-1)s^{2}}{b}},\sqrt{\frac{(n-1)s^{2}}{a}}~\right]$$ \\

  Unlike confidence interval for $\mu$, the confidence interval for $\sigma^{2}$ is \emph{not symmetric} about $s^{2}$. the estimator of $\sigma^{2}$.
  The $X_{n-1}^{2}$ distribution is not a symmetric distribution. \\

  \textbf{Prediction Interval for a Future Observation} \\
  Suppose that $Y \sim G(\mu,\sigma)$ with \textbf{independent} observations, then
  $$Y - \widetilde{\mu} = Y - \bar{Y} \sim N\left(0, \sigma^{2}\left(1 + \frac{1}{n}\right)\right)$$
  Also
  $$\frac{Y - \bar{Y}}{S\sqrt{1 + \frac{1}{n}}} \sim t_{n-1}$$
  is a pivotal quantity which can be used to obtain an interval of values for $Y$. \\
  Let $t^{*}$ be a value such that $P(-t^{*} \leq T \leq t^{*}) = p$ or $P(T \leq t^{*}) = (1+p)/2$ which is obtained from tables. Thus
  $$\left[ ~\bar{y} \pm t^{*}s\sqrt{1 + \frac{1}{n}} ~\right]$$

  \section{Tests of Hypothesis}
  \begin{defn}
    A \emph{hypothesis} in statistic is a claim made about the values of a certain parameter of the population.
  \end{defn}

  There are \textbf{two} competing hypotheses:
  \begin{itemize}
    \item \emph{Null} Hypothesis, denoted $H_{0}$; current ``status quo'' assumption.
    \item \emph{Alternative} Hypothesis, denoted $H_{1}$; seeks to challenge $H_{0}$.
  \end{itemize}

  \begin{defn}
    A \emph{test statistic \emph{or} discrepancy measure} $D$ is a function of the data \textbf{Y} that is constructed to measure the degree of ``agreement'' between the data \textbf{Y} and the null hypothesis $H_{0}$.
  \end{defn}

  For every testing decision, there is a possibility of making two kinds of errors:
  \begin{itemize}
    \item[\textbf{Type I}] $H_{0}$ is true; $H_{0}$ is rejected.
    \item[\textbf{Type II}] $H_{1}$ is true; $H_{0}$ is not rejected.
  \end{itemize}
  If Type I error goes down, then Type II error goes up; vice versa holds as well.

  \subsection{p-value}
  Suppose there's the test statistic $D = D(\textbf{Y})$ to test the hypothesis $H_{0}$. \\
  Also suppose that $d = D(\textbf{y})$ is the observed value of $D$.
  \begin{defn}
    A \emph{p-value} or observed significance level of the test of hypothesis $H_{0}$ using test statistic $D$ is
    $$\emph{p-value} = P(D \geq d; H_{0})$$
  \end{defn}
  \textbf{Caution}: The \emph{p-value} is \textbf{not} the probability that $H_{0}$ is true.
  \begin{table}[H]
  \caption{Interpretation of \emph{p-values}}
  \begin{center}
  \begin{tabular}{c | p{9cm}}
    \bf \emph{p-value} & \bf Interpretation \\ \hline \hline
    \emph{p-value} $> 0.1$ & No evidence against $H_0$ based on the observed data. \\ \hline
    $0.05 < \emph{p-value} \leq 0.10$ & Weak evidence against $H_0$ based on the observed data. \\ \hline
    $0.01 < \emph{p-value} \leq 0.05$ & Evidence against $H_0$ based on the observed data. \\ \hline
    $0.001 < \emph{p-value} \leq 0.01$ & Strong evidence against $H_0$ based on the observed data. \\ \hline
    \emph{p-value} $\leq 0.001$ & Very strong evidence against $H_0$ based on the observed data. \\ \hline
  \end{tabular}
  \end{center}
  \end{table}
  If the \emph{p-value} is not small, it \textbf{cannot be concluded that} $H_{0}$ \textbf{is true}.
  It can only be said that there is \textbf{no evidence against the null hypothesis in light of the observed data}. \\

  \textbf{Confidence Interval vs. Hypothesis Testing} \\
  \emph{Confidence interval} is the range of ``reasonable'' values for $\theta$, given the level of confidence and sample data. \\
  \emph{Hypothesis testing} tests whether a particular value of $\theta$ is ``reasonable'' given the \emph{p-value} and sample data.

  \subsection{Tests of Hypotheses for Parameters in the $G(\mu, \sigma)$ Model}
  \textbf{Hypothesis Tests for} $\mu$ \\
  Using the test statistic
  $$D = \frac{|\bar{Y} - \mu_0|}{S/\sqrt{n}}$$
  Then using the sample mean $\bar{y}$ and standard deviation $s$, we get
  $$d = \frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}$$
  The \emph{p-value} can be then obtained via
  \begin{align*}
    \emph{p-value} &= P(D \geq d) \\
    &= P(|T| \geq d) \\
    &= 1 - P(-d \leq T \leq d) \\
    &= 2[1 - P(T \leq d)] ~~~\text{where } T \sim t_{n-1}
  \end{align*}

  \textbf{One-sided hypothesis tests} \\
  Suppose that the null hypothesis is $H_{0} : \mu = \mu_{0}$ and the alternative hypothesis is $H_{1} : \mu > \mu_{0}$. \\
  To test $\mu = \mu_{0}$, use the same test statistic and observed value.
  Then \emph{p-value} can be obtained via
  \begin{align*}
    \emph{p-value} &= P(D \geq d) \\
    &= P(T \geq d) \\
    &= 1 - P(T \leq d) ~~~\text{where } T \sim t_{n-1}
  \end{align*}

  \textbf{Relationship Between Hypothesis Testing and Interval Estimation} \\
  Suppose $y_1, y_2, \dots, y_n$ is an observed random sample from the $G(\mu, \sigma)$ distribution. \\
  Suppose $H_0 : \mu = \mu_0$ is tested, and we have
  $$\emph{p-value} \geq 0.05$$
  $$\text{if and only if } P\left(\frac{|\bar{Y} - \mu_0|}{S/\sqrt{n}} \geq \frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}; H_0 : \mu = \mu_0 \text{is true}\right) \geq 0.05$$
  $$
  \text{if and only if } P\left(|T| \geq \vphantom{\frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}}\right.
  \underbrace{\frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}}_{\text{b}}
  \left.\vphantom{\frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}}\right) \geq 0.05 ~~~\text{where } T \sim t_{n-1}
  $$
  $$
  \text{if and only if } P\left(|T| \leq \vphantom{\frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}}\right.
  \underbrace{\frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}}_{\text{a}}
  \left.\vphantom{\frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}}\right) \leq 0.95
  $$
  $$\text{if and only if } \frac{|\bar{y} - \mu_0|}{s/\sqrt{n}} \leq a ~~~\text{where } P(|T| \leq a) = 0.95$$
  $$\text{if and only if } \mu_0 \in \left[~\bar{y} - a\frac{s}{\sqrt{n}}, \bar{y} + a\frac{s}{\sqrt{n}}~\right]$$
  which is a $95\%$ confidence interval for $\mu$. \\

  In general, suppose we have data $\textbf{y}$, a model $f(\textbf{y},\theta)$ and we use the same pivotal quantity to construct a confidence interval for $\theta$ and a test of the hypothesis $H_0 : \mu = \mu_0$. \\
  Then the parameter value $\theta = \theta_0$ is inside a $100q\%$ confidence interval for $\theta$ if and only if the \emph{p-value} for testing $H_0 : \mu = \mu_0$ is greater than $1 - q$. \\

  The disadvantage is that we need to construct the appropriate test statistics $D$ and that may be difficult if the original distribution is complicated. \\

  \textbf{Hypothesis tests for} $\sigma$ \\
  For testing $H_0 : \sigma = \sigma_0$, use the test statistic
  $$U = \frac{(n-1)S^{2}}{\sigma_{0}^{2}}$$
  Note that for large values of $U$ and small values of $U$ provide evidence against $H_0$ due to the asymmetric shape of Chi-squared distributions. \\
  To approximate the \emph{p-value}:
  \begin{itemize}
    \item[1.] Let $u = (n-1)s^{2}/\sigma_{0}^{2}$ denote the observed value of $U$ from the data
    \item[2.] If $u$ is large (that is, if $P(U \leq u) > 0.5$) compute the \emph{p-value} as
    $$\emph{p-value} = 2P(U \geq u)$$
    where $U \sim \chi^{2}_{n-1}$
    \item[3.] If $u$ is small (that is, if $P(U \leq u) < 0.5$) compute the \emph{p-value} as
    $$\emph{p-value} = 2P(U \leq u)$$
    where $U \sim \chi^{2}_{n-1}$
  \end{itemize}

  \subsection{Likelihood Ratio Tests of Hypotheses - One Parameter}
  When a pivotal quantity does not exist then a general method for finding a test statistic with good properties can be based on the likelihood function.
  \begin{theorem}
    Suppose
    \begin{align*}
      \theta &= ~\text{unknown parameter} \\
      n &= ~\text{sample size} \\
      \hat{\theta} &= ~\text{MLE for $\theta$} \\
      \widetilde{\theta} &= ~\text{Maximum Likelihood Estimator} \\
      H_{0} &: ~\theta = \theta_{0} \\
      H_{1} &: ~\theta \not = \theta_{0}
    \end{align*}
    Then for large n, the Likelihood Ratio Test Statistic is
    \begin{align*}
    \Lambda(\theta_{0}) &= -2\ln{\frac{L(\theta_{0})}{L(\widetilde{\theta})}} \sim X_{1}^{2} \\
    \Lambda(\theta_{0}) &= 2[L(\widetilde{\theta}) - L(\theta_{0})]
    \end{align*}
    Using the observed value of $\Lambda(\theta_{0})$, denoted by
    $$
    \lambda(\theta_{0})
    = -2\ln{\left[\frac{L(\theta_{0})}{L(\hat{\theta})}\right]}
    = -2\ln{R(\theta_0)}
    $$
    where $R(\theta_0)$ is the relative likelihood function evaluated at $\theta = \theta_0$. \\
    The \emph{p-value} can then be approximated via
    \begin{align*}
      \emph{p-value}
      &\approx P[W\geq \lambda(\theta_0)]~~~\text{where }W\sim \chi_{1}^{2} \\
      &= P\left(|Z|\geq \sqrt{\lambda(\theta_0)}\right) ~~~\text{where }Z\sim G(0, 1) \\
      &= 2\left[1 - P(Z \leq \sqrt{\lambda(\theta_0)}\right]
    \end{align*}
  \end{theorem}

  \subsubsection{Likelihood Ratio Test Statistic for Binomial}
  \begin{align*}
    \lambda(\theta_0)
    &= -2\ln{\left[\left(\frac{\theta_0}{\hat{\theta}}\right)^{y}\left(\frac{1 - \theta_0}{1 - \hat{\theta}}\right)^{n - y}\right]}
  \end{align*}
  where $\hat{\theta} = y/n$

  \subsubsection{Likelihood Ratio Test Statistic for Exponential}
  Suppose $y_1, y_2, \dots, y_n \sim \text{Exponential}(\theta)$
  \begin{align*}
    \lambda(\theta_0) = -2
    \ln{
    \left[
      \left(\frac{\hat{\theta}}{\theta_0}\right)^{n}
      e^{n(1 - \hat{\theta}/\theta_0)}
    \right]
    }
  \end{align*}

  \subsubsection{Likelihood Ratio Test Statistic and $G(\mu, \sigma)$}
  Suppose $Y \sim G(\mu, \sigma)$ with p.d.f.
  $$
  f(y; \mu, \sigma) =
  \frac{1}{\sqrt{2\pi}\sigma}
  \text{exp}\left[-\frac{1}{2\sigma^{2}}(y - \mu)^{2}\right]
  $$
  Then the likelihood ratio test statistic is
  $$
  \Lambda(\theta_0) =
  \left(\frac{\bar{Y} - \mu_0}{\sigma/\sqrt{n}}
  \right)^{2}
  $$
  Notice that $\Lambda(\theta_0)$ is the square of the standard Normal Distribution random variable
  $$\frac{\bar{Y} - \mu_0}{\sigma/\sqrt{n}}$$
  Therefore, it has exactly a $\chi_{1}^{2}$ distribution.

\end{document}
