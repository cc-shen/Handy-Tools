\documentclass[12pt, leqno]{article}
\usepackage[margin = 1.5in]{geometry}
\setlength{\parindent}{0in}
\usepackage{amsfonts, amssymb, amsthm, mathtools, tikz, qtree, float}
\usepackage[lined]{algorithm2e}
\usepackage[T1]{fontenc}
\usepackage{ae, aecompl, color}
\usepackage[pdftex, pdfauthor={Charles Shen}, pdftitle={STAT 231: Statistics}, pdfsubject={Notes from STAT 231: at the University of Waterloo}, pdfkeywords={course notes, notes, Waterloo, University of Waterloo}, pdfproducer={LaTeX}, pdfcreator={pdflatex}]{hyperref}
\usepackage{cleveref}
\usepackage{wrapfig}
\allowdisplaybreaks
\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace}

\definecolor{darkish-blue}{RGB}{25,103,185}

\hypersetup{
    colorlinks,
    citecolor=darkish-blue,
    filecolor=darkish-blue,
    linkcolor=darkish-blue,
    urlcolor=darkish-blue
}

\theoremstyle{definition}
\newtheorem*{defn}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{corollary}{Corollary}
\newtheorem{ex}{Example}[section]

\crefname{ex}{Example}{Example}

\setlength{\marginparwidth}{1.5in}
\newcommand{\lecture}[1]{\marginpar{{\footnotesize $\leftarrow$ \underline{#1}}}}

\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

\begin{document}
  \let\ref\Cref
  \pagenumbering{roman}
  \title{\bf{STAT 231: Statistics}}
  \date{Spring 2016, University of Waterloo \\ \center Formulas and Notes.}
  \author{Charles Shen}

  \blfootnote{Feel free to email feedback to me at \href{mailto:ccshen902@gmail.com}{ccshen902@gmail.com}.}

  \maketitle
  \begin{center}
    \textbf{Assume all $\log$ are in base $e$ unless specified.} \\
    I've tried to use $\ln$ for consistency, \\
    but there may be a few inconsistency.
  \end{center}
  \newpage
  \tableofcontents
  \newpage
  \pagenumbering{arabic}

  \section{Numerical Summaries}
  \subsection{Measure of Location}
  \subsubsection{Mean}
  The \emph{sample mean}, also called the sample average is
  $$\bar{y} = \frac{1}{n}\sum_{i=1}^{n}y_{i}$$
  \subsubsection{Median}
  The \emph{sample median} $\hat{m}$ is the middle value(s) of an ordered sample. \\
  Median is less affected by a few extreme observations so it is a more robust measure of location. \\
  It is also the second quartile (\ref{iqr}).
  \subsubsection{Mode}
  The \emph{sample mode} is the most common value of $y$ in a sample; it may not be unique when there are multiple values of same frequency.

  \subsection{Measure of Dispersion or Variability}
  \subsubsection{Variance and Standard Deviation}
  The \emph{sample variance} is roughly the average of the squared deviation from the mean:
  \begin{align*}
  s^{2} &= \frac{1}{n-1}\sum_{i=1}^{n}(y_{i} - \bar{y})^{2} \\
  &= \frac{1}{n-1}
  \left[
  \sum_{i=1}^{n}y_{i}^{2} - n(\bar{y})^{2}
  \right]
  \end{align*}
  In addition, \emph{standard deviation} is then
  $$s = \sqrt{s^{2}}$$
  \subsubsection{Range}
  Range is the difference between highest and lowest value in the sample
  $$range = y_{(n)} - y_{(1)}$$
  where
  $$y_{(1)} = min(y_{1}, \dots, y_{n})$$
  and
  $$y_{(n)} = max(y_{1}, \dots, y_{n})$$
  \subsubsection{Quantiles and Interquartile Range} \label{iqr}
  \begin{defn}
    Let $\{y_{(1)}, y_{(2)}, \dots, y_{(n)}\}$ where $y_{(1)} \leq y_{(2)} \leq \dots \leq y_{(n)}$ be the ordered statistic for the data set $\{y_1, y_2, \dots, y_n\}$.
    The $pth$ quantile (also called the $100pth$ percentile) is a value, call it $q(p)$, determined as follows:
    \begin{itemize}
    \item Let $m = (n+1)p$ where $n$ is the sample size
    \item If $m$ is an integer between $1$ and $n$, then $q(p) = y_{(m)}$ which is the $mth$ largest value in the data set
    \item If $m$ is not an integer but $1 < m < n$ then determine the closest integer $j$ such that $j < m < j+1$ and take $q(p) = \frac{1}{2}[y_{(j)} + y_{(j+1)}]$
    \end{itemize}
  \end{defn}
  The first (lower) quartile is $q(0.25)$, also the 25\textsuperscript{th} percentile. \\
  The second quartile is $q(0.50)$, also the 50\textsuperscript{th} percentile and the median. \\
  The third(upper) quartile is $q(0.75)$, also the 75\textsuperscript{th} percentile. \\

  The \emph{interquartile range} is $IQR = q(0.75) - q(0.25)$

  \subsection{Measure of Shape}
  \subsubsection{Skewness}
  The skewness $g_{1}$ can be measured precisely by
  \begin{align*}
  g_{1} =
  \frac{\frac{1}{n}\sum_{i=1}^{n}(y_{i} - \bar{y})^{3}}{\left[\frac{1}{n}\sum_{i=1}^{n}(y_{i} - \bar{y})^{2}\right]^{3/2}}
  \end{align*}
  It's a measure on the (lack of) symmetry in the data. \\
  If $g_{1} = 0$, then data is symmetric. \\
  If $g_{1} < 0$, then data is left skewed (long left tail). \\
  If $g_{1} > 0$, then data is right skewed (long right tail). \\
  A quick estimate on skewness is $mean - median$
  \subsubsection{Kurtosis}
  $$g_{2} = \frac{\frac{1}{n}\sum_{i=1}^{n}(y_{i} - \bar{y})^{4}}{\left[\frac{1}{n}\sum_{i=1}^{n}(y_{i} - \bar{y})^{2}\right]^{2}}$$
  measures the heaviness of the tails and the peakedness of the data relative to data that are Normally distributed. \\
  Kurtosis is always positive. \\
  For the Normal distribution, kurtosis is equal to 3. \\
  If $g_{2} < 3$, then more stacked peaks and smaller tails. \\
  If $g_{2} > 3$, then more peaked center and heavier tails.

  \subsection{More Definitions}
  \subsubsection{Five Numbers Summary}
  \begin{defn}
    The five number summary of a data set consists of the three quartiles and the minimum and maximum values of the data set. \\
    That is, $q(0.25)$, $q(0.5)$, $q(0.75)$, $y_{(1)}$, and $y_{(n)}$.
  \end{defn}
  \subsubsection{Correlation}
  \begin{defn}
  The sample \emph{correlation}, denoted by $r$, for data $\{(x_{1},y_{1}), (x_{2},y_{2}), \dots, (x_{n},y_{n})\}$ is
  $$
    r = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}
  $$
  where
  \begin{align*}
    S_{xx} &= \sum_{i=1}^{n}(x_{i} - \bar{x})^{2}
    = \sum_{i=1}^{n}x_{i}^{2} - n(\bar{x})^2 \\
    S_{xy} &= \sum_{i=1}^{n}(x_{i} - \bar{x})(y_{i} - \bar{y})
    = \sum_{i=1}^{n}x_{i}y_{i} - n\bar{x}\bar{y} \\
    S_{yy} &= \sum_{i=1}^{n}(y_{i} - \bar{y})^{2}
    = \sum_{i=1}^{n}y_{i}^{2} - n(\bar{y})^2 \\
  \end{align*}
  \end{defn}
  If the value of $r$ is close to $1$, then there is a strong positive linear relationship. \\
  If the value of $r$ is close to $-1$, then there is a strong negative linear relationship. \\
  If the value of $r$ is close to $0$, then there is no linear relationship between the two variates.

  \section{Distribution Theory}
  If $Y_{1}, Y_{2}, \dots, Y_{n} ~ N(\mu, \sigma^{2})$ and they're independent, then
  $$\bar{Y} \sim N(\mu, \frac{\sigma^{2}}{n})$$
  and
  $$\frac{\bar{Y} - \mu}{\sigma/\sqrt{n}} = Z \sim N(0, 1) $$
  For large $n$, then
  $$\frac{\bar{Y} - \mu}{s/\sqrt{n}} \approx Z \sim N(0, 1) $$

  \section{Statistical Models and Maximum Likelihood Estimation}
  \begin{defn}
    \emph{The \textbf{relative likelihood function} is defined as}
    $$R(\theta) = \frac{L(\theta)}{L(\hat{\theta})} ~~~for~\theta \in \Omega$$
    $Note~that~0 \leq R(\theta) \leq 1~for~all~\theta \in \Omega$.
  \end{defn}
  \begin{defn}
    \emph{The \textbf{log likelihood function} is defined as}
    $$l(\theta) = \ln L(\theta) ~~~for~\theta \in \Omega$$
  \end{defn}
  \subsection{Likelihood Function for Binomial Distribution}
  The maximum likelihood estimate of $\theta$ is $\bar{\theta} = y / n$.

  \subsection{Likelihood Function for Poisson Distribution}
  The value $\theta = \bar{y}$ maximizes $l(\theta)$ and so $\hat{\theta} = \bar{y}$ is the maximum likelihood estimate of $\theta$.

  \subsection{Likelihood Function for Exponential Distribution}
  The value $\theta = \bar{y}$ maximizes $l(\theta)$ and so $\hat{\theta} = \bar{y}$ is the maximum likelihood estimate of $\theta$ for an Exponential Distribution $\sim Exp(\theta)$.

  \subsection{Likelihood Function for Gaussian Distribution}
  The maximum likelihood estimate of $\theta$ is $\hat{\theta} = (\hat{\mu}, \hat{\sigma})$, where
  $$
  \hat{\mu} = \frac{1}{n}\sum_{i=1}^{n}y_{i} = \bar{y}
  ~~\text{and}~~
  \hat{\sigma} = \left[\frac{1}{n}\sum_{i=1}^{n}(y_{i} - \bar{y})^{2}\right]^{1/2}
  $$
  Note that $\hat{\sigma} \not = \sigma$ (sample variance).

  \subsection{Invariance Property of Maximum Likelihood Estimates}
  \begin{theorem}
    \emph{If $\hat{\theta}$ is the maximum likelihood estimate of $\theta$, then $g(\hat{\theta})$ is the maximum likelihood estimate of $g(\theta)$.}
  \end{theorem}

  \section{Estimation}
  \subsection{Confidence Intervals and Pivotal Quantities}
  In general, construct a pivot using the estimator, use that to construct coverage interval, estimate it and find the confidence interval.

  \begin{defn}
    A $100p\%$, where $0 \leq p \leq 1$, confidence interval tells $100p\%$ of the intervals constructed from samples will contain the true unknown value of $\mu$ (or $\sigma$).
  \end{defn}

  To determine the correct value to look for in the distribution tables, calculate $(1+p)/2$ where $100p\%$ is the level of confidence. \\
  For example, the $95\%$ confidence interval needs to look at $\frac{1+0.95}{2} = 0.975$.

  \begin{theorem}
  \textbf{Central Limit Theorem} \\
    If $n$ is large, and if $Y_{1}, \dots, Y_{n}$ are drawn from a distribution with mean $\mu$ and variance $\sigma^{2}$, then $\bar{Y} \sim N\left(\mu, \frac{\sigma^{2}}{n}\right)$.
  \end{theorem}

  For a Binomial Distribution, the confidence interval is
  $$\left[~\hat{\pi} \pm z^{*}\sqrt{\frac{\hat{\pi}(1 - \hat{\pi})}{n}}~\right]$$
  where $\hat{\pi} = \frac{y}{n}$, $y$ is the observed data. \\
  To determine the sample size
  $$n \geq \left(\frac{z^{*}}{MoE}\right)^{2}\hat{\pi}(1 - \hat{\pi})$$
  where $MoE$ is the margin of error. \\
  To be conservative, we usually pick $\hat{\pi} = 0.5$ as it maximizes $\hat{\pi}(1 - \hat{\pi})$. \\

  For a Poisson Distribution, $Y_1, Y_2, \dots, Y_n \sim Poi(\mu)$, the pivotal quantity is
  $$\frac{\bar{Y} - \mu}{\sqrt{\frac{\bar{Y}}{n}}} = Z \sim N(0,1)$$
  and the confidence interval is
  $$\left[~\bar{y} \pm z^{*}\sqrt{\frac{\bar{y}}{n}}~\right]$$

  For an Exponential Distribution $Y_1, Y_2, \dots, Y_n \sim Exp(\mu)$, for large $n$, the pivotal quantity is
  $$\frac{\bar{Y} - \mu}{\mu / \sqrt{n}} = Z \sim N(0, 1)$$
  and the confidence interval is then
  $$\left[~ \frac{\bar{Y}}{1 + z^{*}\frac{1}{\sqrt{n}}}, \frac{\bar{Y}}{1 - z^{*}\frac{1}{\sqrt{n}}} ~\right]$$
  Otherwise, consider
  $$\sum_{i=1}^{n}\frac{2Y_{i}}{\mu} \sim \chi_{2n}^{2}$$
  and
  $$P\left(a \leq \sum_{i=1}^{n}\frac{2Y_{i}}{\mu} \leq b\right) = p$$

  \subsection{Chi-Squared Distribution $\sim X_{k}^{2}$}
  The Gamma function is
  $$\Gamma(\alpha) = \int_{0}^{\infty}y^{\alpha -1}e^{-y}dy ~~\text{for}~\alpha > 0$$
  Properties of the Gamma Function:
  \begin{itemize}
    \item $\Gamma(\alpha) = (\alpha - 1)\Gamma(\alpha - 1)$
    \item $\Gamma(\alpha) = (\alpha - 1)!$
    \item $\Gamma(1/2) = \sqrt{\pi}$
  \end{itemize}
  The $X_{k}^{2}$ distribution is a continuous family of distributions on $(0, \infty)$ with probability density function
  $$f(x;k) = \frac{1}{2^{k/2}\Gamma(k/2)}x^{(k/2)-1}e^{-x/2}~~~\text{for}~x > 0$$
  where $k \in \{1, 2, \dots\}$ is a parameter of the distribution. \\
  $k$ is referred to as the ``degrees of freedom'' (d.f) parameter. \\

  For $X \sim X_{k}^{2}$
  \begin{itemize}
    \item $E(X) = k$ and $Var(X) = 2k$
    \item If $k = 1$, $X = Z^{2}$ and $Z \sim G(0,1)$
    \item If $k = 2$, $X \sim Exp(2)$ ($\theta = 2$)
    \item If $k$ is large, $X \stackrel{Appr.}{\sim} N(k, 2k)$
    \item Let $X_{k_{1}}, X_{k_{1}}$ be independent random variables with $X_{k_{i}} \sim X_{k_{i}}^{2}$. \\
    Then $X_{k_{1}} + X_{k_{2}} =  X_{k_{1} + k_{2}}^{2}$.
  \end{itemize}

  \begin{theorem}
    If $Y_{i} \sim Exp(\mu)$, then
    $$\frac{2Y_{i}}{\mu} \sim Exp(2) \rightarrow X_{2}^{2}$$
  \end{theorem}

  \subsection{Student's \emph{t} Distribution}
  Student's $t$ distribution has probability density function
  $$
  f(t;k) = c_{k}\left(1 + \frac{t^{2}}{k}\right)^{-(k+1)/2}
  ~~~\text{for}~t \in \Re~\text{and}~ k = 1, 2, \dots
  $$
  where the constant $c_{k}$ is given by
  $$
  c_{k} = \frac{\Gamma(\frac{k+1}{2})}{\sqrt{k\pi}\Gamma(\frac{k}{2})}
  ~~~k\text{ is the }degrees~of~freedom
  $$
  Properties of $T$:
  \begin{itemize}
    \item[i)] Range of $T$: $(-\infty, \infty)$
    \item[ii)] T is symmetric around 0
    \item[iii)] As $k \uparrow$, $T \rightarrow Z$
  \end{itemize}

  \begin{theorem}
    \emph{Suppose $Z \sim G(0,1)$ and $U \sim X_{k}^{2}$ independently. Let}
    $$T = \frac{Z}{\sqrt{U / k}}$$
    $$\rightarrow \frac{\bar{Y} - M}{s / \sqrt{n}} \sim t_{n-1}$$
    \emph{Then T has \textbf{Student's} t \textbf{distribution with} k \bf{degrees of freedom}}.
  \end{theorem}

  \subsection{Likelihood-Based Confidence Intervals}
  \begin{theorem}
    $A~100p\%~likelihood~interval~is~an~approximate~100q\%~where~q = 2P(Z \leq \sqrt{-2\ln p}) - 1~and~Z \sim N(0,1).$
  \end{theorem}

  \begin{ex}
    Show that a 1\% likelihood interval is an approximate 99.8\% confidence interval. \\
    Note that $p = 0.01$
    \begin{align*}
      q &= 2P(Z \leq \sqrt{-2\ln(0.01)}) - 1 \\
        &\approx 2P(Z \leq 3.03) - 1 \\
        &= 2(0.99878) - 1 \\
        &= 0.998 = 99.8\%
    \end{align*}
  \end{ex}

  \begin{theorem}
    \emph{If a is a value such that}
    $$P = 2P(Z \leq a) - 1 ~~ where ~ Z \sim N(0,1)$$
    \emph{then the likelihood interval }$\{\theta : R(\theta) \geq e^{-a^{2}/2}\}$ \emph{is an approximate $100p\%$ confidence interval.}
  \end{theorem}

  \begin{ex}
    Since
    $$0.95 = 2P(Z \leq 1.96) - 1 ~\text{where}~Z\sim N(0,1)$$
    and
    $$e^{-(1.96)^{2}/2} = e^{-1.9208} \approx 0.1465 \approx 0.15$$
    therefore a $15\%$ \emph{likelihood interval} for $\theta$ is also an approximate $95\%$ \emph{confidence interval} for $\theta$.`'
  \end{ex}

  \subsection{Confidence Intervals for Parameters in the $G(\mu, \sigma)$ Model}
  If $Y_{1}, \dots, Y_{n}$ are independent $N(\mu, \sigma^{2})$, then $\bar{Y} \sim N(\mu, \frac{\sigma^{2}}{n})$ and
  \begin{align}
     \frac{\bar{Y} - \mu}{s / \sqrt{n}}~ &\sim t_{n-1} \\
     \frac{(n-1)S^{2}}{\sigma^{2}} ~&\sim X_{n-1}^{2}
  \end{align}

  General Rule: \\
  The Confidence Interval for $\mu$ if $\sigma$ is unknown is
  $$\left[~\bar{y} \pm t^{*}\frac{s}{\sqrt{n}}~\right]$$
  When $\sigma$ is unknown, we replace $\sigma$ by its estimate $s$, and we use t-pivot. \\
  Confidence interval when $\sigma$ is known is
  $$\left[~\bar{y} \pm z^{*}\frac{\sigma}{\sqrt{n}}~\right]$$
  When $\sigma$ is known, we use z-pivot. \\

  If $n$ is really large, then the $t^{*}$ value converges to the corresponding $z^{*}$ value (by Central Limit Theorem). \\

  \textbf{Confidence Intervals for $\sigma^{2}$ and $\sigma$}
  \begin{theorem}
    \emph{Suppose} $Y_{1}, Y_{2}, \dots, Y_{n}$ \emph{is a random sample from the} $G(\mu, \sigma)$ \emph{distribution with sample variance} $S^{2}$.
    \emph{Then the random variable}
    $$\frac{(n-1)S^{2}}{\sigma^{2}} = \frac{1}{\sigma^{2}}\sum_{i=1}^{n}(Y_{i} - \bar{Y})^{2}$$
    \emph{has a Chi-squared distribution with} $n - 1$ \emph{degrees of freedom}.
  \end{theorem}
  Using the theorem, we can construct a $100p\%$ confidence interval for the parameter $\sigma^{2}$ or $\sigma$. \\
  Recall this is the same as the equation (2) in this sub-section. \\
  We can find constants $a$ and $b$ such that
  $$P(a \leq U \leq b) = p$$
  where $U \sim X_{n-1}^{2}$. \\
  So a $100p\%$ confidence interval for $\sigma^{2}$ is
  $$\left[~\frac{(n-1)s^{2}}{b},\frac{(n-1)s^{2}}{a}~\right]$$
  and a $100p\%$ confidence interval for $\sigma$ is
  $$\left[~\sqrt{\frac{(n-1)s^{2}}{b}},\sqrt{\frac{(n-1)s^{2}}{a}}~\right]$$ \\

  Unlike confidence interval for $\mu$, the confidence interval for $\sigma^{2}$ is \emph{not symmetric} about $s^{2}$. the estimator of $\sigma^{2}$.
  The $X_{n-1}^{2}$ distribution is not a symmetric distribution. \\

  \textbf{Prediction Interval for a Future Observation} \\
  Suppose that $Y \sim G(\mu,\sigma)$ with \textbf{independent} observations, then
  $$Y - \widetilde{\mu} = Y - \bar{Y} \sim N\left(0, \sigma^{2}\left(1 + \frac{1}{n}\right)\right)$$
  Also
  $$\frac{Y - \bar{Y}}{S\sqrt{1 + \frac{1}{n}}} \sim t_{n-1}$$
  is a pivotal quantity which can be used to obtain an interval of values for $Y$. \\
  Let $t^{*}$ be a value such that $P(-t^{*} \leq T \leq t^{*}) = p$ or $P(T \leq t^{*}) = (1+p)/2$ which is obtained from tables. Thus
  $$\left[ ~\bar{y} \pm t^{*}s\sqrt{1 + \frac{1}{n}} ~\right]$$

  \section{Tests of Hypothesis}
  \begin{defn}
    A \emph{hypothesis} in statistic is a claim made about the values of a certain parameter of the population.
  \end{defn}

  There are \textbf{two} competing hypotheses:
  \begin{itemize}
    \item \emph{Null} Hypothesis, denoted $H_{0}$; current ``status quo'' assumption.
    \item \emph{Alternative} Hypothesis, denoted $H_{1}$; seeks to challenge $H_{0}$.
  \end{itemize}

  \begin{defn}
    A \emph{test statistic \emph{or} discrepancy measure} $D$ is a function of the data \textbf{Y} that is constructed to measure the degree of ``agreement'' between the data \textbf{Y} and the null hypothesis $H_{0}$.
  \end{defn}

  For every testing decision, there is a possibility of making two kinds of errors:
  \begin{itemize}
    \item[\textbf{Type I}] $H_{0}$ is true; $H_{0}$ is rejected.
    \item[\textbf{Type II}] $H_{1}$ is true; $H_{0}$ is not rejected.
  \end{itemize}
  If Type I error goes down, then Type II error goes up; vice versa holds as well.

  \subsection{p-value}
  Suppose there's the test statistic $D = D(\textbf{Y})$ to test the hypothesis $H_{0}$. \\
  Also suppose that $d = D(\textbf{y})$ is the observed value of $D$.
  \begin{defn}
    A \emph{p-value} or observed significance level of the test of hypothesis $H_{0}$ using test statistic $D$ is
    $$\emph{p-value} = P(D \geq d; H_{0})$$
  \end{defn}
  \textbf{Caution}: The \emph{p-value} is \textbf{not} the probability that $H_{0}$ is true.
  \begin{table}[H]
  \caption{Interpretation of \emph{p-values}}
  \begin{center}
  \begin{tabular}{c | p{9cm}}
    \bf \emph{p-value} & \bf Interpretation \\ \hline \hline
    \emph{p-value} $> 0.1$ & No evidence against $H_0$ based on the observed data. \\ \hline
    $0.05 < \emph{p-value} \leq 0.10$ & Weak evidence against $H_0$ based on the observed data. \\ \hline
    $0.01 < \emph{p-value} \leq 0.05$ & Evidence against $H_0$ based on the observed data. \\ \hline
    $0.001 < \emph{p-value} \leq 0.01$ & Strong evidence against $H_0$ based on the observed data. \\ \hline
    \emph{p-value} $\leq 0.001$ & Very strong evidence against $H_0$ based on the observed data.
  \end{tabular}
  \end{center}
  \end{table}
  If the \emph{p-value} is not small, it \textbf{cannot be concluded that} $H_{0}$ \textbf{is true}.
  It can only be said that there is \textbf{no evidence against the null hypothesis in light of the observed data}. \\

  \textbf{Confidence Interval vs. Hypothesis Testing} \\
  \emph{Confidence interval} is the range of ``reasonable'' values for $\theta$, given the level of confidence and sample data. \\
  \emph{Hypothesis testing} tests whether a particular value of $\theta$ is ``reasonable'' given the \emph{p-value} and sample data.

  \subsection{Tests of Hypotheses for Parameter in the Poi($\mu$) Model}
  Suppose $Y_{1}, Y_{2}, \dots, Y_{n} \sim Poi(\mu)$ \\
  $H_{0}: \mu = \mu_{0}$ and $H_{1}: \mu \not = \mu_{0}$ \\
  Mean and Variance both are $\mu$ \\
  By Central Limit Theorem, $\bar{Y} \sim Poi(\mu, \frac{\mu}{n})$ \\
  Thus the test statistic $D$ is
  \begin{align*}
  &\frac{\bar{Y} - \mu}{\sqrt{\mu / n}} = Z \sim N(0,1) \\
  \rightarrow~~~ &\frac{\bar{y} - \mu_{0}}{\sqrt{\mu_{0} / n}} = Z \sim N(0,1)
  \end{align*}

  \subsection{Tests of Hypotheses for Parameters in the $G(\mu, \sigma)$ Model}
  \textbf{Hypothesis Tests for} $\mu$ \\
  Using the test statistic
  $$D = \frac{|\bar{Y} - \mu_0|}{S/\sqrt{n}}$$
  Then using the sample mean $\bar{y}$ and standard deviation $s$, we get
  $$d = \frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}$$
  The \emph{p-value} can be then obtained via
  \begin{align*}
    \emph{p-value} &= P(D \geq d) \\
    &= P(|T| \geq d) \\
    &= 1 - P(-d \leq T \leq d) \\
    &= 2[1 - P(T \leq d)] ~~~\text{where } T \sim t_{n-1}
  \end{align*}

  \textbf{One-sided hypothesis tests} \\
  Suppose that the null hypothesis is $H_{0} : \mu = \mu_{0}$ and the alternative hypothesis is $H_{1} : \mu > \mu_{0}$. \\
  To test $\mu = \mu_{0}$, use the same test statistic and observed value.
  Then \emph{p-value} can be obtained via
  \begin{align*}
    \emph{p-value} &= P(D \geq d) \\
    &= P(T \geq d) \\
    &= 1 - P(T \leq d) ~~~\text{where } T \sim t_{n-1}
  \end{align*}

  \textbf{Relationship Between Hypothesis Testing and Interval Estimation} \\
  Suppose $y_1, y_2, \dots, y_n$ is an observed random sample from the $G(\mu, \sigma)$ distribution. \\
  Suppose $H_0 : \mu = \mu_0$ is tested, and we have
  $$\emph{p-value} \geq 0.05$$
  $$\text{if and only if } P\left(\frac{|\bar{Y} - \mu_0|}{S/\sqrt{n}} \geq \frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}; H_0 : \mu = \mu_0 \text{is true}\right) \geq 0.05$$
  $$
  \text{if and only if } P\left(|T| \geq \vphantom{\frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}}\right.
  \underbrace{\frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}}_{\text{b}}
  \left.\vphantom{\frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}}\right) \geq 0.05 ~~~\text{where } T \sim t_{n-1}
  $$
  $$
  \text{if and only if } P\left(|T| \leq \vphantom{\frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}}\right.
  \underbrace{\frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}}_{\text{a}}
  \left.\vphantom{\frac{|\bar{y} - \mu_0|}{s/\sqrt{n}}}\right) \leq 0.95
  $$
  $$\text{if and only if } \frac{|\bar{y} - \mu_0|}{s/\sqrt{n}} \leq a ~~~\text{where } P(|T| \leq a) = 0.95$$
  $$\text{if and only if } \mu_0 \in \left[~\bar{y} - a\frac{s}{\sqrt{n}}, \bar{y} + a\frac{s}{\sqrt{n}}~\right]$$
  which is a $95\%$ confidence interval for $\mu$. \\

  In general, suppose we have data $\textbf{y}$, a model $f(\textbf{y},\theta)$ and we use the same pivotal quantity to construct a confidence interval for $\theta$ and a test of the hypothesis $H_0 : \mu = \mu_0$. \\
  Then the parameter value $\theta = \theta_0$ is inside a $100q\%$ confidence interval for $\theta$ if and only if the \emph{p-value} for testing $H_0 : \mu = \mu_0$ is greater than $1 - q$. \\

  The disadvantage is that we need to construct the appropriate test statistics $D$ and that may be difficult if the original distribution is complicated. \\

  \textbf{Hypothesis tests for} $\sigma$ \\
  For testing $H_0 : \sigma = \sigma_0$, use the test statistic
  $$\frac{(n-1)S^{2}}{\sigma_{0}^{2}} = U \sim \chi_{n-1}^{2}$$
  Note that for large values of $U$ and small values of $U$ provide evidence against $H_0$ due to the asymmetric shape of Chi-squared distributions. \\
  To approximate the \emph{p-value}:
  \begin{itemize}
    \item[1.] Let $u = (n-1)s^{2}/\sigma_{0}^{2}$ denote the observed value of $U$ from the data
    \item[2.] If $u$ is large (that is, if $P(U \leq u) > 0.5$) compute the \emph{p-value} as
    $$\emph{p-value} = 2P(U \geq u)$$
    where $U \sim \chi^{2}_{n-1}$
    \item[3.] If $u$ is small (that is, if $P(U \leq u) < 0.5$) compute the \emph{p-value} as
    $$\emph{p-value} = 2P(U \leq u)$$
    where $U \sim \chi^{2}_{n-1}$
  \end{itemize}

  \subsection{Likelihood Ratio Tests of Hypotheses - One Parameter}
  When a pivotal quantity does not exist then a general method for finding a test statistic with good properties can be based on the likelihood function.
  \begin{theorem}
    Suppose
    \begin{align*}
      \theta &= ~\text{unknown parameter} \\
      n &= ~\text{sample size} \\
      \hat{\theta} &= ~\text{MLE for $\theta$} \\
      \widetilde{\theta} &= ~\text{Maximum Likelihood Estimator} \\
      H_{0} &: ~\theta = \theta_{0} \\
      H_{1} &: ~\theta \not = \theta_{0}
    \end{align*}
    Then for large n, the Likelihood Ratio Test Statistic is
    \begin{align*}
    \Lambda(\theta_{0}) &= -2\ln{\frac{L(\theta_{0})}{L(\widetilde{\theta})}} \sim X_{1}^{2} \\
    \Lambda(\theta_{0}) &= 2[L(\widetilde{\theta}) - L(\theta_{0})]
    \end{align*}
    Using the observed value of $\Lambda(\theta_{0})$, denoted by
    $$
    \lambda(\theta_{0})
    = -2\ln{\left[\frac{L(\theta_{0})}{L(\hat{\theta})}\right]}
    = -2\ln{R(\theta_0)}
    $$
    where $R(\theta_0)$ is the relative likelihood function evaluated at $\theta = \theta_0$. \\
    The \emph{p-value} can then be approximated via
    \begin{align*}
      \emph{p-value}
      &\approx P[W\geq \lambda(\theta_0)]~~~\text{where }W\sim \chi_{1}^{2} \\
      &= P\left(|Z|\geq \sqrt{\lambda(\theta_0)}\right) ~~~\text{where }Z\sim G(0, 1) \\
      &= 2\left[1 - P(Z \leq \sqrt{\lambda(\theta_0)}\right]
    \end{align*}
  \end{theorem}

  \subsubsection{Likelihood Ratio Test Statistic for Binomial}
  \begin{align*}
    \lambda(\theta_0)
    &= -2\ln{\left[\left(\frac{\theta_0}{\hat{\theta}}\right)^{y}\left(\frac{1 - \theta_0}{1 - \hat{\theta}}\right)^{n - y}\right]}
  \end{align*}
  where $\hat{\theta} = y/n$

  \subsubsection{Likelihood Ratio Test Statistic for Exponential}
  Suppose $y_1, y_2, \dots, y_n \sim \text{Exponential}(\theta)$
  \begin{align*}
    \lambda(\theta_0) = -2
    \ln{
    \left[
      \left(\frac{\hat{\theta}}{\theta_0}\right)^{n}
      e^{n(1 - \hat{\theta}/\theta_0)}
    \right]
    }
  \end{align*}

  \subsubsection{Likelihood Ratio Test Statistic for $G(\mu, \sigma)$}
  Suppose $Y \sim G(\mu, \sigma)$ with p.d.f.
  $$
  f(y; \mu, \sigma) =
  \frac{1}{\sqrt{2\pi}\sigma}
  \text{exp}\left[-\frac{1}{2\sigma^{2}}(y - \mu)^{2}\right]
  $$
  Then the likelihood ratio test statistic is
  $$
  \Lambda(\theta_0) =
  \left(\frac{\bar{Y} - \mu_0}{\sigma/\sqrt{n}}
  \right)^{2}
  $$
  Notice that $\Lambda(\theta_0)$ is the square of the standard Normal Distribution random variable
  $$\frac{\bar{Y} - \mu_0}{\sigma/\sqrt{n}}$$
  Therefore, it has exactly a $\chi_{1}^{2}$ distribution.

  \section{Simple Linear Regression Model}
  $Y_{i}$ is the \emph{Response Variate}; attribute whose variability we want to explain. \\
  $X_{i}$ is the \emph{Explanatory Variable} and is given.
  We want explain $Y$ by using $X$. \\

  The relevant degree of freedom for an additive model is
  $$= n - \text{number of unknown parameters in the systematic part of the model}$$

  Assumptions made:
  \begin{enumerate}
    \item Linearity: Mean of $Y$ is a linear function of $x$.\\
    $E(Y_{i}) = \alpha + \beta x_{i}$
    \item Variance is the same for any $x$; homoscedasticity. \\
    $\sigma^{2} = \sigma^{2}(x)$; heteroscedasticity.
    \item Normality: $Y_{i}$ are normally distributed. \\
    $Y_{i} = \text{Constant} + \text{Normal}$ \\
    $Y_{i} = \alpha + \beta x_{i} + R_{i}$ \\
    where $i = 1, \dots, n$, $R_{i} \sim N(\mu, \sigma^{2})$, $R_{i}$ independent
  \end{enumerate}

  Our model is independent $Y_{i}$ such that
  $$Y_{i} \sim N(\mu(x_{i}), \sigma^{2}) \text{ where } \mu(x_{i}) = \alpha + \beta x_{i}$$

  \subsection{Maximum Likelihood Estimators}
  Let
  $$a_{i} = \frac{x_{i} - \bar{x}}{S_{xx}}$$
  which has properties
  \begin{align*}
  \sum a_{i} &= 0 \\
  \sum a_{i}x_{i} &= 1 \\
  \sum a_{i}^{2} &= \frac{1}{S_{xx}}
  \end{align*}

  Also, we have
  \begin{align*}
  S_{xx} &= \sum_{i=1}^{n}(x_{i} - \bar{x})^{2}
  = \sum_{i=1}^{n}(x_{i} - \bar{x})x_{i}
  = \sum_{i=1}^{n}x_{i}^{2} - n\bar{x}^{2} \\
  S_{xy} &= \sum_{i=1}^{n}(x_{i} - \bar{x})(Y_{i} - \bar{Y})
  = \sum_{i=1}^{n}(x_{i} - \bar{x})Y_{i}
  = \sum_{i=1}^{n}x_{i}Y_{i} - n\bar{x}\bar{Y} \\
  S_{yy} &= \sum_{i=1}^{n}(Y_{i} - \bar{Y})^{2}
  = \sum_{i=1}^{n}Y_{i}^{2} - n\bar{Y}^{2}
  \end{align*}

  \subsubsection{$\beta$}
  \begin{align*}
  \widetilde{\beta} &= \frac{S_{xy}}{S_{xx}} \\
  &= \sum a_{i}y_{i}  ~~\text{where } a_{i} = \frac{x_{i} - \bar{x}}{S_{xx}}
  \end{align*}
  \textbf{Distribution for } $\widetilde{\beta}$ \\
  The mean is
  \begin{align*}
  E(\widetilde{\beta}) &= \sum_{i=1}^{n} a_{i}E(Y_{i}) = \sum_{i=1}^{n} a_{i}(\alpha + \beta x_{i}) \\
  &= \beta \sum_{i=1}^{n}a_{i}x_{i} ~~\text{since } \sum_{i=1}^{n} a_{i} = 0 \\
  &= \beta ~~\text{since } \sum_{i=1}^{n} a_{i}x_{i} = 1
  \end{align*}
  Similarly for variance is
  \begin{align*}
  Var(\widetilde{\beta}) &= \sum_{i=1}^{n} a_{i}^{2}Var(Y_{i}) \\
  &= \sigma^{2} \sum_{i=1}^{n} a_{i}^{2} \\
  &= \frac{\sigma^{2}}{S_{xx}} ~~\text{since } \sum_{i=1}^{n}a_{i}^{2} = \frac{1}{S_{xx}}
  \end{align*}
  Thus
  $$\widetilde{\beta} \sim N\left(\beta, \frac{\sigma^{2}}{S_{xx}}\right)$$

  \textbf{Confidence Interval for} $\beta$ \\
  If $\sigma$ is known
  $$\frac{\widetilde{\beta} - \beta}{\sigma / \sqrt{S_{xx}}} \sim N(0, 1)$$
  then a $100p\%$ confidence interval for $\beta$ is given by
  $$\left[~ \hat{\beta} \pm z^{*}\frac{\sigma}{\sqrt{S_{xx}}} ~\right]$$
  where
  $$P(|Z| \leq z^{*}) = p ~~~ Z \sim N(0, 1)$$
  \newline
  Otherwise,
  $$\frac{\widetilde{\beta} - \beta}{S_{e} / \sqrt{S_{xx}}} \sim t_{n-2}$$
  then a $100p\%$ confidence interval for $\beta$ is given by
  $$\left[~ \hat{\beta} \pm t^{*}\frac{\sigma}{\sqrt{S_{xx}}} ~\right]$$
  where
  $$P(|T| \leq t^{*}) = p ~~~ T \sim t_{n-2}$$
  \newline
  \textbf{Test of Hypothesis of No Relationship} \\
  To test the hypothesis of no relationship or $H_{0}: \beta = 0$, we use the test statistic
  $$D = \frac{|\widetilde{\beta} - 0|}{S_{e} / \sqrt{S_{xx}}}$$
  with observed value of
  $$d = \frac{|\hat{\beta} - 0|}{s_{e} / \sqrt{S_{xx}}}$$
  and p-value given by
  $$\text{p-value} = P\left(~|T| \geq \frac{|\hat{\beta} - 0|}{s_{e} / \sqrt{S_{xx}}}~\right)$$
  where $T \sim t_{n-2}$

  \subsubsection{$\alpha$}
  \begin{align*}
  \widetilde{\alpha} &= \bar{Y} - \widetilde{\beta}\bar{x} \\
  &= \bar{Y} - \left(\frac{S_{xy}}{S_{xx}}\right)\bar{x}
  \end{align*}
  \textbf{Distribution for } $\widetilde{\alpha}$ \\
  $$\widetilde{\alpha} \sim N\left(\alpha, \sigma^{2}\left(\frac{1}{n} + \frac{\bar{x}^{2}}{S_{xx}}\right)\right)$$

  \textbf{Confidence Interval for} $\alpha$ \\
  If $\sigma$ is known
  $$\frac{\widetilde{\alpha} - \alpha}{\sigma \sqrt{\frac{1}{n} + \frac{\bar{x}^{2}}{S_{xx}}}} \sim N(0, 1)$$
  then a $100p\%$ confidence interval for $\alpha$ is given by
  $$\left[~ \hat{\alpha} \pm z^{*}\sigma \sqrt{\frac{1}{n} + \frac{\bar{x}^{2}}{S_{xx}}} ~\right]$$
  where
  $$P(|Z| \leq z^{*}) = p ~~~\text{and } Z \sim N(0, 1)$$
  \newline
  Otherwise,
  $$\frac{\widetilde{\alpha} - \alpha}{S_{e}\sqrt{\frac{1}{n} + \frac{\bar{x}^{2}}{S_{xx}}}} \sim t_{n-2}$$
  then a $100p\%$ confidence interval for $\alpha$ is given by
  $$\left[~ \hat{\alpha} \pm t^{*}s_{e}\sqrt{\frac{1}{n} + \frac{\bar{x}^{2}}{S_{xx}}} ~\right]$$
  where
  $$P(|T| \leq t^{*}) = p ~~~ T \sim t_{n-2}$$
  \newline
  \textbf{Test of Hypothesis} \\
  To test the hypothesis or $H_{0}: \alpha = \alpha_{0}$, we use the test statistic
  $$D = \frac{|\widetilde{\alpha} - \alpha_{0}|}{S_{e}\sqrt{\frac{1}{n} + \frac{\bar{x}^{2}}{S_{xx}}}}$$
  with observed value of
  $$d = \frac{|\hat{\alpha} - \alpha_{0}|}{s_{e}\sqrt{\frac{1}{n} + \frac{\bar{x}^{2}}{S_{xx}}}}$$
  and p-value given by
  $$\text{p-value} = P\left(~|T| \geq \frac{|\hat{\alpha} - \alpha_{0}|}{s_{e}\sqrt{\frac{1}{n} + \frac{\bar{x}^{2}}{S_{xx}}}}~\right)$$
  where $T \sim t_{n-2}$

  \subsubsection{$\sigma^{2}$ and $S_{e}^{2}$}
  \begin{align*}
  \sigma^{2} &= \frac{1}{n} \sum_{i=1}^{n} (Y_{i} - \widetilde{\alpha} - \widetilde{\beta}x_{i})^{2} \\
  &= \frac{1}{n}\left[~S_{yy} - \widetilde{\beta}S_{xy}~\right]
  \end{align*}
  The \emph{Standard Error} is defined as
  $$
  S_{e}^{2}
  = \frac{1}{n - 2} \sum_{i=1}^{n} (Y_{i} - \widetilde{\alpha} - \widetilde{\beta}x_{i})^{2}
  = \frac{1}{n - 2}\left[~S_{yy} - \widetilde{\beta}S_{xy}~\right]
  $$
  \textbf{Confidence Interval for} $\sigma$ \\
  Notice that
  $$\frac{(n-2)S_{e}^{2}}{\sigma^{2}} \sim \chi_{n-2}^{2}$$
  And the $100p\%$ confidence interval for $\sigma^{2}$ is
  $$\left[~\frac{(n-2)s_{e}^{2}}{b}, \frac{(n-2)s_{e}^{2}}{a}~\right]$$
  where
  $$P(a \leq X \leq b) = p ~~~\text{and } X \sim \chi_{n-2}^{2}$$

  \subsection{Least Squares Estimation}
  Given data $(x_{i}, y_{i}), i = 1, 2, \dots, n$ \\
  The goal is to obtain a line of ``best fit'', find a line which minimizes the sum of the squares of the distances between the observed points and the fitted line $y = \alpha + \beta x$ \\
  In other words, find $\alpha$ and $\beta$ to minimize the function
  $$g(\alpha, \beta) = \sum_{i=1}^{n}[y_{i} - (\alpha + \beta x_{i})]^{2}$$
  those are the \emph{least squares estimates}. \\
  By maximizing the maximum likelihood estimates, we're minimizing least squared.

  \subsection{Confidence Intervals for the Mean Response}
  The Mean Response is defined as $\mu(x) = \alpha + \beta x$ \\
  The maximum likelihood estimator of $\mu(x)$ is
  \begin{align*}
  \widetilde{\mu}(x) &= \widetilde{\alpha} + \widetilde{\beta}x \\
  &= \bar{Y} + \widetilde{\beta}(x - \bar{x}) \\
  &= \sum_{i=1}^{n}a_{i}Y_{i} ~~~\text{where } a_{i} = \frac{1}{n} + (x - \bar{x})\frac{(x_{i} - \bar{x})}{S_{xx}}
  \end{align*}
  Notice that $a_{i}$ has the following properties
  $$\sum_{i=1}^{n}a_{i} = 1, ~\sum_{i=1}^{n}a_{i}x_{i} = x ~\text{and } \sum_{i=1}^{n}a_{i}^{2} = \frac{1}{n} + \frac{(x - \bar{x})^{2}}{S_{xx}}$$

  Thus, $\mu(x)$ has the distribution
  $$\widetilde{\mu}(x) \sim N\left(\mu(x), \sigma^{2}\left(\frac{1}{n} + \frac{(x - \bar{x})^{2}}{S_{xx}}\right)\right)$$
  So
  $$\frac{\widetilde{\mu}(x) - \mu(x)}{\sigma\sqrt{\frac{1}{n} + \frac{(x - \bar{x})^{2}}{S_{xx}}}} \sim N(0, 1)$$
  And
  $$\left[~\hat{\mu}(x) \pm z^{*}\sigma\sqrt{\frac{1}{n} + \frac{(x - \bar{x})^{2}}{S_{xx}}}~\right], ~P(|Z| \leq z^{*}) = p$$
  \newline
  Otherwise
  $$\frac{\widetilde{\mu}(x) - \mu(x)}{S_{e}\sqrt{\frac{1}{n} + \frac{(x - \bar{x})^{2}}{S_{xx}}}} \sim t_{n-2}$$
  Thus,
  $$\left[~\hat{\mu}(x) \pm t^{*}s_{e}\sqrt{\frac{1}{n} + \frac{(x - \bar{x})^{2}}{S_{xx}}}~\right], ~P(|T| \leq t^{*}) = p$$
  \newline
  Note that, $\hat{x}(x) = \hat{\alpha} + \hat{\beta}x$ \\
  \newline
  \textbf{Prediction Interval for Future Response} \\
  $$Y - \widetilde{\mu}(x) \sim N\left(0, \sigma^{2}\left(1 + \frac{1}{n} + \frac{(x - \bar{x})^{2}}{S_{xx}}\right)\right)$$
  Thus,
  $$\frac{Y - \widetilde{\mu}(x)}{S_{e}\sqrt{1 + \frac{1}{n} + \frac{(x - \bar{x})^{2}}{S_{xx}}}} \sim N(0, 1)$$
  Or
  $$\frac{Y - \widetilde{\mu}(x)}{S_{e}\sqrt{1 + \frac{1}{n} + \frac{(x - \bar{x})^{2}}{S_{xx}}}} \sim t_{n-2}$$
  \newline
  Prediction Interval is then
  $$\left[~\hat{\mu}(x) \pm t^{*}s_{e}\sqrt{1 + \frac{1}{n} + \frac{(x - \bar{x})^{2}}{S_{xx}}} ~\right]$$

  \subsection{Terminologies}
  Total Sum of Squares, denoted TSS, is
  $$\sum (y_{i} - \bar{y})^{2} = S_{yy}$$

  Regression Sum of Squares, denoted RSS, is
  $$\hat{\beta}S_{xy}$$
  It is part of the variability of $Y$ that can be explained by change in $X$.

  Error Sum of Squares, denoted ESS, is
  $$[S_{yy} - \hat{\beta}S_{xy}] = \sum [y_{i} - (\hat{\alpha} + \hat{\beta}x_{i})]^{2}$$
  It is part of the variability explained by $X$.

  Thus, the Total Sum of Squares is the sum of Regression Sum of Squares and Error Sum of Squares
  $$TSS = RSS + ESS$$
  $$\sum (y_{i} - \bar{y})^{2} = \hat{\beta}S_{xy} + [S_{yy} - \hat{\beta}S_{xy}]$$
  Note that $\frac{RSS}{TSS}$ should be high if the model is a good fit. \\
  As ESS $\uparrow$, the model is not a good fit.

\end{document}
