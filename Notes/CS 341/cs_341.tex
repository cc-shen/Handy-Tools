\documentclass[12pt]{article}
\usepackage[margin = 1.5in]{geometry}
\setlength{\parindent}{0in}
\usepackage{amsfonts, amssymb, amsthm, mathtools, tikz, qtree, float}
\usepackage{algpseudocode, algorithm, algorithmicx}
\usepackage{ae, aecompl, color}
\usepackage{wrapfig}
\usepackage{multicol, multicol, array}
\usepackage{imakeidx}
\makeindex[columns=2, title=Indices, intoc]

\usepackage[pdftex, pdfauthor={Charles Shen}, pdftitle={CS 341: Algorithms}, pdfsubject={Lecture notes from CS 341: at the University of Waterloo}, pdfkeywords={course notes, notes, Waterloo, University of Waterloo}, pdfproducer={LaTeX}, pdfcreator={pdflatex}]{hyperref}
\usepackage{cleveref}

\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace}
\definecolor{darkish-blue}{RGB}{25,103,185}

\hypersetup{
  colorlinks,
  citecolor=darkish-blue,
  filecolor=darkish-blue,
  linkcolor=darkish-blue,
  urlcolor=darkish-blue
}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[subsection]

\theoremstyle{definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{ex}[theorem]{Example}
\newtheorem*{ex*}{Example}
\newtheorem{defn}{Definition}

\crefname{ex}{Example}{Example}

\setlength{\marginparwidth}{1.5in}
\newcommand{\lecture}[1]{\marginpar{{\footnotesize $\leftarrow$ \underline{#1}}}}

\newcommand{\defnterm}[1]{\textbf{\textcolor{teal}{#1}}\index{#1}}

\newcommand{\floor}[1]{\ensuremath{\left\lfloor #1 \right\rfloor}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil #1 \right\rceil}}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\allowdisplaybreaks

\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

%%%%%%%%%%%%%%%%%%%%%
%% D O C U M E N T %%
%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\let\ref\Cref
\pagenumbering{roman}

\title{\bf{CS 341: Algorithms}}
\date{Fall 2016, University of Waterloo \\ \center Notes written from Jeffrey Shallit's lectures.}
\author{Charles Shen}

\blfootnote{Feel free to email feedback to me at
\href{mailto:ccshen902@gmail.com}{ccshen902@gmail.com}.}

\maketitle
\thispagestyle{empty}
\newpage
\tableofcontents
\newpage
\pagenumbering{arabic}

\newpage
\section{Selection in deterministic linear time}
\lecture{October 25, 2016}
In the selection problem, the goal is to determine the $i$-th smallest element from an unsorted list of $n$ elements in linear time. \\
``Deterministic'' here means that no random numbers are used. \\
For simplicity, suppose that all elements are distinct, but this is not crucial as a small modification will make it work any list. \\

One method to choose a pivot is via randomness. \\
We want a deterministic method to clever choose a pivot to be used to partition a list. \\
Thus we arrive at the ``medians-of-five'' algorithm by Blum, Floyd, Pratt, Rivest, and Tarjan (1973). \\

Idea: \\
Split the list into $\floor{\frac{n}{5}}$ groups of 5 elements, and one additional group of at most 4 elements. \\
Then sort each group of 5 elements, and find the median of each group of 5.
This can be done in constant time, so sorting each group of 5 costs $O(n)$. \\
Note that that median is the 3rd (center) element in the group of 5.
This gives a list of $\floor{\frac{n}{5}}$ medians. \\
Now recursively determine the median of these $\floor{\frac{n}{5}}$ medians, which is the element we'll partition around. \\

We arrive at the algorithm:
\begin{algorithmic}
\Function{select}{$A, i$}
  \State $n \gets |A|$
  \If{$n < 60$}
    \State \Call{sort}{$A$}
    \State \Return $i$-th smallest element
  \Else
    \State $m \gets \floor{\frac{n}{5}}$
    \State divide A up to $m$ groups of 5 elements, with at most one remaining group of $\leq 4$ elements
    \State sort each of the $m$ groups in ascending order
    \State $M \gets$ array of medians of each group
    \State $x \gets \Call{select}{M}{\ceil{m/2}}$
    \Comment{median of all the medians}
    \State $k \gets \Call{x-partition}{A, x}$
    \Comment{partition array A into elements $\leq$ x and elements $>$ x; returns number of elements on ``low side'' of the partition}
    \If{$i = k$}
      \State \Return $x$
    \ElsIf{$i < k$}
      \State \Return \Call{select}{$A[1..k-1], i$}
    \Else
      \State \Return \Call{select}{$A[k+1..n], i - k$}
    \EndIf
  \EndIf
\EndFunction
\end{algorithmic}

Let's obtain a lower bound on the number of elements $> x$, the median of medians. \\
Here's a picture for $n = 37$:
\begin{verbatim}
  smallest   S    S    S    S    *    *    *    *

             S    S    S    S    *    *    *    *

             S    S    S    m    L    L    L

             *    *    *    L    L    L    L

  largest    *    *    *    L    L    L    L

  m = median of medians
\end{verbatim}

There are $\floor{\frac{n}{5}}$ total columns in which 5 elements appear. \\
Of these, at least $\frac{1}{2}$ (more precisely, $\ceil{\floor{n/5}/2}$) contain an L.
All of these columns, except the one where $x$ appears, contributes 3 to the count of L's; the one where $x$ appears contributes 2. \\

The conclusion is that $\geq 3\ceil{\floor{n/5}/2} - 1$ elements are L's, that is, greater than $x$.
Hence, at most $\leq n - (3\ceil{\floor{n/5}/2} - 1)$ elements are $\leq x$. \\

Then, $n - (3\ceil{\floor{n/5}/2} - 1) \leq 7n/10 + 3$ for all $n \geq 1$.
To prove this, see that
$$\floor{n/5} \geq n/5 - 1$$
so
$$ceil{floor{n/5}/2} \geq n/10 - 1/2$$
then
$$3\ceil{\floor{n/5}/2} - 1 \geq 3n/10 - 5/2$$
hence
$$n - (3\ceil{\floor{n/5}/2} - 1) \leq 7n/10 + 3$$

We can also claim that at most $(7n/10 + 3)$ elements are $\geq x$ using the same proof above. \\
So whether we recurse in the smaller elements or larger elements, we can use this bound. \\

It follows that the time $T(n)$ to select from a list of n elements satisfies the following inequality:
$$T(n) \leq T(\floor{n/5}) + T(r) + dn$$
where $r \leq 7n/10 + 3$, and the $dn$ term soaks up the time needed to do the sorting of 5-element groups, and the partitioning. \\

Suppose that this recurrence obeys $T(n) \leq cn$ for some $c$,
\begin{align*}
T(n) &\leq T(\floor{n/5}) + T(r) + dn \\
&\leq cn/5 + cr + dn \\
&\leq cn/5 + c(7n/10 + 3) + dn \\
&\leq 9cn/10 + 3c + dn
\end{align*}
and we want this to be less than $cn$. \\

Now, $9cn/10 + 3c + dn \leq cn \iff (c/10 - d)n \geq 3c$ \\
Now $n$ had better be bigger than 30 here, or else we are in a heap o' trouble.
So let's assume $n \geq 60$. Then $(c/10 - d) n \geq (c/10 - d) 60$, and if this is to be $\geq 3c$, then we must have $6c - 60d \geq 3c$, and so $c \geq 20d$. \\
If $n \geq 60$, and $c \geq 20d$, then the induction step indeed works. \\

To complete the proof, it remains to see that $T(n) \leq cn$ for $n = 1, 2, 3, \dots, 59$. \\
To do this, simply choose
$$c = max(20d, T(1), T(2)/2, ..., T(59)/59)$$

That looks like cheating, but it works. \\
Now the basis (namely $1 \leq n \leq 59$) works out just fine, and so does the induction step.

The presence of these large constants (such as $c = 20d$) suggests that this algorithm is of more theoretical than practical interest. \\
However, with a bit more work, the number of comparisons can be reduced quite a lot, down to $(3+o(1))n = 3n + o(n)$. \label{sec:linear_select}

\newpage
\section{Lower bounds}
Can we prove that an algorithm is best possible?
Sometimes. \\

We argued that any comparison-based algorithm for sorting must use $\Omega(n \lg n)$ time.
And in reference to the convex hull algorithm, we argued that we couldn't do better than $\Omega(n \lg n)$, because we could use convex hull as a subroutine to create a sorting algorithm;
if we could solve convex hull in $o(n \lg n)$ time, then we would end up being able to sort in $o(n \lg n)$ time, a contradiction. \\

By ``lower bounds'', we mean a lower bound on the complexity of a \emph{problem}, not an algorithm.
Basically we need to prove that \emph{no algorithm}, no matter how complicated or clever, can do better than our bound. \\
We don't need each algorithm to be ``slow'' as $f(n)$ on the \emph{same} input, but we do need each algorithm to be ``slow'' as $f(n)$ on \emph{some} input. \\
Also, a lower bound doesn't rule out the possibility that some algorithm might be very fast on \emph{some inputs} (but not all).

What do we mean by complexity of a problem? \\
We say that a function $f$ is a lower bound on the complexity of a problem $P$ if, for \emph{every algorithm} $A$ to solve $P$, and every positive integer $n$, there exists some input $I$ of size $n$ such that $A$ uses at least $f(n)$ steps on input $I$. \\

The easiest kind of lower bound is the ``information-theoretic bound''. \\
Idea: every algorithm that uses \emph{only} yes/no questions to decide among $k$ possible alternatives, \emph{must} ask at least $(\lg k)$ questions in the worst case.
\begin{proof}
Use an ``adversary argument''. \\
The adversary doesn't decide on one of the alternatives ahead of time, but rather it answers the algorithm's questions by always choosing the alternative that maximizes the size of the solution space. \\
Since the questions are yes/no, the solution space goes down in size by a factor of at most two with each question. \\
The algorithm cannot answer correctly with certainty unless the solution space is of size 1. \\
Thus at least $(\lg k)$ questions are needed.
\end{proof}

\begin{theorem}
Any comparison-based sorting algorithm must use at least $\lg(n!) = \Omega(n log n)$ questions in the worst-case.
\end{theorem}
\begin{proof}
Use the information-theoretic bound, and observe that the space of possible solutions includes the $n!$ different ways the input could be ordered.
\end{proof}

\newpage
\section{Adversary Strategy}
\subsection{Adversary Arguments in General}
More generally, we can think of a lower bound proof as a game between the algorithm and an "adversary". \\
The algorithm is "asking questions" (for example, comparing two elements of an array), while the adversary is answering them (providing the results of the comparison). \\

The adversary should be thought of as a very powerful, clever being that is answering in such a way as to make your algorithm run as slowly as possible. \\
The adversary cannot "read the algorithm's mind", but it can be prepared for anything the algorithm might do. \\
Finally, the adversary is not allowed to "cheat";
that is, the adversary cannot answer questions inconsistently.
The adversary does not not need to have a particular input in mind at all times when it answers the questions, but when the algorithm is completed, there must be at least one input that matches the answers the adversary gave (otherwise it would have cheated). \\

The algorithm is trying to run as quickly as possible. \\
The adversary is trying (through its cleverness) to \emph{force} the algorithm to run slowly. \\
This interaction proves that the lower-bound obtained by this type of argument applies to \emph{any possible} algorithm from the class under consideration. \\

\begin{theorem}
Every comparison-based algorithm for determining the minimum of a set of $n$ elements must use at least $(\frac{n}{2})$ comparisons.
\end{theorem}
\begin{proof}
Every element must participate in at least one comparison;
if not, the not compared element can be chosen (by an adversary) to be the minimum. \\
Each comparison compares 2 elements. \\
Hence, at least $(\frac{n}{2})$ comparisons must be made.
\end{proof}

\begin{theorem}
Every comparison-based algorithm for determining the minimum of a set of $n$ elements must use at least $(n - 1)$ comparisons.
\end{theorem}
\begin{proof}
To say that a given element, $x$, is the minimum, implies that (1) every other element has won at least one comparison with another element (not necessarily $x$). (By "$x$ wins a comparison with $y$" we mean $x > y$.) \\
Each comparison produces at most one winner. \\
Hence at least $(n - 1)$ comparisons must be used. \\

To convert this to an adversary strategy, do the following: for each element, record whether it has won a comparison ($W$) or not ($N$). \\
Initially all elements are labelled with $N$. \\
When we compare an $N$ to an $N$, select one arbitrarily to be the $W$. \\
If no values assigned, choose them to make this so. \\
If values are assigned, decrease the loser (if necessary) to make this so. \\
When we compare $W$ to $N$, always make the $N$ the loser and set or adjust values to make sure this is the case. (You might need to decrease the loser's value to ensure this.) \\
When we compare $W$ to $W$, use the values already assigned to decide who is the winner. \\

Now each comparison increases the total number of items labelled $W$ by at most 1. \\
In order to "know" the minimum, the algorithm must label $(n - 1)$ items $W$, so this proves at least $(n - 1)$ comparisons are required. \\
The values the adversary assigned are the inputs that forced that algorithm to do the $(n - 1)$ comparisons.
\end{proof}

\subsection{Finding Both the Maximum and Minimum of a List of n Numbers}
It is possible to compute both the maximum and minimum of a list of n numbers, using $(\frac{3}{2}n - 2)$ comparisons if n is even, and $(\frac{3}{2}n - \frac{3}{2})$ comparisons if n is odd. \\
It can be proven that \emph{no comparison-based method} can correctly determine both the max and min using fewer comparisons in the worst case. \\

We do this by constructing an adversary argument. \\

In order for the algorithm to correctly decide that $x$ is the minimum and $y$ is the maximum, it must know that
\begin{enumerate}
  \item every element other than $x$ has won at least one comparison (i.e. $x > y$ if $x$ wins a comparison with $y$),
  \item every element other than $y$ has lost at least one comparison
\end{enumerate}
Calling a win $W$ and a loss $L$, the algorithm must assign $(n - 1)$ $W$'s and $(n - 1)$ $L$'s.
That is, the algorithm must determine $(2n - 2)$ "units of information" to always give the correct answer. \\

We now construct an adversary strategy that will force the algorithm to learn its $(2n - 2)$ "units of information" as slowly as possible. \\
The adversary labels each element of the input as $N$, $W$, $L$, or $WL$.
These labels may change over time. \\
$N$ signifies that the element has never been compared to any other by the algorithm. \\
$W$ signifies the element has won at least one comparison. \\
$L$ signifies the element has lost at least one comparison. \\
$WL$ signifies the element has won at least one and lost at least one comparison. \\

\lecture{October 27, 2016}

So the the adversary uses the following table. \\
\begin{table}[H]
  \centering
  \begin{tabular}{C{3cm}  C{3cm}  C{3cm}  C{3cm}}
  \hline
  labels when comparing elements $(x, y)$ & the adversary's response & the new label & unit of information given to the alg. \\ \hline \hline
  $(N, N)$ & $x > y$ & $(W, L)$ & 2 \\ \hline
  $(W, N)$ or $(WL, N)$ & $x > y$ & $(W, L)$ or $(WL, L)$ & 1 \\ \hline
  $(L, N)$ & $x < y$ & $(L, W)$ & 1 \\ \hline
  $(W, W)$ & $x > y$ & $(W, WL)$ & 1 \\ \hline
  $(L, L)$ & $x > y$ & $(WL, L)$ & 1 \\ \hline
  $(W, L)$ or $(WL, L)$ or $(W, WL)$ & $x > y$ & no change & 0 \\ \hline
  $(WL, WL)$ & consistent with assigned values & no change & 0
  \end{tabular}
\end{table}
The adversary also tentatively assigns values to the elements, which may change over time. \\
However, they can only change in a fashion \emph{consistent} with previous answers.
That is, an element labelled $L$ may only \emph{decrease} in value (since it lost all previous comparisons, if it is decreased, it will still lose all of them), and an element labelled $W$ may only increase in value. \\
An element labelled $WL$ cannot change. \\

\begin{ex*}
Consider the following sequence of possible questions asked by the algorithm and the adversary's responses:
\begin{verbatim}
  -----------------------------------------------------------
  Algorithm  |   Adversary's responses and worksheet
  compares   |   x1      x2      x3      x4      x5      x6
  -----------------------------------------------------------
   (x1, x2)  |  W-20    L-10      N       N       N       N
   (x4, x5)  |  W-20    L-10      N     W-30     L-15     N
   (x1, x4)  |  W-40    L-10      N    WL-30     L-15     N     (*)
   (x3, x6)  |  W-40    L-10    W-11   WL-30     L-15    L-2
   (x2, x5)  |  W-40   WL-10    W-11   WL-30     L-7     L-2
   (x3, x1)  | WL-40   WL-10    W-50   WL-30     L-7     L-2
   (x2, x4)  | WL-40   WL-10    W-50   WL-30     L-7     L-2    (**)
   (x5, x6)  | WL-40   WL-10    W-50   WL-30    WL-7     L-2
  -----------------------------------------------------------
\end{verbatim}
At this point the algorithm knows that $x_3$ is the maximum, since it is the only element that has never lost a comparison, and $x_6$ is the minimum, since it is the only element that has never won a comparison. \\
Note that in step (*), the adversary was forced to reassign the value he had previously assigned to $x_1$, since $x_1$ had to win the comparison against $x_4 = 30$. \\
This is permitted, since $x_1$ had won every previous comparison and so increasing its value ensures consistency with previous answers. \\
Also note that step (**) is superfluous, as the algorithm didn't learn any new information.
\end{ex*}

\begin{theorem}
Every \emph{comparison-based} method for determining both the maximum and minimum of a set of $n$ numbers must use at least $(\frac{3}{2}n - 2)$ comparisons (if $n$ even), or $(\frac{3}{2}n - \frac{3}{2})$ comparisons (if $n$ odd), in the worst case.
\end{theorem}
\begin{proof}
Suppose that $n$ is even. \\
As shown before, the algorithm must learn $(2n - 2)$ units of information. \\
The most it can learn in one comparison is 2 units;
when both elements have never participated before in a comparison, labelled by $(N, N)$.
This can happen at most $(\frac{n}{2})$ times. \\
To learn the remaining $(n - 2)$ units of info., the algorithm must ask $(n - 2)$ questions. \\
The total number of questions is therefore at least $\frac{n}{2} + n - 2 = \frac{3}{2}n - 2$. \\
The same kind of argument works for $n$ odd.
\end{proof}

So we have a lower bound for the problem of finding both the maximum and minimum of a set of $n$ numbers. \\

So the algorithm is as followed: \\
If $n$ is even, compare $x[1]$ with $x[2]$ (determining both the maximum and minimum with 1 comparison), $x[3]$ with $x[4]$, etc.
We get $(\frac{n}{2})$ maxima and $(\frac{n}{2})$ minima. \\
To find the maximum, use the usual method on the $(\frac{n}{2})$ maxima, which uses $(\frac{n}{2} - 1)$ comparisons, and the same for the minima. \\
The total cost is $\frac{n}{2} + \frac{n}{2} - 1 + \frac{n}{2} - 1 = \frac{3}{2}n - 2$.
A similar method works when $n$ is odd. \\

Notice that each comparison adds at most one $W$, and we need to get $(n - 1)$ $W$'s before we know the remaining element must be the minimum. \\
So we need at least $(n - 1)$ comparisons. \\
If the algorithm declares a minimum before getting $(n - 1)$ $W$'s, then there are at least two elements without $W$, so if the algorithm declares one of the minimum, the adversary can truthfully produce the other as the minimum.

\subsection{Remarks on Constructing an Adversary Strategy}
Lower bound arguments are some of the most difficult and deepest areas in theoretical computer science. \\

\textbf{Tips for producing an adversary strategy}: \\

Recall that the lower bound argument can be viewed as a game between you (playing the role of an algorithm) and an adversary. \\
You want the algorithm to run quickly (for example, by making a small number of queries to the input). \\
The adversary wants the algorithm to run slowly. \\
The adversary "wins" (and the lower bound of $f(n)$ is proved) if he/she can force the algorithm to execute $f(n)$ steps. \\

A proof behaves something like: for all sequences of queries into the input made by the algorithm, there exists a sequence of replies by the adversary, such that the algorithm is forced to execute f(n) steps.
So your adversary argument must apply to \emph{every} possible "move" the algorithm could make. \\

In constructing its replies, the adversary can do as much or as little bookkeeping as necessary. \\
There is no requirement that the adversary's computation be efficient or run in any particular time bound.

\subsection{Finding the Second Largest}
Problem: given a list of $n$ elements, find the 2nd largest. \\

The naive method (finding the largest, remove it, and then find the largest again) takes $2n - 3$ comparisons. \\
Instead, we can do a ``tournament'' method. \\
Have the elements in pairs, the larger one (winner) advances;
then the 2nd largest (runner-up) is among those defeated by the winner!
This is because any other element must have played two other elements superior to it. \\

This algorithm uses $(n - 1)$ comparisons to determine the largest, plus $\ceil{\lg n} - 1$ comparisons to determine the runner-up (2nd largest). \\
The total is $(n + \ceil{\lg n} - 2)$ comparisons. \\

We can prove that the algorithm just given is \emph{optimal}, in the sense that no comparison-based algorithm uses fewer than $(n + \ceil{\lg n} - 2)$ comparisons in the worst case. \\

\emph{Note}: see Lecture 14 for full proof! (\href{https://www.student.cs.uwaterloo.ca/~cs341/lecture14.html}{Click here.})

\subsection{A Lower Bound for Finding the Median}
The best algorithm known (in terms of number of comparisons used) uses $3n + o(n)$ comparisons in the worst case (see \ref{sec:linear_select}). \\
The best \emph{lower bound} known for the median-finding problem is $2n + o(n)$ comparisons. \\

We will prove a weaker lower bound: $\frac{3}{2}n + O(1)$, where $n$ odd for convenience. \\

First, note that any comparison-based algorithm for finding the median must use at least $(n - 1)$ comparisons.
That's because the algorithm \emph{must} establish the relationship of each element to the median---either by direct comparison, or by comparison with another element whose relationship to the median is known. \\
The algorithm \emph{must} eventually gain enough knowledge equivalent to the following tree:
\begin{verbatim}
      *   *
       \ /                  elements > median
        *  *
        | /
  ------x-------------------------median-----
      / | \
     *  *  *                elements < median
        |
        *
\end{verbatim}

Note that if there were an element that didn't participate in a comparison, we couldn't know the median. \\
For an adversary could change the value of that element from, say, less than the median to greater than the median, changing the median to some other element. \\
Similarly, if there is some element whose relationship to the median is not known (because, for example, it is greater than some element that is less than the median), we could simply move it from one side of the line above to the other, without changing comparisons.
This would also change the median. \\

Now there are n elements in the tree above, so there are $(n - 1)$ edges, and hence $(n - 1)$ comparisons. \\
We now improve this to $\frac{3}{2}n + O(1)$. \\
First, we call a comparison involving x crucial, say x against y, if it is the first comparison where
\begin{enumerate}
  \item $x < y$ and $y \leq$ median OR if
  \item $x > y$ and $y \geq$ median
\end{enumerate}
So the algorithm must perform at least $(n - 1)$ crucial comparisons in order to determine the median. \\

\lecture{November 1, 2016}

We now give an adversary strategy to \emph{force} the algorithm to perform $(n - 1)/2$ \emph{non-crucial} comparisons (since we've proved earlier that the algorithm requires $(n - 1)$ \emph{crucial} comparisons. \\

Idea: \\
First, the adversary picks some value (\emph{not} some element) to be the median. \\
Then it assigns a label in $\{N, L, S\}$ to each element, and also values, as appropriate. \\
Initially each element is assigned an $N$ (``Never participated in any comparison'').
$L$ means Larger than the median and $S$ means Smaller than the median. \\
The adversary follows the following strategy:
\begin{table}[H]
  \label{tab:ad_median_strat}
  \centering
  \begin{tabular}{L{4cm}|L{8cm}}
  \hline
  Algorithm compares & Adversary responds \\ \hline \hline
  $(N, N)$ & assign one element to be larger than the median, one smaller; result is $(L, S)$ \\ \hline
  $(S, N)$ or $(N, S)$ & assign the $N$-element to be larger than the median; result is $(S, L)$ or $(L, S)$ \\ \hline
  $(L, N)$ or $(N, L)$ & assign the $N$-element to be smaller than the median; result is $(L, S)$ or $(S, L)$ \\ \hline
  $(S, L)$ or $(L, S)$ or $(S, S)$ or $(L, L)$ & consistent with previously assigned values \\ \hline
  \end{tabular}
\end{table}
\emph{Note 1}: this strategy continues until $((n - 1)/2)$ $S$'s (or $((n - 1)/2)$ $L$'s) have been assigned.
If at some point $((n - 1)/2)$ $S$'s are assigned, then the adversary assigns the remaining elements to be greater than the median, except for one, which \emph{is} the median.
A similar thing is done if $((n - 1)/2)$ L's have been assigned. \\

\emph{Note 2}: the last element assigned is \emph{always} the median. \\

I claim that this strategy will always force the algorithm to perform $((n - 1)/2)$ non-crucial comparisons. \\
For any time an $N$-element is compared, a non-crucial comparison is done (except at the very end, when a crucial comparison may be done with the median itself). \\
The least number of comparisons with $N$-elements that can be done is $((n - 1)/2)$. \\
The \emph{total} number of comparisons is therefore $n - 1 + (n - 1)/2 = (3/2)(n - 1)$.

\begin{ex*}
The adversary strategy when $n = 7$.
The adversary arbitrarily chooses the median to be 43.
Then
\begin{verbatim}
  ---------------------------------------------------
  Algorithm   Adversary's strategy (label:value)
  compares    x1    x2    x3    x4    x5    x6    x7
  ----------------------------------------------------
              N     N     N     N     N     N     N
  (x1, x3)   L:50   N    S:31   N     N     N     N
  (x3, x4)   L:50   N    S:31  L:47   N     N     N
  (x6, x7)   L:50   N    S:31  L:47   N    L:60  S:20

  -------  at this point there are (7-1)/2 L's -------
  -------  assigned, so adversary assigns the --------
  -------  remaining elements to L and the median ----

             L:50  43    S:31  L:47  S:15  L:60  S:20
  (x2, x5)   a crucial comparison
  (x2, x1)   a crucial comparison
  (x2, x3)   a crucial comparison
  (x2, x4)   a crucial comparison
  (x2, x6)   a crucial comparison
  (x2, x7)   a crucial comparison
\end{verbatim}
At this point the algorithm concludes that \texttt{x2} is the median. \\
The first three comparisons were non-crucial; only the last six are crucial.
\end{ex*}

The best lower bound known for the median: Dor and Zwick, Median selection requires $(2 + \epsilon)n$ comparisons.

\subsection{Lower Bound on Average-Case Sorting}
\begin{theorem}
Any comparison-based algorithm must use $\Omega(n \lg n)$ comparisons on \emph{average}.
\end{theorem}

\begin{lemma}
In any binary tree $T$ where each node has 0 or 2 children,
$$\sum_{\ell \text{ a leaf of } T} 2^{-\text{depth}(T, \ell)} = 1$$
where depth($T, \ell$) is the length of the path from $\ell$ to the root.
\end{lemma}
\begin{proof}
Prove by induction on the height of $T$. \\
Easy to see the lemma holds when $T$ consists of a single node. \\
Assume it holds for all tree of height $< k$;
prove it for trees of height $k$. \\
Then a tree of height $k$ consists of a root and two subtrees, $T'$ and $T''$. \\
Then the lemma applies to each of these subtrees recursively, so we know that
$$\sum_{\ell \text{ a leaf of } T'} 2^{-\text{depth}(T', \ell)} = 1$$
and
$$\sum_{\ell \text{ a leaf of } T''} 2^{-\text{depth}(T'', \ell)} = 1$$
Then we have
$$\sum_{\ell \text{ a leaf of } T'} 2^{-\text{depth}(T, \ell)} = \frac{1}{2}$$
since the depth of a leaf in $T$ goes up by 1 when considered as a leaf of $T$. \\
The same result holds for $T''$. \\
Adding these together yields the result.
\end{proof}

\begin{lemma}
Suppose $a, b \in \mathbb{R}$ and $a, b > 0$. \\
If $a \neq b$, then we have
$$\lg a - \lg b > 2\lg((a + b)/2)$$
\end{lemma}
\begin{proof}
If $a \neq b$, then
\begin{align*}
(\sqrt{a} - \sqrt{b})^{2} &> 0 \\
a - 2\sqrt{ab} + b &> 0 \\
(a + b)/2 &> \sqrt{ab} \\
\lg((a + b)/2) &> \frac{1}{2}\lg(ab) \\
2\lg((a + b)/2) &> \lg{a} + \lg{b} \\
-2\lg((a + b)/2) &< -\lg{a} - \lg{b}
\end{align*}
as required.
\end{proof}

\begin{lemma}
Suppose $x_{1}, x_{2}, \dots, x_{k} \in \mathbb{R}$ and $x_{1}, x_{2}, \dots, x_{k} > 0$ such that $\sum_{1 \leq i \leq k}x_{i} = 1$.
Then
$$\sum_{1 \leq i \leq k}(-\lg x_{i}) \geq k\lg k $$
\end{lemma}
\begin{proof}
If all the $x_{i} = \frac{1}{k}$, then the inequality clearly holds - actually then it is an equality! \\
Suppose we claim this sum actually achieves its minimum at this point. \\
For if the minimum occurred at some \emph{other} point, then two of the $x_{i}$ must be different, say $a$ and $b$. \\
Then by Lemma 3.6.3, we could replace $(-\lg{a}) + (-\lg{b})$ with two copies of $-\lg((a + b)/2)$ and get a smaller sum, a contradiction.
\end{proof}

\begin{proof}
Consider the comparison tree used by the algorithm. \\
There are at least $k = n!$ leaves, since there are are $n!$ possible ways to order $n$ numbers. \\
There are also at most this number of leaves, since otherwise two leaves would be labelled with the same ordering, and then, by tracing up to the lowest common ancestor, we would get to a node where there are two different answers, implying the orderings cannot be the same. \\
So there are exactly $n!$ leaves. \\
By Lemma 3.6.2,
$$\sum_{\ell \text{ a leaf of } T}2^{-\text{depth}(\ell)} = 1$$
Then by Lemma 3.6.4, with $x_{i} = 2^{-\text{depth}(\ell)}$, we get
$$\sum_{\ell \text{ a leaf of } T}\text{depth}(\ell) \geq n! \lg(n!)$$
It follows that
$$\frac{1}{n!}\sum_{\ell \text{ a leaf of } T}\text{depth}(\ell) \geq \lg(n!)$$
but the expression on the left is just the average depth of each leaf, i.e., the average number of comparisons used (averaged over all possible arrangements of the input). \\
Hence the average number of comparisons used is $\Omega(n \lg{n})$.
\end{proof}

\newpage
\section{Graphs and Graph Algorithms}
A graph consists of a bunch of points, called vertices (or nodes), and a bunch of connections between pairs of points, called edges (or arcs). \\
So $G = (V, E)$, where $V$ is a set of vertices and $E$ is a subset of $V \times V$. \\
Where $|V|$ is the number of vertices and $|E|$ is the number of edges in $G$. \\

Graphs can be directed or undirected;
in a directed graph, edges have a direction, and you can only traverse the graph by going in that direction.
In an undirected graph, edges can be traversed in any direction. \\

Sometimes we allow graphs to have edges from a vertex to itself;
such an edge is called a "loop" or a "self-loop" and the graph is no longer \emph{simple}. \\
Most graphs in this course won't have such edges, we will allow self-loops for directed graphs and only sometimes for undirected graphs.
We will rarely have two or more different edges between the same two vertices. \\

\emph{Note}: in any graph, $|E| = O(|V|^{2})$, and if the graph is connected, $|E| = \Omega(|V|)$. \\

A \emph{path} is a series of vertices where each consecutive vertex is connected by an edge to the previous vertex. \\
An undirected graph is \emph{connected} if there is a path from every vertex to every other vertex. \\
The \emph{length} of a path is the number of edges in the path. \\
A path is a \emph{cycle} if the first vertex is the same as the last vertex.
Usually in a path we demand that there are no repeated vertices (with the exception in a cycle). \\
A \emph{walk} (resp., \emph{closed walk}) is a path (resp., cycle) where repeated vertices and edges are allowed. \\
A graph is \emph{acyclic} if there are no cycles. \\

A \emph{tree} is a connected acyclic graph. \\
An undirected graph is a tree if every pair of vertices are connected by a \emph{unique} simple path. \\
Trees can be rooted (have a distinguishable vertex called the root) or unrooted. \\
A \emph{forest} is a vertex-disjoint collection of 1 or more trees. \\

Two ways to store a graph:
\begin{itemize}
  \item[1.] \textbf{Adjacency Matrix} \\
  We represent a graph by a $|V| \times |V|$ matrix with a 1 in row $i$ and column $j$ is $(i, j)$ is an edge, and 0 otherwise.
  \item[2.] \textbf{Adjacency List} \\
  We represent a graph by an array Adj of $|V|$ lists, one list for each vertex. \\
  For each vertex Adj[u] contains all vertices (or pointers to the vertices) $v$ such that $uv$ is an edge.
\end{itemize}

Advantages and disadvantages of each representation: \\
\emph{Adjacency Matrix}:
\begin{itemize}
  \item easy to manipulate
  \item can check if $uv$ is an edge in $O(1)$ time
  \item but storage is always $\Omega(|V|^{2})$---not good for sparse graphs
  \item time to add an edge:
  \begin{itemize}
    \item existing vertex and new edge: $O(1)$ time
    \item existing vertex and existing edge: $O(1)$ time, can check in $O(1)$ time
    \item new vertex and new edge: $O(|V|^{2})$ time, create a new, bigger matrix
  \end{itemize}
\end{itemize}

\emph{Adjacency List}:
\begin{itemize}
  \item harder to manipulate
  \item checking if $uv$ is an edge takes $O(|V|) time$
  \item but storage is $O(|V| + |E|)$
  \item time to add an edge:
  \begin{itemize}
    \item existing vertex and new edge: $O(|V|)$ time
    \item existing vertex and existing edge: $O(|V|)$ time
    \item new vertex and new edge: $O(|V|)$ time
  \end{itemize}
\end{itemize}

Sometimes there are weights on the edges. \\
In this case we can represent a weighted graph by a $|V| \times |V|$ matrix with a number $x$ in row $i$ and column $j$ if the edge from $i$ to $j$ has weight $x$. \\
If $i$ is not directly connected to $j$, then the weight is typically taken to be infinite. \\
Or we can use the adjacency list representation, with weights given together with the vertices in a list. \\

The \emph{weight of a path} is the sum of the weights of all the edges traversed along that path. \\

\subsection{Minimum Spanning Tree}
\emph{Note}: here we are working with undirected graphs. \\
A spanning tree is an (unrooted) tree that connects all the vertices of a graph. \\
Every connected graph has at least one spanning tree;
typically, a graph can have many, many different spanning trees. \\
If there are weights on the edges, a minimum spanning tree is one that minimizes the sum of the weights of the edges used. \\
Note that edge weights need not be distances!
They can even be 0 or negative. \\
A graph can have more than one minimum spanning tree. \\

If the graph is not connected, we sometimes talk about spanning \emph{forests} instead of spanning trees. \\

Minimum spanning trees come up all the time in network problems. \\
Here the nodes represent nodes of a network that need to be connected, and the weights represent distances (or perhaps, amounts of wire required) between the vertices. \\
We want to hook up all the nodes in our network using the smallest total distance (or amount of wire). \\

\lecture{November 3, 2016}

Algorithms for \emph{minimum spanning tree}:
\begin{itemize}
  \item[\textbf{Kruskal}:] many different trees that are eventually joined up to form one tree
  \item[\textbf{Prim}:] one tree that grows by sequentially adding edges
\end{itemize}

\emph{Neither} Kruskal nor Prim work for directed graphs. \\
That problem is called the ``minimum cost arborescence'' problem;
it can be solved in $min(n^{2}, m\lg{n})$ time.

\subsubsection{Kruskal's Algorithm}
We consider each edge in turn, starting with the lowest-weight edge and continuing in ascending order by weight.
We only add the edge if it doesn't create a cycle. \\
It's a greedy algorithm where, at each step, we choose the minimum-weight edge that doesn't create a cycle. \\

To implement Kruskal's algorithm, we need to consider the edges in turn, ordered in ascending order by weight;
which we can do by sorting the edges. \\
Then when we pick an edge $uv$ to consider, we have to decide if they are in different trees, or the same tree.
This is not so easy!
But it is an example of the "disjoint-set data structure problem." \\

In the disjoint-set data structure problem, we have three operations:
\begin{itemize}
  \item[\texttt{MAKE-SET}:] create a set containing a single vertex
  \item[\texttt{FIND}:] given a vertex, find the ``name'' of the set it is in (sets are disjoint so only one name is possible for each vertex)
  \item[\texttt{UNION}:] union together two sets, possibly giving the resulting set a new name (destructive)
\end{itemize}

Due to the two main operations, this is also called the ``union-find'' problem. \\

It is possible to do $m$ \texttt{MAKE-SET}, \texttt{UNION} and \texttt{FIND} operations,$n$ of which are \texttt{MAKE-SET}, in $O(m~\alpha(n))$ time. \\
$\alpha(n)$ is an extremely slow-growing function called the ``inverse Ackerman'' function;
the function grows so slowly that $\alpha(n) \leq 4$ for all values of $n$ that one would ever encounter in real life. \\

How Kruskal's algorithm may be implemented:
\begin{algorithmic}
\Function{kruskal}{$G, w$}
  \Comment{$G = (V, E)$ and $w$ is the weight function}
  \State $A \gets$ empty set
  \For{each $v$ in $V$}
    \State \Call{make-set}{$v$}
  \EndFor
  \State sort $E$ into increasing order of weight
  \For{each edge $uv$ in $E$, in increasing order}
    \If{\Call{find}{$u$} $\neq$ \Call{find}{v}}
      \State $A \gets A \cup \{uv\}$
      \State \Call{union}{$u$, $v$}
    \EndIf
  \EndFor
  \State \Return{$A$}
\EndFunction
\end{algorithmic}

\texttt{MAKE-SET(v)} creates a new set whose only element is $v$. \\
\texttt{FIND(u)} returns the name of the set that $u$ is contained in. \\
\texttt{UNION(u,v)} unions together the trees containing $u$ and $v$ and gives the result a name (which can be found with \texttt{FIND}). \\

Sets are stored as linked lists of vertices. \\
Each element in the list has a pointer to its next element, and a pointer to the head of the list. \\
There is also a tail pointer to the end of the list, and the list maintains how many elements it contains. \\
The "name" is the first vertex in the list. \\

To do \texttt{MAKE-SET}, we create a linked list of a single element, and the \texttt{num} field set to 1.
This costs $O(1)$. \\

To do a \texttt{FIND}, we follow the pointer to the head of the list and return the element there.
This costs $O(1)$. \\

To do a \texttt{UNION(u,v)}, we follow the pointer of $u$ to its head, and the pointer of $v$ to its head. \\
There we find the number of elements in each list. \\
We can do this via ``weighted-union heuristic.'' \\
If (say) $u$ is in the smaller list, we link that list to the end of the bigger list ($v$'s list). \\
Then traverse $u$'s list, moving each head pointer to point to the head of $v$'s list. \\
Finally, we update the \texttt{num} field of $v$'s list. \\
This costs $O(t)$, where $t$ is the number of vertices of the smaller list. \\

Because we always update the head pointers of the elements of the smaller list, each \texttt{UNION} at least doubles the size of the smaller list. \\
So an element's name gets updated at most $O(log n)$ times. \\
Thus, over the course of the entire algorithm, we see that (all) $n$ \texttt{UNION}s take at most $O(n\lg{n})$ time.

\begin{theorem}
Kruskal's algorithm takes $O(|E|\lg{|E|}) = O(|E|\lg{|V|})$ time.
\end{theorem}
\begin{proof}
First, note that the algorithm does indeed produce a spanning tree. \\
The output cannot have a cycle, because if it did, the edge added that forms the cycle would join together two vertices in the same tree. \\
The output must be connected, for if it weren't, when we considered the edge that connected two components, we would have added it. \\

Next, we need to argue that the spanning tree $T$ produced by Kruskal's algorithm is a minimum spanning tree. \\
Assume it isn't.
Among all minimum spanning trees for $G$, the input, let $T'$ be the one with the largest number of edges in $T$. \\
If $T' = T$, we're done. \\
Otherwise, among all the edges in $T - T'$, let $e$ be the one that is added earliest by Kruskal's algorithm when we run it on $G$.

Because $T'$ is a spanning tree, $T' \cup \{e\}$ has a cycle $C$. \\
$T$ cannot contain all the edges of this cycle (otherwise it would not be a tree) so $C$ contains an edge $f$ that is not in $T$. \\
Now consider $T'' = (T' \cup \{e\}) - \{f\}$. T'' is also a spanning tree;
since T' was minimum, $weight(e) \geq weight(f)$. \\

If $weight(f) < weight(e)$, then the algorithm would have considered the edge $f$ before it would have considered the edge $e$. \\
Adding $f$ wouldn't create a cycle ($e$ was the first edge not in T'), so it would have been added. \\
But it wasn't;
$e$ was. \\
So $weight(f) \geq weight(e)$. \\
Thus $weight(e) = weight(f)$. \\

So $T''$ is also a minimum spanning tree. \\
But $T''$ has one more edge in common with $T$ than $T'$ does, contradicting the choice of $T'$.
\end{proof}

\subsubsection{Prim's Algorithm}
\lecture{November 8, 2016}
It is a greedy algorithm where, at each step, we choose a previously unconnected vertex that becomes connected by a lowest-weight edge. \\

At every step, we need to pick an appropriate vertex that is not in the tree $T$, say $r$. \\
The vertex $v$ that we want will have the minimum-cost edge connecting $v$ to some vertex already in $T$. \\
So it makes sense to keep those vertices not in $T$ in a min-priority queue (min-heap). \\

For each vertex $v$, $key[v]$ is the minimum weight of any edge connecting $v$ to a vertex in $T$;
$key[v] = \infty$ if there is no such edge.
Initially the keys of all vertices equal $\infty$. \\

At every step we do an \texttt{EXTRACT-MIN} operation on the heap to get the vertex with the minimum edge weight associated with it. \\
Once we get that vertex, we have to look at all edges adjacent to it, and find the minimum weight edge. \\
We then update the key associated with each vertex, decreasing the key (via \texttt{DECRESE-KEY} if we find a lower-weight edge. \\

We use an array $\pi[u]$, indexed by the vertices $u$ of $V$, such that $\pi[u]$ is the parent of $u$ in the minimum spanning tree. \\
The actual tree itself will consist of the edges $(v, \pi[v])$ for all vertices $v$ in $V$ (except $r$, which has no parent). \\

The key idea is that whenever a new vertex $v$ is added to the tree $T$, we look at all the new edges $(v, u)$ incident on $v$ and update our knowledge about the lowest-cost edge connecting $u$ to $T$. \\
That way we examine all edges, but the priority queue only has to hold vertices, not edges. \\

\begin{algorithmic}
\Function{prim}{$G, w, r$}
  \For{each $u \in V$}
    \State \Comment{$key[u]$ is current estimate on cheapest edge $uv$ joining $u$ to vertices not yet considered}
    \State $key[u] \gets \infty$
    \State \Comment{$\pi[u]$ is the current predecessor of $u$ in minimum spanning tree created}
    \State $\pi[u] = nil$
  \EndFor
  \State $key[r] = 0$
  \State \Comment{Q is the priority queue, vertices that remain to be considered}
  \State $Q = V$
  \While{$Q \neq \varnothing$}
    \State $u = \Call{extract-min}{Q}$
    \For{each $v$ in $Adj[u]$}
      \If{$v \in Q$ and $w(u, v) < key[v]$}
        \State $\pi[v] = u$
        \State $key[v] = w(u, v)$
      \EndIf
    \EndFor
  \EndWhile
\EndFunction
\end{algorithmic}

\emph{Note}: as each vertex is removed from the priority queue $Q$, its connection to the tree is established through the parent pointer.
Before that time, the parent pointer of a vertex might point to a different vertex. \\

How do we determine, in \texttt{PRIM}, whether $v \in Q$? \\
One simple way is just to have every vertex have a flag, and when we do an \texttt{EXTRACT-MIN} and the vertex is removed, we set this flag. \\
Alternatively we can assume the vertices are numbered 1 through $n$ and use a bit-vector (an array of size $n$) to record whether a vertex is in $Q$ or not;
this would be updated over time. \\

What is the running time? \\
We can build the heap initially in $O(|V|)$ time. \\
We call \texttt{EXTRACT-MIN} $|V|$ times, and each call costs $O(\lg{|V|})$.
So this part is $O(|V|\lg{|V|})$. \\
We have to traverse all the edges, so that is $O(|E|)$. \\
For each edge, we potentially call \texttt{DECREASE-KEY}, which costs $O(\lg{|V|})$.
So that costs $O(|E|\lg{|V|})$. \\
The total is $O(|V|\lg{|V|} + |E|\lg{|V|}) = O(|E|\lg{|V|})$, which is the same time as Kruskal's algorithm. \\

Its running time can be \emph{improved} to $O(|E| + |V|\lg{|V|})$ with a more sophisticated data structure --- Fibonacci heaps --- that supports \texttt{DECREASE-KEY} more efficiently. \\
If the graph is dense (more than a linear number of edges), this time is asymptotically better than before. \\

\subsection{Shortest Path Problems}
The \emph{single-source} shortest path problem is the following: given a source vertex $s$, and a sink vertex $v$, we'd like to find the shortest path from $s$ to $v$. \\
Here shortest path means a sequence of directed edges from $s$ to $v$ with the smallest total weight. \\
Now it turns out that for most of the known algorithms, it is just as efficient to find the shortest paths from $s$ to all the other vertices of the graph. \\

Applications of shortest paths include speech recognition, to try to figure out the meaning of sentences. \\
Here we have to distinguish between different words that sound alike (homophones), such as ``to'', ``two'', and ``too''. \\
To do this, construct a graph whose vertices are words and whose edges are words that follow the given words in a sentence. \\
The edge between two words carries a weight measuring the likelihood of the transition;
the lower the weight, the more reasonable the transition. \\
So ``to school'' would have low weight, while ``two school'' would have a high weight. \\
Then a shortest path gives the best interpretation of an uttered sentence. \\

Other applications include network routing protocols such as ``IS-IS'' (Intermediate System to Intermediate System) and ``OSPF'' (Open Shortest Path First) use shortest paths algorithms. \\

Another application is graph drawing. \\
We'd like to ``center'' a graph on a page. \\
But what's the center?
It could be a vertex that minimizes the maximum distance to any other vertex in the graph. \\
We could find this by finding the shortest path between all pairs of vertices. \\

\subsubsection{Aspects of Shortest Paths}
The definition of shortest paths involves some some subtleties. \\
Maybe we really want a shortest walk, not shortest path? \\
So should we allow \emph{negative edges}?
Of course, there are no negative distances;
nevertheless, there are actually some cases where negative edges make logical sense. \\
But then there may not \emph{be} a shortest walk, because if there is a \emph{cycle} with negative weight, we could simply go around that cycle as many times as we want and reduce the cost of the path as much as we like. \\
To avoid this, we might want to detect negative cycles.
This can be done in the shortest-path algorithm itself. \\
Non-negative cycles aren't helpful, either. \\
Suppose our shortest walk contains a cycle of non-negative weight. \\
Then by cutting it out we get a walk with the same weight or less, so we might as well cut it out. \\
So we can assume our walks are actually paths.

\subsection{Shortest Path Algorithms}
Given a source vertex $s$, a shortest-path algorithm constructs:
\begin{itemize}
  \item a predecessor $\pi[v]$ for each vertex $v$;
  this is the predecessor of $v$ in the shortest path from $s$ to $v$
  \item $d[v]$, the current upper bound on the weight of the shortest path from $s$ to $v$ ("shortest-path estimate")
\end{itemize}

\begin{algorithmic}
\Function{initialize}{$G, s$}
  \For{each vertex $v \in V$}
    \State $d[v] = \infty$
    \State $\pi[v] = nil$
  \EndFor
  \State $d[s] = 0$
\EndFunction
\end{algorithmic}

Main idea: ``relaxation'' of edges. \\
Here we already have a shortest-path estimate to both $u$ and $v$. \\
We consider an edge $(u,v)$ and then update our estimate on $v$, if $d[u] + w(u, v) < d[v]$.

\begin{algorithmic}
\Function{relax}{$u, v, w$}
  \State $t = d[u] + w(u, v)$
  \If{$d[v] > t$}
    \State $d[v] = t$
    \State $\pi[v] = u$
  \EndIf
\EndFunction
\end{algorithmic}

The Bellman-Ford algorithm: it computes the length of a shortest path $d[]$ originating from $s$ to all other vertices on the graph $G = (V,E)$, using the weight function $w$. \\
Further, it returns either true or false, false iff the graph contains a reachable negative-weight cycle, true otherwise. \\
It makes no assumptions at all about the weights on the edges; they can be negative. \\





\clearpage
\printindex
\end{document}

%%%%%%%%%%%%%%%%%%%%%
%% D O C U M E N T %%
%%%%%%%%%%%%%%%%%%%%%
