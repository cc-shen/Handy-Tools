\documentclass[12pt]{article}
\usepackage[lmargin=0.7in, rmargin=1.1in, bmargin=1.0in, tmargin=1.0in]{geometry}
\setlength{\parindent}{0in}
\usepackage{amsfonts, amssymb, amsthm, mathtools, tikz, qtree, float}
\usepackage{algpseudocode, algorithm, algorithmicx}
\usepackage{ae, aecompl, color}
\usepackage{wrapfig}
\usepackage{multicol, multicol, array}
\usepackage{imakeidx}
\makeindex[columns=2, title=Indices, intoc]

\usepackage[pdftex, pdfauthor={Charles Shen}, pdftitle={CS 341: Algorithms}, pdfsubject={Lecture notes from CS 341: at the University of Waterloo}, pdfkeywords={course notes, notes, Waterloo, University of Waterloo}, pdfproducer={LaTeX}, pdfcreator={pdflatex}]{hyperref}
\usepackage{cleveref}

\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace}
\definecolor{darkish-blue}{RGB}{25,103,185}

\hypersetup{
  colorlinks,
  citecolor=darkish-blue,
  filecolor=darkish-blue,
  linkcolor=darkish-blue,
  urlcolor=darkish-blue
}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[subsection]

\theoremstyle{definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{ex}[theorem]{Example}
\newtheorem*{ex*}{Example}
\newtheorem{defn}{Definition}
\newtheorem*{defn*}{Definition}

\crefname{ex}{Example}{Example}

\setlength{\marginparwidth}{1.0in}
\newcommand{\lecture}[1]{\marginpar{{\footnotesize $\leftarrow$ \underline{#1}}}}

\newcommand{\floor}[1]{\ensuremath{\left\lfloor #1 \right\rfloor}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil #1 \right\rceil}}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\allowdisplaybreaks

\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

%%%%%%%%%%%%%%%%%%%%%
%% D O C U M E N T %%
%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\let\ref\Cref
\pagenumbering{roman}

\title{\bf{CS 341: Algorithms}}
\date{Fall 2016, University of Waterloo \\ \center Notes written from Jeffrey Shallit's lectures.}
\author{Charles Shen}

\blfootnote{Feel free to email feedback to me at
\href{mailto:ccshen902@gmail.com}{ccshen902@gmail.com}.}

\maketitle
\thispagestyle{empty}
\newpage
\tableofcontents
\newpage
\pagenumbering{arabic}

\newpage
\section{Introduction}
\lecture{2016/09/08}
A few things to review first:
\begin{itemize}
  \item sorting algorithms (quicksort, heapsort, mergesort, counting sort, radix sort)
  \item lower bound for sorting in comparison-based model
  \item searching algorithms (binary search, interpolation search)
  \item data structures (trees, graphs, binary search trees, balanced binary trees, priority queues)
  \item hashing
  \item string search (Knuth-Morris-Pratt linear time)
  \item big-O notation
\end{itemize}

What is an algorithm? \\
A series of computational steps intended to solve a problem. \\
Algorithms should be,
\begin{itemize}
  \item effective (can actually carry out steps)
  \item unambiguous --- steps are not subject to interpretation
  \item have a finite description (only finitely many lines of code)
  \item finite (should eventually halt; some exceptions to this rule, such as servers or operating systems)
  \item deterministic (although randomized algorithms are also studied these days)
\end{itemize}

Focuses on,
\begin{itemize}
  \item Design: what are some techniques to build algorithms to solve problems?
  \item Analysis: what resources are consumed by the algorithm?
  \begin{itemize}
    \item Time
    \item Space
    \item Randomness
  \end{itemize}
  \item Hardness: what problems cannot be solved with algorithms? Or cannot be solved efficiently? Lower bounds for problems.
\end{itemize}

\newpage
\section{Algorithm Analysis}
Algorithm analysis is hard!
A simple algorithm can have a very difficult or incomplete analysis. \\

We want a rigorous analysis of algorithms, where we count the number of steps an algorithm requires.
Steps are usually measured as a function of $n$, the number of inputs. \\

The way we count steps depends on our \emph{model of computation}. \\
There are many models, and most are theoretical models that are far simpler than a modern computer because they are easier to analyze! \\
More complicated models are much harder to analyze.
Think of all the parts of a computer: the RAM, the primary cache, the secondary cache, the disk, etc.
Modelling all of these precisely would be a real pain. \\
So we simply things by assuming that each ``basic operations'' takes 1 unit of time. \\
This is the so-called ``\emph{unit cost}'' model. \\

The unit-cost model is good if all operations use numbers that fit in a single computer word and if you are interested in analyzing an algorithm's behaviour on $n$ inputs as $n$ gets large. \\

Another model is the ``\emph{log-cost}'' model, where all basic operations are charged a cost proportional to the \textbf{size} (number of bits) of the number(s) you are dealing with;
in other words, the base-2 logarithms of the number(s). \\
If there is more than one number, you take the logarithms of each and add them up. \\
When you're all done, you often phrase the result in terms of $n$, which is now the \textbf{number of bits of the input}. \\
The number of bits required to represent a positive number $x$ is $(\floor{\lg{|x|}} + 1)$ for $x \neq 0$ and 1 if $x = 0$, but there is usually no harm in just using $\lg{x}$ as an approximation.

\begin{ex*}
Suppose we have $x \times y + z$. \\
There are two operations: a multiplication and an addition. \\
The multiplication costs $\lg{x} + \lg{y}$. \\
In the addition, we are adding $xy$ to $z$, so the cost is $\lg(xy) + \lg{z}$. \\
The total log cost is thus $\lg{x} + \lg{y} + \lg(xy) + \lg{z} = 2(\lg{x} + \lg{y}) + \lg{z}$.
\end{ex*}

\textbf{Summarizing}: \\
Use the unit-cost model if your algorithm has a variable number of inputs, each of which fits in a machine word. \\

Use the log-cost model if your algorithm has a single input, or where the individual inputs typically don't fit in a single machine word.

\subsection{More on Algorithm Analysis}
Sometimes computing the cost of a step strongly depends on how things are implemented.

\begin{ex*}
Consider concatenating (joining) two lists of length $n$. \\
The cost in the unit-cost model depends entirely on \emph{how the lists are represented}. \\

If the lists are represented as arrays, then you'll have to allocate a new array of length $2n$, then copy the elements of the two lists into it.
All that is \textbf{non-destructive} (doesn't affect the two lists) and it would cost $\Theta(n)$.\\

If the lists are linked lists, and you maintain pointers to the front and back of the list, then you could concatenate two such lists, in a \textbf{destructive} way, by moving pointers. \\
This would cost only $\Theta(1)$.
\end{ex*}

Therefore, when estimating costs, we need to think carefully about how the items being manipulated would be represented at the very lowest level of the machine.

\subsection{Big-O, Big-Theta, and so forth}
Four important kinds of asymptotic notations: $O$, $\Omega$, $\Theta$, $o$

\begin{defn*}
We say $f(n)$ is $O(g(n))$ if there exist constants $c > 0$, $n_{0}$ such that $f(n) \leq cg(n)$ for all $n \geq n_{0}$.
\end{defn*}

\begin{defn*}
We say $f(n)$ is $\Omega(g(n))$ if there exist constants $c > 0$, $n_{0}$ such that $f(n) \geq cg(n)$ for all $n \geq n_{0}$.
\end{defn*}

\begin{defn*}
We say $f(n)$ is $\Theta(g(n))$ if there exist constants $c_{1} > 0$, $c_{2} > 0$, $n_{0}$ such that $c_{1}g(n) \leq f(n) \leq c_{2}g(n)$ for all $n \geq n_{0}$. \\
Equivalently, $f(n)$ is $\Theta(g(n))$ if both $f(n) = O(g(n))$ and $g(n) = O(f(n))$.
\end{defn*}

\begin{defn*}
We say $f(n)$ is $o(g(n))$ if $\lim_{n\to\infty}f(n)/g(n) = 0$.
\end{defn*}

If $f = O(g)$, then the function $f$ \emph{grows more slowly than} $g$ or \emph{has slower growth rate} than $g$. \\
Slow-growing functions corresponds to quickly-running algorithms. \\

\textbf{Stirling's approximation}:
$$n! = n^{n}e^{-n}\sqrt{2\pi n}\left(1 + \Theta\left(\frac{1}{n}\right)\right) = \left(\frac{n}{e}\right)^{n}\sqrt{2\pi n}\left(1 + \Theta\left(\frac{1}{n}\right)\right)$$

\subsubsection{Tricks}
\textbf{Trick 1}: \\
If $\lim_{n\to\infty}f(n)/g(n) = c$, then
\begin{itemize}
  \item if $c = 0$, then $f = o(g)$;
  \item if $0 < c < \infty$, then $f = \Theta(g)$; and
  \item if $c = \infty$, then $g = o(f)$
\end{itemize}
\medskip \textbf{Trick 2}: \\
$n^{b} = o(a^{n})$ for all real numbers $a$ and $b$ such that $a > 1$. \\
$(\lg{n})^{b} = o(n^{a})$ for every fixed positive number $a$ and every fixed real number $b$. \\

\medskip \textbf{Trick 3}: \\
L'H\^opital's Rule, which says that if $f$ and $g$ are differentiable functions, and \emph{both} $f$ and $g$ tend to $\infty$ as $n$ tends to $\infty$, then
$$\lim_{n\to\infty}\frac{f(n)}{g(n)} = \lim_{n\to\infty}\frac{f'(n)}{g'(n)}$$ \\

\textbf{Trick 4}: \\
If you're trying to prove $f(n) = O(g(n))$, sometimes it's easier just to prove an explicit inequality, such as $f(n) < cg(n)$ for some appropriate explicit constant $c$. \\

\medskip \textbf{Trick 5}: \\
Use fact that if $f_{1} = O(g_{1})$ and $f_{2} = O(g_{2})$, then $f_{1} + f_{2} = O(g_{1} + g_{2})$ and $f_{1}f_{2} = O(g_{1}g_{2})$. \\

\medskip \textbf{Trick 6}: \\
If $f(n), g(n) \geq 1$ for all sufficiently large $n$ and $f = o(g)$ then $e^{f} = o(e^{g})$.

\subsection{Polynomial Time}
\begin{defn*}
An algorithm runs in \emph{polynomial time} if there exists a $k \geq 0$ such that its worst-case time complexity $T(n)$ is $O(n^{k})$.
\end{defn*}

We generally think of an algorithm as ``good'' if it runs in polynomial time.

\newpage
\section{Maximum Subrange Sum}
Given an array $x[1..n]$ of integers (possibly \emph{negative}) and we want to find the block of contiguous entries with the largest sum.

\subsection{Naive and Still Naive}
Enumerate all possible subblocks of the array,
\begin{verbatim}
  maxsubrangesum1(x, n):
    max = 0
    for lower = 1 to n do
      for upper = lower to n do
        sum = 0
        for i = lower to upper do
          sum = sum + x[i]
        if sum > max then
          max = sum
\end{verbatim}
Run time: $O(n^{3})$. \\

\textbf{Slightly better}: \\
We're

\newpage
\section{Selection in Deterministic Linear Time}
\lecture{2016/10/25}
In the selection problem, the goal is to determine the $i$-th smallest element from an unsorted list of $n$ elements in linear time. \\
``Deterministic'' here means that no random numbers are used. \\
For simplicity, suppose that all elements are distinct, but this is not crucial as a small modification will make it work any list. \\

One method to choose a pivot is via randomness. \\
We want a deterministic method to clever choose a pivot to be used to partition a list. \\
Thus we arrive at the \texttt{medians-of-five} algorithm by Blum, Floyd, Pratt, Rivest, and Tarjan. \\

\textbf{Idea}: \\
Split the list into $\floor{\frac{n}{5}}$ groups of 5 elements, and one additional group of at most 4 elements. \\
Then sort each group of 5 elements, and find the median of each group of 5.
This can be done in constant time, so sorting each group of 5 costs $O(n)$. \\
Note that that median is the 3rd (center) element in the group of 5.
This gives a list of $\floor{\frac{n}{5}}$ medians. \\
Now recursively determine the median of these $\floor{\frac{n}{5}}$ medians, which is the element we'll partition around. \\

We arrive at the algorithm:
\begin{algorithmic}
\Function{select}{$A, i$}
  \State $n \gets |A|$
  \If{$n < 60$}
    \State \Call{sort}{$A$}
    \State \Return $i$-th smallest element
  \Else
    \State $m \gets \floor{\frac{n}{5}}$
    \State divide A up to $m$ groups of 5 elements, with at most one remaining group of $\leq 4$ elements
    \State sort each of the $m$ groups in ascending order
    \State $M \gets$ array of medians of each group
    \State $x \gets \Call{select}{M}{\ceil{m/2}}$
    \Comment{median of all the medians}
    \State $k \gets \Call{x-partition}{A, x}$
    \Comment{partition array A into elements $\leq$ x and elements $>$ x; returns number of elements on ``low side'' of the partition}
    \If{$i = k$}
      \State \Return $x$
    \ElsIf{$i < k$}
      \State \Return \Call{select}{$A[1..k-1], i$}
    \Else
      \State \Return \Call{select}{$A[k+1..n], i - k$}
    \EndIf
  \EndIf
\EndFunction
\end{algorithmic}

Let's obtain a lower bound on the number of elements $> x$, the median of medians. \\
Here's a picture for $n = 37$:
\begin{verbatim}
  smallest   S    S    S    S    *    *    *    *

             S    S    S    S    *    *    *    *

             S    S    S    m    L    L    L

             *    *    *    L    L    L    L

  largest    *    *    *    L    L    L    L

  m = median of medians
\end{verbatim}

There are $\floor{\frac{n}{5}}$ total columns in which 5 elements appear. \\
Of these, at least $\frac{1}{2}$ (more precisely, $\ceil{\floor{n/5}/2}$) contain an L.
All of these columns, except the one where $x$ appears, contributes 3 to the count of L's; the one where $x$ appears contributes 2. \\

The conclusion is that $\geq 3\ceil{\floor{n/5}/2} - 1$ elements are L's, that is, greater than $x$.
Hence, at most $\leq n - (3\ceil{\floor{n/5}/2} - 1)$ elements are $\leq x$. \\

Then, $n - (3\ceil{\floor{n/5}/2} - 1) \leq 7n/10 + 3$ for all $n \geq 1$.
To prove this, see that
$$\floor{n/5} \geq n/5 - 1$$
so
$$ceil{floor{n/5}/2} \geq n/10 - 1/2$$
then
$$3\ceil{\floor{n/5}/2} - 1 \geq 3n/10 - 5/2$$
hence
$$n - (3\ceil{\floor{n/5}/2} - 1) \leq 7n/10 + 3$$

We can also claim that at most $(7n/10 + 3)$ elements are $\geq x$ using the same proof above. \\
So whether we recurse in the smaller elements or larger elements, we can use this bound. \\

It follows that the time $T(n)$ to select from a list of n elements satisfies the following inequality:
$$T(n) \leq T(\floor{n/5}) + T(r) + dn$$
where $r \leq 7n/10 + 3$, and the $dn$ term soaks up the time needed to do the sorting of 5-element groups, and the partitioning. \\

Suppose that this recurrence obeys $T(n) \leq cn$ for some $c$,
\begin{align*}
T(n) &\leq T(\floor{n/5}) + T(r) + dn \\
&\leq cn/5 + cr + dn \\
&\leq cn/5 + c(7n/10 + 3) + dn \\
&\leq 9cn/10 + 3c + dn
\end{align*}
and we want this to be less than $cn$. \\

Now, $9cn/10 + 3c + dn \leq cn \iff (c/10 - d)n \geq 3c$ \\
Now $n$ had better be bigger than 30 here, or else we are in a heap o' trouble.
So let's assume $n \geq 60$. Then $(c/10 - d) n \geq (c/10 - d) 60$, and if this is to be $\geq 3c$, then we must have $6c - 60d \geq 3c$, and so $c \geq 20d$. \\
If $n \geq 60$, and $c \geq 20d$, then the induction step indeed works. \\

To complete the proof, it remains to see that $T(n) \leq cn$ for $n = 1, 2, 3, \dots, 59$. \\
To do this, simply choose
$$c = max(20d, T(1), T(2)/2, ..., T(59)/59)$$

That looks like cheating, but it works. \\
Now the basis (namely $1 \leq n \leq 59$) works out just fine, and so does the induction step.

The presence of these large constants (such as $c = 20d$) suggests that this algorithm is of more theoretical than practical interest. \\
However, with a bit more work, the number of comparisons can be reduced quite a lot, down to $(3+o(1))n = 3n + o(n)$. \label{sec:linear_select}

\newpage
\section{Lower bounds}
Can we prove that an algorithm is best possible?
Sometimes. \\

We argued that any comparison-based algorithm for sorting must use $\Omega(n \lg n)$ time.
And in reference to the convex hull algorithm, we argued that we couldn't do better than $\Omega(n \lg n)$, because we could use convex hull as a subroutine to create a sorting algorithm;
if we could solve convex hull in $o(n \lg n)$ time, then we would end up being able to sort in $o(n \lg n)$ time, a contradiction. \\

By ``lower bounds'', we mean a lower bound on the complexity of a \emph{problem}, not an algorithm.
Basically we need to prove that \emph{no algorithm}, no matter how complicated or clever, can do better than our bound. \\
We don't need each algorithm to be ``slow'' as $f(n)$ on the \emph{same} input, but we do need each algorithm to be ``slow'' as $f(n)$ on \emph{some} input. \\
Also, a lower bound doesn't rule out the possibility that some algorithm might be very fast on \emph{some inputs} (but not all).

What do we mean by complexity of a problem? \\
We say that a function $f$ is a lower bound on the complexity of a problem $P$ if, for \emph{every algorithm} $A$ to solve $P$, and every positive integer $n$, there exists some input $I$ of size $n$ such that $A$ uses at least $f(n)$ steps on input $I$. \\

The easiest kind of lower bound is the ``information-theoretic bound''. \\
Idea: every algorithm that uses \emph{only} yes/no questions to decide among $k$ possible alternatives, \emph{must} ask at least $(\lg k)$ questions in the worst case.
\begin{proof}
Use an ``adversary argument''. \\
The adversary doesn't decide on one of the alternatives ahead of time, but rather it answers the algorithm's questions by always choosing the alternative that maximizes the size of the solution space. \\
Since the questions are yes/no, the solution space goes down in size by a factor of at most two with each question. \\
The algorithm cannot answer correctly with certainty unless the solution space is of size 1. \\
Thus at least $(\lg k)$ questions are needed.
\end{proof}

\begin{theorem}
Any comparison-based sorting algorithm must use at least $\lg(n!) = \Omega(n log n)$ questions in the worst-case.
\end{theorem}
\begin{proof}
Use the information-theoretic bound, and observe that the space of possible solutions includes the $n!$ different ways the input could be ordered.
\end{proof}

\newpage
\section{Adversary Strategy}
\subsection{Adversary Arguments in General}
More generally, we can think of a lower bound proof as a game between the algorithm and an "adversary". \\
The algorithm is "asking questions" (for example, comparing two elements of an array), while the adversary is answering them (providing the results of the comparison). \\

The adversary should be thought of as a very powerful, clever being that is answering in such a way as to make your algorithm run as slowly as possible. \\
The adversary cannot "read the algorithm's mind", but it can be prepared for anything the algorithm might do. \\
Finally, the adversary is not allowed to "cheat";
that is, the adversary cannot answer questions inconsistently.
The adversary does not not need to have a particular input in mind at all times when it answers the questions, but when the algorithm is completed, there must be at least one input that matches the answers the adversary gave (otherwise it would have cheated). \\

The algorithm is trying to run as quickly as possible. \\
The adversary is trying (through its cleverness) to \emph{force} the algorithm to run slowly. \\
This interaction proves that the lower-bound obtained by this type of argument applies to \emph{any possible} algorithm from the class under consideration. \\

\begin{theorem}
Every comparison-based algorithm for determining the minimum of a set of $n$ elements must use at least $(\frac{n}{2})$ comparisons.
\end{theorem}
\begin{proof}
Every element must participate in at least one comparison;
if not, the not compared element can be chosen (by an adversary) to be the minimum. \\
Each comparison compares 2 elements. \\
Hence, at least $(\frac{n}{2})$ comparisons must be made.
\end{proof}

\begin{theorem}
Every comparison-based algorithm for determining the minimum of a set of $n$ elements must use at least $(n - 1)$ comparisons.
\end{theorem}
\begin{proof}
To say that a given element, $x$, is the minimum, implies that (1) every other element has won at least one comparison with another element (not necessarily $x$). (By "$x$ wins a comparison with $y$" we mean $x > y$.) \\
Each comparison produces at most one winner. \\
Hence at least $(n - 1)$ comparisons must be used. \\

To convert this to an adversary strategy, do the following: for each element, record whether it has won a comparison ($W$) or not ($N$). \\
Initially all elements are labelled with $N$. \\
When we compare an $N$ to an $N$, select one arbitrarily to be the $W$. \\
If no values assigned, choose them to make this so. \\
If values are assigned, decrease the loser (if necessary) to make this so. \\
When we compare $W$ to $N$, always make the $N$ the loser and set or adjust values to make sure this is the case. (You might need to decrease the loser's value to ensure this.) \\
When we compare $W$ to $W$, use the values already assigned to decide who is the winner. \\

Now each comparison increases the total number of items labelled $W$ by at most 1. \\
In order to "know" the minimum, the algorithm must label $(n - 1)$ items $W$, so this proves at least $(n - 1)$ comparisons are required. \\
The values the adversary assigned are the inputs that forced that algorithm to do the $(n - 1)$ comparisons.
\end{proof}

\subsection{Finding Both the Maximum and Minimum of a List of n Numbers}
It is possible to compute both the maximum and minimum of a list of n numbers, using $(\frac{3}{2}n - 2)$ comparisons if n is even, and $(\frac{3}{2}n - \frac{3}{2})$ comparisons if n is odd. \\
It can be proven that \emph{no comparison-based method} can correctly determine both the max and min using fewer comparisons in the worst case. \\

We do this by constructing an adversary argument. \\

In order for the algorithm to correctly decide that $x$ is the minimum and $y$ is the maximum, it must know that
\begin{enumerate}
  \item every element other than $x$ has won at least one comparison (i.e. $x > y$ if $x$ wins a comparison with $y$),
  \item every element other than $y$ has lost at least one comparison
\end{enumerate}
Calling a win $W$ and a loss $L$, the algorithm must assign $(n - 1)$ $W$'s and $(n - 1)$ $L$'s.
That is, the algorithm must determine $(2n - 2)$ "units of information" to always give the correct answer. \\

We now construct an adversary strategy that will force the algorithm to learn its $(2n - 2)$ "units of information" as slowly as possible. \\
The adversary labels each element of the input as $N$, $W$, $L$, or $WL$.
These labels may change over time. \\
$N$ signifies that the element has never been compared to any other by the algorithm. \\
$W$ signifies the element has won at least one comparison. \\
$L$ signifies the element has lost at least one comparison. \\
$WL$ signifies the element has won at least one and lost at least one comparison. \\

\lecture{2016/10/27}

So the the adversary uses the following table. \\
\begin{table}[H]
  \centering
  \begin{tabular}{C{3cm}  C{3cm}  C{3cm}  C{3cm}}
  \hline
  labels when comparing elements $(x, y)$ & the adversary's response & the new label & unit of information given to the alg. \\ \hline \hline
  $(N, N)$ & $x > y$ & $(W, L)$ & 2 \\ \hline
  $(W, N)$ or $(WL, N)$ & $x > y$ & $(W, L)$ or $(WL, L)$ & 1 \\ \hline
  $(L, N)$ & $x < y$ & $(L, W)$ & 1 \\ \hline
  $(W, W)$ & $x > y$ & $(W, WL)$ & 1 \\ \hline
  $(L, L)$ & $x > y$ & $(WL, L)$ & 1 \\ \hline
  $(W, L)$ or $(WL, L)$ or $(W, WL)$ & $x > y$ & no change & 0 \\ \hline
  $(WL, WL)$ & consistent with assigned values & no change & 0
  \end{tabular}
\end{table}
The adversary also tentatively assigns values to the elements, which may change over time. \\
However, they can only change in a fashion \emph{consistent} with previous answers.
That is, an element labelled $L$ may only \emph{decrease} in value (since it lost all previous comparisons, if it is decreased, it will still lose all of them), and an element labelled $W$ may only increase in value. \\
An element labelled $WL$ cannot change. \\

\begin{ex*}
Consider the following sequence of possible questions asked by the algorithm and the adversary's responses:
\begin{verbatim}
  -----------------------------------------------------------
  Algorithm  |   Adversary's responses and worksheet
  compares   |   x1      x2      x3      x4      x5      x6
  -----------------------------------------------------------
   (x1, x2)  |  W-20    L-10      N       N       N       N
   (x4, x5)  |  W-20    L-10      N     W-30     L-15     N
   (x1, x4)  |  W-40    L-10      N    WL-30     L-15     N     (*)
   (x3, x6)  |  W-40    L-10    W-11   WL-30     L-15    L-2
   (x2, x5)  |  W-40   WL-10    W-11   WL-30     L-7     L-2
   (x3, x1)  | WL-40   WL-10    W-50   WL-30     L-7     L-2
   (x2, x4)  | WL-40   WL-10    W-50   WL-30     L-7     L-2    (**)
   (x5, x6)  | WL-40   WL-10    W-50   WL-30    WL-7     L-2
  -----------------------------------------------------------
\end{verbatim}
At this point the algorithm knows that $x_3$ is the maximum, since it is the only element that has never lost a comparison, and $x_6$ is the minimum, since it is the only element that has never won a comparison. \\
Note that in step (*), the adversary was forced to reassign the value he had previously assigned to $x_1$, since $x_1$ had to win the comparison against $x_4 = 30$. \\
This is permitted, since $x_1$ had won every previous comparison and so increasing its value ensures consistency with previous answers. \\
Also note that step (**) is superfluous, as the algorithm didn't learn any new information.
\end{ex*}

\begin{theorem}
Every \emph{comparison-based} method for determining both the maximum and minimum of a set of $n$ numbers must use at least $(\frac{3}{2}n - 2)$ comparisons (if $n$ even), or $(\frac{3}{2}n - \frac{3}{2})$ comparisons (if $n$ odd), in the worst case.
\end{theorem}
\begin{proof}
Suppose that $n$ is even. \\
As shown before, the algorithm must learn $(2n - 2)$ units of information. \\
The most it can learn in one comparison is 2 units;
when both elements have never participated before in a comparison, labelled by $(N, N)$.
This can happen at most $(\frac{n}{2})$ times. \\
To learn the remaining $(n - 2)$ units of info., the algorithm must ask $(n - 2)$ questions. \\
The total number of questions is therefore at least $\frac{n}{2} + n - 2 = \frac{3}{2}n - 2$. \\
The same kind of argument works for $n$ odd.
\end{proof}

So we have a lower bound for the problem of finding both the maximum and minimum of a set of $n$ numbers. \\

So the algorithm is as followed: \\
If $n$ is even, compare $x[1]$ with $x[2]$ (determining both the maximum and minimum with 1 comparison), $x[3]$ with $x[4]$, etc.
We get $(\frac{n}{2})$ maxima and $(\frac{n}{2})$ minima. \\
To find the maximum, use the usual method on the $(\frac{n}{2})$ maxima, which uses $(\frac{n}{2} - 1)$ comparisons, and the same for the minima. \\
The total cost is $\frac{n}{2} + \frac{n}{2} - 1 + \frac{n}{2} - 1 = \frac{3}{2}n - 2$.
A similar method works when $n$ is odd. \\

Notice that each comparison adds at most one $W$, and we need to get $(n - 1)$ $W$'s before we know the remaining element must be the minimum. \\
So we need at least $(n - 1)$ comparisons. \\
If the algorithm declares a minimum before getting $(n - 1)$ $W$'s, then there are at least two elements without $W$, so if the algorithm declares one of the minimum, the adversary can truthfully produce the other as the minimum.

\subsection{Remarks on Constructing an Adversary Strategy}
Lower bound arguments are some of the most difficult and deepest areas in theoretical computer science. \\

\textbf{Tips for producing an adversary strategy}: \\

Recall that the lower bound argument can be viewed as a game between you (playing the role of an algorithm) and an adversary. \\
You want the algorithm to run quickly (for example, by making a small number of queries to the input). \\
The adversary wants the algorithm to run slowly. \\
The adversary "wins" (and the lower bound of $f(n)$ is proved) if he/she can force the algorithm to execute $f(n)$ steps. \\

A proof behaves something like: for all sequences of queries into the input made by the algorithm, there exists a sequence of replies by the adversary, such that the algorithm is forced to execute f(n) steps.
So your adversary argument must apply to \emph{every} possible "move" the algorithm could make. \\

In constructing its replies, the adversary can do as much or as little bookkeeping as necessary. \\
There is no requirement that the adversary's computation be efficient or run in any particular time bound.

\subsection{Finding the Second Largest}
Problem: given a list of $n$ elements, find the 2nd largest. \\

The naive method (finding the largest, remove it, and then find the largest again) takes $2n - 3$ comparisons. \\
Instead, we can do a ``tournament'' method. \\
Have the elements in pairs, the larger one (winner) advances;
then the 2nd largest (runner-up) is among those defeated by the winner!
This is because any other element must have played two other elements superior to it. \\

This algorithm uses $(n - 1)$ comparisons to determine the largest, plus $\ceil{\lg n} - 1$ comparisons to determine the runner-up (2nd largest). \\
The total is $(n + \ceil{\lg n} - 2)$ comparisons. \\

We can prove that the algorithm just given is \emph{optimal}, in the sense that no comparison-based algorithm uses fewer than $(n + \ceil{\lg n} - 2)$ comparisons in the worst case. \\

\emph{Note}: see Lecture 14 for full proof! (\href{https://www.student.cs.uwaterloo.ca/~cs341/lecture14.html}{Click here.})

\subsection{A Lower Bound for Finding the Median}
The best algorithm known (in terms of number of comparisons used) uses $3n + o(n)$ comparisons in the worst case (see \ref{sec:linear_select}). \\
The best \emph{lower bound} known for the median-finding problem is $2n + o(n)$ comparisons. \\

We will prove a weaker lower bound: $\frac{3}{2}n + O(1)$, where $n$ odd for convenience. \\

First, note that any comparison-based algorithm for finding the median must use at least $(n - 1)$ comparisons.
That's because the algorithm \emph{must} establish the relationship of each element to the median---either by direct comparison, or by comparison with another element whose relationship to the median is known. \\
The algorithm \emph{must} eventually gain enough knowledge equivalent to the following tree:
\begin{verbatim}
      *   *
       \ /                  elements > median
        *  *
        | /
  ------x-------------------------median-----
      / | \
     *  *  *                elements < median
        |
        *
\end{verbatim}

Note that if there were an element that didn't participate in a comparison, we couldn't know the median. \\
For an adversary could change the value of that element from, say, less than the median to greater than the median, changing the median to some other element. \\
Similarly, if there is some element whose relationship to the median is not known (because, for example, it is greater than some element that is less than the median), we could simply move it from one side of the line above to the other, without changing comparisons.
This would also change the median. \\

Now there are n elements in the tree above, so there are $(n - 1)$ edges, and hence $(n - 1)$ comparisons. \\
We now improve this to $\frac{3}{2}n + O(1)$. \\
First, we call a comparison involving x crucial, say x against y, if it is the first comparison where
\begin{enumerate}
  \item $x < y$ and $y \leq$ median OR if
  \item $x > y$ and $y \geq$ median
\end{enumerate}
So the algorithm must perform at least $(n - 1)$ crucial comparisons in order to determine the median. \\

\lecture{2016/11/01}

We now give an adversary strategy to \emph{force} the algorithm to perform $(n - 1)/2$ \emph{non-crucial} comparisons (since we've proved earlier that the algorithm requires $(n - 1)$ \emph{crucial} comparisons. \\

Idea: \\
First, the adversary picks some value (\emph{not} some element) to be the median. \\
Then it assigns a label in $\{N, L, S\}$ to each element, and also values, as appropriate. \\
Initially each element is assigned an $N$ (``Never participated in any comparison'').
$L$ means Larger than the median and $S$ means Smaller than the median. \\
The adversary follows the following strategy:
\begin{table}[H]
  \label{tab:ad_median_strat}
  \centering
  \begin{tabular}{L{4cm}|L{8cm}}
  \hline
  Algorithm compares & Adversary responds \\ \hline \hline
  $(N, N)$ & assign one element to be larger than the median, one smaller; result is $(L, S)$ \\ \hline
  $(S, N)$ or $(N, S)$ & assign the $N$-element to be larger than the median; result is $(S, L)$ or $(L, S)$ \\ \hline
  $(L, N)$ or $(N, L)$ & assign the $N$-element to be smaller than the median; result is $(L, S)$ or $(S, L)$ \\ \hline
  $(S, L)$ or $(L, S)$ or $(S, S)$ or $(L, L)$ & consistent with previously assigned values \\ \hline
  \end{tabular}
\end{table}
\emph{Note 1}: this strategy continues until $((n - 1)/2)$ $S$'s (or $((n - 1)/2)$ $L$'s) have been assigned.
If at some point $((n - 1)/2)$ $S$'s are assigned, then the adversary assigns the remaining elements to be greater than the median, except for one, which \emph{is} the median.
A similar thing is done if $((n - 1)/2)$ L's have been assigned. \\

\emph{Note 2}: the last element assigned is \emph{always} the median. \\

I claim that this strategy will always force the algorithm to perform $((n - 1)/2)$ non-crucial comparisons. \\
For any time an $N$-element is compared, a non-crucial comparison is done (except at the very end, when a crucial comparison may be done with the median itself). \\
The least number of comparisons with $N$-elements that can be done is $((n - 1)/2)$. \\
The \emph{total} number of comparisons is therefore $n - 1 + (n - 1)/2 = (3/2)(n - 1)$.

\begin{ex*}
The adversary strategy when $n = 7$.
The adversary arbitrarily chooses the median to be 43.
Then
\begin{verbatim}
  ---------------------------------------------------
  Algorithm   Adversary's strategy (label:value)
  compares    x1    x2    x3    x4    x5    x6    x7
  ----------------------------------------------------
              N     N     N     N     N     N     N
  (x1, x3)   L:50   N    S:31   N     N     N     N
  (x3, x4)   L:50   N    S:31  L:47   N     N     N
  (x6, x7)   L:50   N    S:31  L:47   N    L:60  S:20

  -------  at this point there are (7-1)/2 L's -------
  -------  assigned, so adversary assigns the --------
  -------  remaining elements to L and the median ----

             L:50  43    S:31  L:47  S:15  L:60  S:20
  (x2, x5)   a crucial comparison
  (x2, x1)   a crucial comparison
  (x2, x3)   a crucial comparison
  (x2, x4)   a crucial comparison
  (x2, x6)   a crucial comparison
  (x2, x7)   a crucial comparison
\end{verbatim}
At this point the algorithm concludes that \texttt{x2} is the median. \\
The first three comparisons were non-crucial; only the last six are crucial.
\end{ex*}

The best lower bound known for the median: Dor and Zwick, Median selection requires $(2 + \epsilon)n$ comparisons.

\subsection{Lower Bound on Average-Case Sorting}
\begin{theorem}
Any comparison-based algorithm must use $\Omega(n \lg n)$ comparisons on \emph{average}.
\end{theorem}

\begin{lemma}
In any binary tree $T$ where each node has 0 or 2 children,
$$\sum_{\ell \text{ a leaf of } T} 2^{-\text{depth}(T, \ell)} = 1$$
where depth($T, \ell$) is the length of the path from $\ell$ to the root.
\end{lemma}
\begin{proof}
Prove by induction on the height of $T$. \\
Easy to see the lemma holds when $T$ consists of a single node. \\
Assume it holds for all tree of height $< k$;
prove it for trees of height $k$. \\
Then a tree of height $k$ consists of a root and two subtrees, $T'$ and $T''$. \\
Then the lemma applies to each of these subtrees recursively, so we know that
$$\sum_{\ell \text{ a leaf of } T'} 2^{-\text{depth}(T', \ell)} = 1$$
and
$$\sum_{\ell \text{ a leaf of } T''} 2^{-\text{depth}(T'', \ell)} = 1$$
Then we have
$$\sum_{\ell \text{ a leaf of } T'} 2^{-\text{depth}(T, \ell)} = \frac{1}{2}$$
since the depth of a leaf in $T$ goes up by 1 when considered as a leaf of $T$. \\
The same result holds for $T''$. \\
Adding these together yields the result.
\end{proof}

\begin{lemma}
Suppose $a, b \in \mathbb{R}$ and $a, b > 0$. \\
If $a \neq b$, then we have
$$\lg a - \lg b > 2\lg((a + b)/2)$$
\end{lemma}
\begin{proof}
If $a \neq b$, then
\begin{align*}
(\sqrt{a} - \sqrt{b})^{2} &> 0 \\
a - 2\sqrt{ab} + b &> 0 \\
(a + b)/2 &> \sqrt{ab} \\
\lg((a + b)/2) &> \frac{1}{2}\lg(ab) \\
2\lg((a + b)/2) &> \lg{a} + \lg{b} \\
-2\lg((a + b)/2) &< -\lg{a} - \lg{b}
\end{align*}
as required.
\end{proof}

\begin{lemma}
Suppose $x_{1}, x_{2}, \dots, x_{k} \in \mathbb{R}$ and $x_{1}, x_{2}, \dots, x_{k} > 0$ such that $\sum_{1 \leq i \leq k}x_{i} = 1$.
Then
$$\sum_{1 \leq i \leq k}(-\lg x_{i}) \geq k\lg k $$
\end{lemma}
\begin{proof}
If all the $x_{i} = \frac{1}{k}$, then the inequality clearly holds - actually then it is an equality! \\
Suppose we claim this sum actually achieves its minimum at this point. \\
For if the minimum occurred at some \emph{other} point, then two of the $x_{i}$ must be different, say $a$ and $b$. \\
Then by Lemma 3.6.3, we could replace $(-\lg{a}) + (-\lg{b})$ with two copies of $-\lg((a + b)/2)$ and get a smaller sum, a contradiction.
\end{proof}

\begin{proof}
Consider the comparison tree used by the algorithm. \\
There are at least $k = n!$ leaves, since there are are $n!$ possible ways to order $n$ numbers. \\
There are also at most this number of leaves, since otherwise two leaves would be labelled with the same ordering, and then, by tracing up to the lowest common ancestor, we would get to a node where there are two different answers, implying the orderings cannot be the same. \\
So there are exactly $n!$ leaves. \\
By Lemma 3.6.2,
$$\sum_{\ell \text{ a leaf of } T}2^{-\text{depth}(\ell)} = 1$$
Then by Lemma 3.6.4, with $x_{i} = 2^{-\text{depth}(\ell)}$, we get
$$\sum_{\ell \text{ a leaf of } T}\text{depth}(\ell) \geq n! \lg(n!)$$
It follows that
$$\frac{1}{n!}\sum_{\ell \text{ a leaf of } T}\text{depth}(\ell) \geq \lg(n!)$$
but the expression on the left is just the average depth of each leaf, i.e., the average number of comparisons used (averaged over all possible arrangements of the input). \\
Hence the average number of comparisons used is $\Omega(n \lg{n})$.
\end{proof}

\newpage
\section{Graphs and Graph Algorithms}
A graph consists of a bunch of points, called vertices (or nodes), and a bunch of connections between pairs of points, called edges (or arcs). \\
So $G = (V, E)$, where $V$ is a set of vertices and $E$ is a subset of $V \times V$. \\
Where $|V|$ is the number of vertices and $|E|$ is the number of edges in $G$. \\

Graphs can be directed or undirected;
in a directed graph, edges have a direction, and you can only traverse the graph by going in that direction.
In an undirected graph, edges can be traversed in any direction. \\

Sometimes we allow graphs to have edges from a vertex to itself;
such an edge is called a "loop" or a "self-loop" and the graph is no longer \emph{simple}. \\
Most graphs in this course won't have such edges, we will allow self-loops for directed graphs and only sometimes for undirected graphs.
We will rarely have two or more different edges between the same two vertices. \\

\emph{Note}: in any graph, $|E| = O(|V|^{2})$, and if the graph is connected, $|E| = \Omega(|V|)$. \\

A \emph{path} is a series of vertices where each consecutive vertex is connected by an edge to the previous vertex. \\
An undirected graph is \emph{connected} if there is a path from every vertex to every other vertex. \\
The \emph{length} of a path is the number of edges in the path. \\
A path is a \emph{cycle} if the first vertex is the same as the last vertex.
Usually in a path we demand that there are no repeated vertices (with the exception in a cycle). \\
A \emph{walk} (resp., \emph{closed walk}) is a path (resp., cycle) where repeated vertices and edges are allowed. \\
A graph is \emph{acyclic} if there are no cycles. \\

A \emph{tree} is a connected acyclic graph. \\
An undirected graph is a tree if every pair of vertices are connected by a \emph{unique} simple path. \\
Trees can be rooted (have a distinguishable vertex called the root) or unrooted. \\
A \emph{forest} is a vertex-disjoint collection of 1 or more trees. \\

Two ways to store a graph:
\begin{itemize}
  \item[1.] \textbf{Adjacency Matrix} \\
  We represent a graph by a $|V| \times |V|$ matrix with a 1 in row $i$ and column $j$ is $(i, j)$ is an edge, and 0 otherwise.
  \item[2.] \textbf{Adjacency List} \\
  We represent a graph by an array Adj of $|V|$ lists, one list for each vertex. \\
  For each vertex Adj[u] contains all vertices (or pointers to the vertices) $v$ such that $uv$ is an edge.
\end{itemize}

Advantages and disadvantages of each representation: \\
\emph{Adjacency Matrix}:
\begin{itemize}
  \item easy to manipulate
  \item can check if $uv$ is an edge in $O(1)$ time
  \item but storage is always $\Omega(|V|^{2})$---not good for sparse graphs
  \item time to add an edge:
  \begin{itemize}
    \item existing vertex and new edge: $O(1)$ time
    \item existing vertex and existing edge: $O(1)$ time, can check in $O(1)$ time
    \item new vertex and new edge: $O(|V|^{2})$ time, create a new, bigger matrix
  \end{itemize}
\end{itemize}

\emph{Adjacency List}:
\begin{itemize}
  \item harder to manipulate
  \item checking if $uv$ is an edge takes $O(|V|) time$
  \item but storage is $O(|V| + |E|)$
  \item time to add an edge:
  \begin{itemize}
    \item existing vertex and new edge: $O(|V|)$ time
    \item existing vertex and existing edge: $O(|V|)$ time
    \item new vertex and new edge: $O(|V|)$ time
  \end{itemize}
\end{itemize}

Sometimes there are weights on the edges. \\
In this case we can represent a weighted graph by a $|V| \times |V|$ matrix with a number $x$ in row $i$ and column $j$ if the edge from $i$ to $j$ has weight $x$. \\
If $i$ is not directly connected to $j$, then the weight is typically taken to be infinite. \\
Or we can use the adjacency list representation, with weights given together with the vertices in a list. \\

The \emph{weight of a path} is the sum of the weights of all the edges traversed along that path.

\subsection{Minimum Spanning Tree}
\emph{Note}: here we are working with undirected graphs. \\
A spanning tree is an (unrooted) tree that connects all the vertices of a graph. \\
Every connected graph has at least one spanning tree;
typically, a graph can have many, many different spanning trees. \\
If there are weights on the edges, a minimum spanning tree is one that minimizes the sum of the weights of the edges used. \\
Note that edge weights need not be distances!
They can even be 0 or negative. \\
A graph can have more than one minimum spanning tree. \\

If the graph is not connected, we sometimes talk about spanning \emph{forests} instead of spanning trees. \\

Minimum spanning trees come up all the time in network problems. \\
Here the nodes represent nodes of a network that need to be connected, and the weights represent distances (or perhaps, amounts of wire required) between the vertices. \\
We want to hook up all the nodes in our network using the smallest total distance (or amount of wire). \\

\lecture{2016/11/03}

Algorithms for \emph{minimum spanning tree}:
\begin{itemize}
  \item[\textbf{Kruskal}:] many different trees that are eventually joined up to form one tree
  \item[\textbf{Prim}:] one tree that grows by sequentially adding edges
\end{itemize}

\emph{Neither} Kruskal nor Prim work for directed graphs. \\
That problem is called the ``minimum cost arborescence'' problem;
it can be solved in $min(n^{2}, m\lg{n})$ time.

\subsubsection{Kruskal's Algorithm}
We consider each edge in turn, starting with the lowest-weight edge and continuing in ascending order by weight.
We only add the edge if it doesn't create a cycle. \\
It's a greedy algorithm where, at each step, we choose the minimum-weight edge that doesn't create a cycle. \\

To implement Kruskal's algorithm, we need to consider the edges in turn, ordered in ascending order by weight;
which we can do by sorting the edges. \\
Then when we pick an edge $uv$ to consider, we have to decide if they are in different trees, or the same tree.
This is not so easy!
But it is an example of the "disjoint-set data structure problem." \\

In the disjoint-set data structure problem, we have three operations:
\begin{itemize}
  \item[\texttt{MAKE-SET}:] create a set containing a single vertex
  \item[\texttt{FIND}:] given a vertex, find the ``name'' of the set it is in (sets are disjoint so only one name is possible for each vertex)
  \item[\texttt{UNION}:] union together two sets, possibly giving the resulting set a new name (destructive)
\end{itemize}

Due to the two main operations, this is also called the ``union-find'' problem. \\

It is possible to do $m$ \texttt{MAKE-SET}, \texttt{UNION} and \texttt{FIND} operations,$n$ of which are \texttt{MAKE-SET}, in $O(m~\alpha(n))$ time. \\
$\alpha(n)$ is an extremely slow-growing function called the ``inverse Ackerman'' function;
the function grows so slowly that $\alpha(n) \leq 4$ for all values of $n$ that one would ever encounter in real life. \\

How Kruskal's algorithm may be implemented:
\begin{algorithmic}
\Function{kruskal}{$G, w$}
  \Comment{$G = (V, E)$ and $w$ is the weight function}
  \State $A \gets$ empty set
  \For{each $v$ in $V$}
    \State \Call{make-set}{$v$}
  \EndFor
  \State sort $E$ into increasing order of weight
  \For{each edge $uv$ in $E$, in increasing order}
    \If{\Call{find}{$u$} $\neq$ \Call{find}{v}}
      \State $A \gets A \cup \{uv\}$
      \State \Call{union}{$u$, $v$}
    \EndIf
  \EndFor
  \State \Return{$A$}
\EndFunction
\end{algorithmic}

\texttt{MAKE-SET(v)} creates a new set whose only element is $v$. \\
\texttt{FIND(u)} returns the name of the set that $u$ is contained in. \\
\texttt{UNION(u,v)} unions together the trees containing $u$ and $v$ and gives the result a name (which can be found with \texttt{FIND}). \\

Sets are stored as linked lists of vertices. \\
Each element in the list has a pointer to its next element, and a pointer to the head of the list. \\
There is also a tail pointer to the end of the list, and the list maintains how many elements it contains. \\
The "name" is the first vertex in the list. \\

To do \texttt{MAKE-SET}, we create a linked list of a single element, and the \texttt{num} field set to 1.
This costs $O(1)$. \\

To do a \texttt{FIND}, we follow the pointer to the head of the list and return the element there.
This costs $O(1)$. \\

To do a \texttt{UNION(u,v)}, we follow the pointer of $u$ to its head, and the pointer of $v$ to its head. \\
There we find the number of elements in each list. \\
We can do this via ``weighted-union heuristic.'' \\
If (say) $u$ is in the smaller list, we link that list to the end of the bigger list ($v$'s list). \\
Then traverse $u$'s list, moving each head pointer to point to the head of $v$'s list. \\
Finally, we update the \texttt{num} field of $v$'s list. \\
This costs $O(t)$, where $t$ is the number of vertices of the smaller list. \\

Because we always update the head pointers of the elements of the smaller list, each \texttt{UNION} at least doubles the size of the smaller list. \\
So an element's name gets updated at most $O(log n)$ times. \\
Thus, over the course of the entire algorithm, we see that (all) $n$ \texttt{UNION}s take at most $O(n\lg{n})$ time.

\begin{theorem}
Kruskal's algorithm takes $O(|E|\lg{|E|}) = O(|E|\lg{|V|})$ time.
\end{theorem}
\begin{proof}
First, note that the algorithm does indeed produce a spanning tree. \\
The output cannot have a cycle, because if it did, the edge added that forms the cycle would join together two vertices in the same tree. \\
The output must be connected, for if it weren't, when we considered the edge that connected two components, we would have added it. \\

Next, we need to argue that the spanning tree $T$ produced by Kruskal's algorithm is a minimum spanning tree. \\
Assume it isn't.
Among all minimum spanning trees for $G$, the input, let $T'$ be the one with the largest number of edges in $T$. \\
If $T' = T$, we're done. \\
Otherwise, among all the edges in $T - T'$, let $e$ be the one that is added earliest by Kruskal's algorithm when we run it on $G$.

Because $T'$ is a spanning tree, $T' \cup \{e\}$ has a cycle $C$. \\
$T$ cannot contain all the edges of this cycle (otherwise it would not be a tree) so $C$ contains an edge $f$ that is not in $T$. \\
Now consider $T'' = (T' \cup \{e\}) - \{f\}$. T'' is also a spanning tree;
since T' was minimum, $weight(e) \geq weight(f)$. \\

If $weight(f) < weight(e)$, then the algorithm would have considered the edge $f$ before it would have considered the edge $e$. \\
Adding $f$ wouldn't create a cycle ($e$ was the first edge not in T'), so it would have been added. \\
But it wasn't;
$e$ was. \\
So $weight(f) \geq weight(e)$. \\
Thus $weight(e) = weight(f)$. \\

So $T''$ is also a minimum spanning tree. \\
But $T''$ has one more edge in common with $T$ than $T'$ does, contradicting the choice of $T'$.
\end{proof}

\subsubsection{Prim's Algorithm}
\lecture{2016/11/08}
It is a greedy algorithm where, at each step, we choose a previously unconnected vertex that becomes connected by a lowest-weight edge. \\

At every step, we need to pick an appropriate vertex that is not in the tree $T$, say $r$. \\
The vertex $v$ that we want will have the minimum-cost edge connecting $v$ to some vertex already in $T$. \\
So it makes sense to keep those vertices not in $T$ in a min-priority queue (min-heap). \\

For each vertex $v$, $key[v]$ is the minimum weight of any edge connecting $v$ to a vertex in $T$;
$key[v] = \infty$ if there is no such edge.
Initially the keys of all vertices equal $\infty$. \\

At every step we do an \texttt{EXTRACT-MIN} operation on the heap to get the vertex with the minimum edge weight associated with it. \\
Once we get that vertex, we have to look at all edges adjacent to it, and find the minimum weight edge. \\
We then update the key associated with each vertex, decreasing the key (via \texttt{DECRESE-KEY} if we find a lower-weight edge. \\

We use an array $\pi[u]$, indexed by the vertices $u$ of $V$, such that $\pi[u]$ is the parent of $u$ in the minimum spanning tree. \\
The actual tree itself will consist of the edges $(v, \pi[v])$ for all vertices $v$ in $V$ (except $r$, which has no parent). \\

The key idea is that whenever a new vertex $v$ is added to the tree $T$, we look at all the new edges $(v, u)$ incident on $v$ and update our knowledge about the lowest-cost edge connecting $u$ to $T$. \\
That way we examine all edges, but the priority queue only has to hold vertices, not edges.

\begin{algorithmic}
\Function{prim}{$G, w, r$}
  \For{each $u \in V$}
    \State \Comment{$key[u]$ is current estimate on cheapest edge $uv$ joining $u$ to vertices not yet considered}
    \State $key[u] \gets \infty$
    \State \Comment{$\pi[u]$ is the current predecessor of $u$ in minimum spanning tree created}
    \State $\pi[u] = nil$
  \EndFor
  \State $key[r] = 0$
  \State \Comment{Q is the priority queue, vertices that remain to be considered}
  \State $Q = V$
  \While{$Q \neq \varnothing$}
    \State $u = \Call{extract-min}{Q}$
    \For{each $v$ in $Adj[u]$}
      \If{$v \in Q$ and $w(u, v) < key[v]$}
        \State $\pi[v] = u$
        \State $key[v] = w(u, v)$
      \EndIf
    \EndFor
  \EndWhile
\EndFunction
\end{algorithmic}

\emph{Note}: as each vertex is removed from the priority queue $Q$, its connection to the tree is established through the parent pointer.
Before that time, the parent pointer of a vertex might point to a different vertex. \\

How do we determine, in \texttt{PRIM}, whether $v \in Q$? \\
One simple way is just to have every vertex have a flag, and when we do an \texttt{EXTRACT-MIN} and the vertex is removed, we set this flag. \\
Alternatively we can assume the vertices are numbered 1 through $n$ and use a bit-vector (an array of size $n$) to record whether a vertex is in $Q$ or not;
this would be updated over time. \\

What is the running time? \\
We can build the heap initially in $O(|V|)$ time. \\
We call \texttt{EXTRACT-MIN} $|V|$ times, and each call costs $O(\lg{|V|})$.
So this part is $O(|V|\lg{|V|})$. \\
We have to traverse all the edges, so that is $O(|E|)$. \\
For each edge, we potentially call \texttt{DECREASE-KEY}, which costs $O(\lg{|V|})$.
So that costs $O(|E|\lg{|V|})$. \\
The total is $O(|V|\lg{|V|} + |E|\lg{|V|}) = O(|E|\lg{|V|})$, which is the same time as Kruskal's algorithm. \\

Its running time can be \emph{improved} to $O(|E| + |V|\lg{|V|})$ with a more sophisticated data structure --- Fibonacci heaps --- that supports \texttt{DECREASE-KEY} more efficiently. \\
If the graph is dense (more than a linear number of edges), this time is asymptotically better than before.

\subsection{Shortest Path Problems}
The \emph{single-source} shortest path problem is the following: given a source vertex $s$, and a sink vertex $v$, we'd like to find the shortest path from $s$ to $v$. \\
Here shortest path means a sequence of directed edges from $s$ to $v$ with the smallest total weight. \\
Now it turns out that for most of the known algorithms, it is just as efficient to find the shortest paths from $s$ to all the other vertices of the graph. \\

Applications of shortest paths include speech recognition, to try to figure out the meaning of sentences. \\
Here we have to distinguish between different words that sound alike (homophones), such as ``to'', ``two'', and ``too''. \\
To do this, construct a graph whose vertices are words and whose edges are words that follow the given words in a sentence. \\
The edge between two words carries a weight measuring the likelihood of the transition;
the lower the weight, the more reasonable the transition. \\
So ``to school'' would have low weight, while ``two school'' would have a high weight. \\
Then a shortest path gives the best interpretation of an uttered sentence. \\

Other applications include network routing protocols such as ``IS-IS'' (Intermediate System to Intermediate System) and ``OSPF'' (Open Shortest Path First) use shortest paths algorithms. \\

Another application is graph drawing. \\
We'd like to ``center'' a graph on a page. \\
But what's the center?
It could be a vertex that minimizes the maximum distance to any other vertex in the graph. \\
We could find this by finding the shortest path between all pairs of vertices.

\subsubsection{Aspects of Shortest Paths}
The definition of shortest paths involves some some subtleties. \\
Maybe we really want a shortest walk, not shortest path? \\
So should we allow \emph{negative edges}?
Of course, there are no negative distances;
nevertheless, there are actually some cases where negative edges make logical sense. \\
But then there may not \emph{be} a shortest walk, because if there is a \emph{cycle} with negative weight, we could simply go around that cycle as many times as we want and reduce the cost of the path as much as we like. \\
To avoid this, we might want to detect negative cycles.
This can be done in the shortest-path algorithm itself. \\
Non-negative cycles aren't helpful, either. \\
Suppose our shortest walk contains a cycle of non-negative weight. \\
Then by cutting it out we get a walk with the same weight or less, so we might as well cut it out. \\
So we can assume our walks are actually paths.

\subsection{Shortest Path Algorithms}
Given a source vertex $s$, a shortest-path algorithm constructs:
\begin{itemize}
  \item a predecessor $\pi[v]$ for each vertex $v$;
  this is the predecessor of $v$ in the shortest path from $s$ to $v$
  \item $d[v]$, the current upper bound on the weight of the shortest path from $s$ to $v$ ("shortest-path estimate")
\end{itemize}

\begin{algorithmic}
\Function{initialize}{$G, s$}
  \For{each vertex $v \in V$}
    \State $d[v] = \infty$
    \State $\pi[v] = nil$
  \EndFor
  \State $d[s] = 0$
\EndFunction
\end{algorithmic}
\medskip
Main idea: ``relaxation'' of edges. \\
Here we already have a shortest-path estimate to both $u$ and $v$. \\
We consider an edge $(u,v)$ and then update our estimate on $v$, if $d[u] + w(u, v) < d[v]$.

\begin{algorithmic}
\Function{relax}{$u, v, w$}
  \State $t = d[u] + w(u, v)$
  \If{$d[v] > t$}
    \State $d[v] = t$
    \State $\pi[v] = u$
  \EndIf
\EndFunction
\end{algorithmic}
\medskip
The \emph{Bellman-Ford} algorithm: it computes the length of a shortest path $d[]$ originating from $s$ to all other vertices on the graph $G = (V,E)$, using the weight function $w$. \\
Further, it returns either true or false, false iff. the graph contains a reachable negative-weight cycle, true otherwise. \\
It makes no assumptions at all about the weights on the edges;
they can be negative.
\lecture{2016/11/10}
\begin{algorithmic}
\Function{bellman-ford}{$G, w, s$}
  \State \Call{initialize}{$G, s$}
  \For{$i = 1$ to $|V| - 1$}
    \For{each edge $uv$ in $E$}
      \State \Call{relax}{$u, v, w$}
    \EndFor
  \EndFor
  \For{each edge $uv$ in $E$}
    \If{$d[v] > d[u] + w(u, v)$}
      \State \Return false
    \EndIf
  \EndFor
  \State \Return true
\EndFunction
\end{algorithmic}

Let's prove that this method works. \\
Intuitively, the idea is that each iteration for the first \texttt{for} loop increases the total number of edges in a path from $s$ by one. \\
So after at most ($|V| - 1$) iterations, we will have considered all the paths (if there are no negative cycles). \\
If there's a negative cycle, then we can go on improving our cost vertices within the loop, and this is detected by the second \texttt{for} loop.

\begin{lemma}
After $i$ repetitions of the \texttt{for} loop in \texttt{BELLMAN-FORD}, the following properties hold:
\begin{itemize}
  \item If $d[u] \neq \infty$, it is equal to the weight of some path from $s$ to $u$
  \item If there is a path from $s$ to $u$ with at most $i$ edges, then $d[u] \leq$ weight of the shortest path from $s$ to $u$ having at most $i$ edges.
\end{itemize}
\end{lemma}
\begin{proof}
By induction on $i$. \\

The base case is $i = 0$. \\
Consider what happens before the first \texttt{for} loop is executed. \\
Then $d[s] = 0$ for the source vertex $s$, which is correct. \\
For all the other vertices $u$, $d[u] = \infty$, which is also correct because there is no path from $s$ to $u$ with 0 edges. \\

For the induction step, consider when a vertex's estimated distance is updated by $d[v] = d[u] + w(u, v)$. \\
By out inductive hypothesis, $d[u]$ is the weight of some path $p$ from $s$ to $u$. \\
Then $(d[u] + w(u, v))$ is the weight of the path that follows $p$ from $s$ to $u$ and then goes to $v$. \\
So the first property holds. \\

For the second property, consider the shortest path from $s$ to $u$ with at most $i$ edges. \\
Let $v$ be the last vertex before $u$ on this path. \\
Then the part of the path from $s$ to $v$ is the shortest path from $s$ to $v$ with at most $(i - 1)$ edges. \\
By the inductive hypothesis, $d[v]$ after $(i - 1)$ executions of the first \texttt{for} loop is at most the weight of this path. \\
Therefore, $(d[v] + w(u, v))$ is at most the weight of the path from $s$ to $u$. \\
In the $i$-th iteration of the first \texttt{for} loop, $d[u]$ gets compared with $(d[v] + w(u, v))$, and is set to equal to it if $(d[v] + w(u, v))$ is smaller. \\

Therefore, after $i$ iterations of the first \texttt{for} loop, $d[u]$ is at most the weight of the shortest path from $s$ to $u$ that uses at most $i$ edges. \\
So the second property holds. \\

If there are no negative-weight cycles, then every shortest path visits each vertex at most once, so the second \texttt{for} loop does not detect any possible shorter paths. \\

Conversely, suppose no shorter path is detected in the second \texttt{for} loop. \\
Then for any cycle with vertices $v_{0}, \dots, v_{k - 1}$, we have
\begin{align*}
d[v_{1}] &\leq d[v_{0}] + w(v_{0}, v_{1}) \\
d[v_{2}] &\leq d[v_{1}] + w(v_{1}, v_{2}) \\
&\vdots \\
d[v_{0}] &\leq d[v_{k - 1}] + w(v_{k - 1}, v_{0})
\end{align*}
If we now sum both sides, the $d[v_{i}]$ terms cancel, giving us
$$w(v_{0}, v_{1}) + w(v_{1}, v_{2}) + \cdots + w(v_{k - 1}, v_{0}) \geq 0$$
which does not have negative weight.
\end{proof}

\emph{Note}: \texttt{BELLMAN-FORD} actually creates a tree! \\

What is the running time of \texttt{BELLMAN-FORD}?
The \texttt{INITIALIZE} step costs $O(|V|)$.
The first \texttt{for} loop runs $(|V| - 1)$ times, and inside the loop we do $|E|$ work.
The second \texttt{for} loop costs $|E|$.
So the total cost is $O(|V||E|)$.

\subsection{Dijkstra's Algorithm}
\emph{Dijkstra's algorithm} is another shortest-paths algorithm. \\
If implemented carefully, it can run faster than \texttt{BELLMAN-FORD}, but it has the disadvantage that the edge weights \emph{must be non-negative}. \\
In \texttt{BELLMAN-FORD}, every edge is relaxed $|V|$ times.
In \texttt{DIJKSTRA}, we avoid this, at the cost of having (essentially) to do some sorting. \\

Dijkstra's algorithm is quite similar to Prim's algorithm for the minimum spanning tree. \\
The idea is to maintain a set of $S$ of vertices for which you already know the shortest path from the source. \\
At every step you choose a vertex $u$ from $V - S$ with the minimum shortest-path estimate, add $u$ to $S$, and relax all the edges leaving $u$. \\
This can be done by keeping the vertices in a priority queue.

\begin{algorithmic}
\Function{Dijkstra}{$G, w, s$}
  \State \Call{initialize}{$G, s$}
  \State $S \gets \varnothing$
  \State $Q \gets$ a min-priority queue on the vertices $V$
  \While{$Q \neq \varnothing$}
    \State $u \gets \Call{extract-min}{Q}$
    \State $S \gets S \cup \{u\}$
    \For{each vertex $v$ in $Adj[u]$}
      \State \Call{relax}{$u, v, w$}
    \EndFor
  \EndWhile
\EndFunction
\end{algorithmic}

Informally, the correctness of the algorithm follows because we keep the vertices in three groups: the vertices already considered (those in $S$) for which we already know the shortest-path; the vertex currently being considered; and the remaining vertices. \\
At each step we choose the unconsidered vertex for which our distance element is smallest and update our distance estimates. \\

Let's figure out the running time. \\
Each vertex $v$ is added $S$ exactly once, so each edge in $Adj[v]$ is examined exactly once. \\
Since the total number of edges is $|E|$, there are $|E|$ iterations of the for loop, and so at most $|E|$ \texttt{DECREASE-KEY} operations (done in \texttt{RELAX}). \\
The total cost is there for $O(|V| + |E|\lg{|V|})$. \\
If all vertices are reachable from the source, then $|E| \geq |V| - 1$, so this is $O(|E|\lg{|V|})$. \\
This can be improved to $O(|V|\lg{|V|} + |E|)$ with a fancy data structure, the Fibonacci heap. \\

By the way, how does Google Maps find shortest paths? \\
Because there are millions and millions of possible vertices in the database, Google doesn't run a shortest path algorithm on this giant database. \\
Rather, it uses some heuristics. \\
For example, to go from Kitchener to Winnipeg, it first routes you through Kitchener to some big road going in the right direction (using shortest paths on the much-smaller local database), then follows the big road to the vicinity of Winnipeg, then runs shortest-paths on that local area.

\subsection{Floyd-Warshall Algorithm}
\lecture{2016/11/15}
We're interested in the \emph{all-pairs shortest path} problem;
we are given a weighted directed graph and we want to find the shortest-path distance between \emph{every} pair of vertices. \\
Note that the edges can possibly be negative, but no negative-weight cycles. \\

It's easy to find shortest paths when there are 0 edges, or 1 edge, in the path. \\
So maybe the right way to do it is solve all-pairs shortest paths for paths having 0 edges, or 1 edges, etc. \\
Let $cost[i, j, k]$ be the minimum cost of a path from node $i$ to node $j$ using at most $k$ edges.
We arrive at the following recursion,
$$cost[i, j, k] = \underset{1 \leq \ell \leq n}{\min} ~ w(i, \ell) + cost[\ell, j, k - 1]$$
We have to fill in all the entries for $1 \leq i, j, k \leq n$, and filling each entry costs $O(n)$.
This gives an $O(n^{4})$ algorithm which is rather slow, we can do better! \\

Floyd came up with the idea. \\
Let the vertices be numbered $1, 2, \dots, n$. \\
We consider all paths from vertex $i$ to vertex $j$ in which all intermediate vertices are numbered $k$ or lower. \\
Let $d_{ij}^{~(k)}$ be the weight of the shortest path from $i$ to $j$ in which all intermediate vertices are numbered $k$ or lower. \\

Then we get the following recurrence:
$$
d_{ij}^{~(k)} =
\begin{cases}
  w_{ij} & \text{if } k = 0 \\
  \min(d_{ij}^{~(k-1)},~d_{ik}^{~(k-1)} + d_{kj}^{~(k-1)}) & \text{if } k \geq 1
\end{cases}
$$

Note that if $k = 0$, then there can be no intermediate vertices at all, so the only possibility is a one-step path from $i$ to $j$, which has weight $w_{ij}$. \\

Now $d_{ij}^{~(k)}$ is supposed to reflect paths where all intermediate vertices are numbered $\leq k$. \\
So either $k$ appears as an intermediate vertex in a shortest path, or it doesn't. \\
In the latter case, $d_{ij}^{~(k-1)}$. \\
In the former case, $k$ appears only once so we can break the path into the part before $k$ appears and the part after.
In both of these parts $k$ does not appear again, so $d_{ij}^{~(k-1)} = d_{ik}^{~(k-1)} + d_{kj}^{~(k-1)}$. \\

\begin{algorithmic}
\Function{Floyd}{$W, n$}
  \For{$i = 1$ to $n$}
    \For{$j = 1$ to $n$}
      \State $cost[i, j, 0] = W[i, j]$
    \EndFor
  \EndFor
  \For{$k = 1$ to $n$}
    \For{$i = 1$ to $n$}
      \For{$j = 1$ to $n$}
        \State $cost[i, j, k] = \min(cost[i, j, k-1], cost[i, k, k-1] + cost[k, j, k-1])$
      \EndFor
    \EndFor
  \EndFor
  \For{$i = 1$ to $n$}
    \For{$j = 1$ to $n$}
      \State $M[i, j] = cost[i, j, n]$
    \EndFor
  \EndFor
  \State \Return{$M$}
\EndFunction
\end{algorithmic}

The algorithm runs in $O(n^{3})$ in the unit-cost model, so with this we can match the running time of Dijkstra's algorithm run from every vertex, even if we use the complicated Fibonacci heaps in Dijkstra's algorithm. \\

This uses $\theta(n^{3})$ space, but it is easy to rewrite it so that only $\theta(n^{2})$ space is needed. \\
To do this, we can just store the $k-1$ value since we only look at the previous one.

\newpage
\section{P, NP, and all that}
There are some problems that are solvable, but for which it is possible to prove that \textbf{no faster algorithm exists}. \\
There are also problems for which no fast algorithm is \emph{currently known} to exist, and for which there is some evidence that no fast algorithm will ever be found. \\

\subsection{P}
What does ``fast'' mean? \\
The answer that has become traditional by now is that the program's running time should be \textbf{bounded by a polynomial in the input size}, in the log-cost model;
in other words, the worst-case running time should be $O(n^{k})$ for some $k$. \\
This is called \textbf{polynomial time}, and the complexity class P is the class of all problems solvable in polynomial time.
Strictly speaking, P is actually the class of \emph{languages} denoting encoding of polynomial-time-solvable decision problems, but we don't need to be so rigorous for this course. \\

P contains pretty much every problem we have looked at so far in this course: maximum sub-range sum, greatest common divisor, matrix multiplication, all-pairs shortest paths, etc. \\

One problem we discussed that is \emph{not} in P is the problem of listing all the permutations. \\
This cannot be in P because we need to output $n!$ results on an input of size $n$. \\
But this isn't so interesting because the complexity of the problem arises from the large output size.
So sometimes it makes more sense to measure complexity of a problem in terms of the size of the input \emph{and} the size of the output. \\

Why is polynomial-time a synonym for ``fast''?
\begin{itemize}
  \item Nearly every algorithm that people actually consider to be fast in practice runs in polynomial time in theory.
  (A possible exception is the simplex algorithm for linear programming, which runs relatively quickly on most natural examples, but has an exponential worst-case run time.
  However, there are algorithms for linear programming which are genuinely polynomial time, like the ellipsoid method.)

  \item A program that runs in polynomial time in one model runs in polynomial time in almost all other models.
  This is not completely trivial and there is definitely something to prove here.
  (The relatively recent invention of the quantum computing model means this is probably no longer true for all pairs of models.)

  \item The class P has nice closure properties.
  It is closed under composition.
  P is also closed under using a polynomial-time subroutine, \emph{provided} that the input is not expanded each time the subroutine is called. (If the input is expanded in size, then you could get something that runs in exponential, not polynomial time.)
\end{itemize}

Objections to the definition of P:
\begin{itemize}
  \item It doesn't consider the exponent of the polynomial.
  An algorithm running in $n^{10000}$ time is considered ``fast'' and one running in $2^{n/10000}$ is not, although for practical problem sizes the second will be more useful that the first. \\
  Response: true, but ``most'' natural problems in P has algorithms with small polynomial exponents.

  \item It doesn't consider the size of the constant out in front.
  An algorithm running in $1000000n^{2}$ will be less useful than an algorithm running in $2^{n/10000}$. \\
  Response: true again, but ``most'' natural problems in P have algorithms with relatively small constants out in front.

  \item It considers only worst-case behaviour.
  Some problems may be efficiently solvable for most inputs, and only require lots of time on a small set of inputs. \\
  Response: true, but such problems may be relatively rare.

  \item It doesn't consider other models of computation, such as quantum computing. \\
  Response: true, but we don't yet have a practical quantum computer
\end{itemize}

Recall that P is the class of problems solvable in polynomial time. \\
There is one subtlety here, which we have already encountered, and that is how the size of the input is measured. \\
The convention is that numbers are usually represented in base-2 (or any base $\geq$ 2), \emph{not} in unary. \\
Forcing the input to be in unary can transform an algorithm that doesn't run in polynomial time to one that does.
\begin{ex*}
Consider prime factorization, which is generally considered to be a ``hard'' problem. \\
Nobody currently knows an algorithm for the prime factorization of $n$ in time polynomial in $\lg{n}$, but there are easy algorithms that run in time polynomial in $n$.
\end{ex*}

Similarly, if we choose a crazy encoding for a problem (the way the problem is actually represented as input) then we could get a crazy result, like hard problems suddenly become easy. \\
Normally there is an obvious way to encode things, and usually two different reasonable encoding are polynomial-related;
there is a polynomial-time algorithm converting one to the other.

\subsection{Problems not in P}
There are some problems, like the halting problem, that are not solvable in \emph{any} time bound. \\

Consider the following problem: \\
ERE problem: Given two extended regular expressions $E$ and $F$ over an alphabet $\Sigma$, decide if $E$ and $F$ specify the same set. \\

An extended regular expression (ERE) is just like an ordinary regular expression, except that exponentiation is allowed.
\begin{ex*}
$(a^{3} + b^{2})^{2} = (aaa + bb)^{2} = (aaa + bb)(aaa + bb) = aaaaaa + aaabb + bbaaa + bbbb$
\end{ex*}

We may assume that the exponents are decimal numbers.
No other exponents are allowed. \\

The ERE problem is \emph{solvable}: to see this, we can take any ERE and expand out all the exponents to get a regular RE. \\
Do this for $E$ and $F$, getting RE's $E'$ and $F'$. \\
Now convert these to equivalent deterministic finite automata, minimize the automata, and check if the resulting minimized automata are the same. \\

However, the ERE problem is not \emph{efficiently} solvable.

\begin{theorem}
There exists a constant $c$ such that every algorithm that solves the ERE problem must use at least $2^{cn/\lg{n}}$ time on infinitely many inputs.
(Here n, as usual, measures the size of the input.)
\end{theorem}

Therefore the ERE problem cannot be solved in polynomial time. \\
Unfortunately, the number of such natural examples that are known is fairly small.

\subsection{NP}

\clearpage
\printindex
\end{document}

%%%%%%%%%%%%%%%%%%%%%
%% D O C U M E N T %%
%%%%%%%%%%%%%%%%%%%%%
